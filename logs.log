2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:43:41,558:INFO:PyCaret ClassificationExperiment
2024-08-31 13:43:41,559:INFO:Logging name: clf-default-name
2024-08-31 13:43:41,559:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 13:43:41,560:INFO:version 3.3.2
2024-08-31 13:43:41,560:INFO:Initializing setup()
2024-08-31 13:43:41,560:INFO:self.USI: 90ce
2024-08-31 13:43:41,560:INFO:self._variable_keys: {'USI', 'fold_shuffle_param', 'gpu_param', 'html_param', 'memory', 'gpu_n_jobs_param', 'fold_generator', 'is_multiclass', 'fold_groups_param', 'data', 'fix_imbalance', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'seed', 'idx', 'X', 'y_train', '_ml_usecase', 'y_test', 'X_train', 'exp_id', 'logging_param', 'y', '_available_plots', 'target_param', 'X_test', 'pipeline'}
2024-08-31 13:43:41,560:INFO:Checking environment
2024-08-31 13:43:41,560:INFO:python_version: 3.11.5
2024-08-31 13:43:41,560:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-08-31 13:43:41,560:INFO:machine: AMD64
2024-08-31 13:43:41,560:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 13:43:41,560:INFO:Memory: svmem(total=8269512704, available=419684352, percent=94.9, used=7849828352, free=419684352)
2024-08-31 13:43:41,560:INFO:Physical Core: 4
2024-08-31 13:43:41,560:INFO:Logical Core: 8
2024-08-31 13:43:41,560:INFO:Checking libraries
2024-08-31 13:43:41,560:INFO:System:
2024-08-31 13:43:41,560:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-08-31 13:43:41,560:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-08-31 13:43:41,560:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 13:43:41,560:INFO:PyCaret required dependencies:
2024-08-31 13:43:44,146:INFO:                 pip: 23.2.1
2024-08-31 13:43:44,147:INFO:          setuptools: 68.0.0
2024-08-31 13:43:44,147:INFO:             pycaret: 3.3.2
2024-08-31 13:43:44,147:INFO:             IPython: 8.15.0
2024-08-31 13:43:44,147:INFO:          ipywidgets: 8.0.4
2024-08-31 13:43:44,147:INFO:                tqdm: 4.65.0
2024-08-31 13:43:44,147:INFO:               numpy: 1.24.3
2024-08-31 13:43:44,147:INFO:              pandas: 2.0.3
2024-08-31 13:43:44,147:INFO:              jinja2: 3.1.2
2024-08-31 13:43:44,147:INFO:               scipy: 1.11.1
2024-08-31 13:43:44,147:INFO:              joblib: 1.2.0
2024-08-31 13:43:44,148:INFO:             sklearn: 1.4.2
2024-08-31 13:43:44,148:INFO:                pyod: 2.0.1
2024-08-31 13:43:44,148:INFO:            imblearn: 0.12.3
2024-08-31 13:43:44,148:INFO:   category_encoders: 2.6.3
2024-08-31 13:43:44,148:INFO:            lightgbm: 4.5.0
2024-08-31 13:43:44,148:INFO:               numba: 0.57.1
2024-08-31 13:43:44,148:INFO:            requests: 2.31.0
2024-08-31 13:43:44,148:INFO:          matplotlib: 3.7.2
2024-08-31 13:43:44,148:INFO:          scikitplot: 0.3.7
2024-08-31 13:43:44,148:INFO:         yellowbrick: 1.5
2024-08-31 13:43:44,148:INFO:              plotly: 5.24.0
2024-08-31 13:43:44,148:INFO:    plotly-resampler: Not installed
2024-08-31 13:43:44,148:INFO:             kaleido: 0.2.1
2024-08-31 13:43:44,148:INFO:           schemdraw: 0.15
2024-08-31 13:43:44,148:INFO:         statsmodels: 0.14.0
2024-08-31 13:43:44,148:INFO:              sktime: 0.26.0
2024-08-31 13:43:44,148:INFO:               tbats: 1.1.3
2024-08-31 13:43:44,148:INFO:            pmdarima: 2.0.4
2024-08-31 13:43:44,148:INFO:              psutil: 5.9.0
2024-08-31 13:43:44,148:INFO:          markupsafe: 2.1.1
2024-08-31 13:43:44,149:INFO:             pickle5: Not installed
2024-08-31 13:43:44,149:INFO:         cloudpickle: 2.2.1
2024-08-31 13:43:44,149:INFO:         deprecation: 2.1.0
2024-08-31 13:43:44,149:INFO:              xxhash: 2.0.2
2024-08-31 13:43:44,149:INFO:           wurlitzer: Not installed
2024-08-31 13:43:44,149:INFO:PyCaret optional dependencies:
2024-08-31 13:43:44,162:INFO:                shap: Not installed
2024-08-31 13:43:44,162:INFO:           interpret: Not installed
2024-08-31 13:43:44,162:INFO:                umap: Not installed
2024-08-31 13:43:44,162:INFO:     ydata_profiling: Not installed
2024-08-31 13:43:44,162:INFO:  explainerdashboard: Not installed
2024-08-31 13:43:44,162:INFO:             autoviz: Not installed
2024-08-31 13:43:44,162:INFO:           fairlearn: Not installed
2024-08-31 13:43:44,162:INFO:          deepchecks: Not installed
2024-08-31 13:43:44,162:INFO:             xgboost: Not installed
2024-08-31 13:43:44,162:INFO:            catboost: Not installed
2024-08-31 13:43:44,162:INFO:              kmodes: Not installed
2024-08-31 13:43:44,162:INFO:             mlxtend: 0.22.0
2024-08-31 13:43:44,162:INFO:       statsforecast: Not installed
2024-08-31 13:43:44,162:INFO:        tune_sklearn: Not installed
2024-08-31 13:43:44,162:INFO:                 ray: Not installed
2024-08-31 13:43:44,162:INFO:            hyperopt: Not installed
2024-08-31 13:43:44,162:INFO:              optuna: Not installed
2024-08-31 13:43:44,163:INFO:               skopt: Not installed
2024-08-31 13:43:44,163:INFO:              mlflow: Not installed
2024-08-31 13:43:44,163:INFO:              gradio: Not installed
2024-08-31 13:43:44,163:INFO:             fastapi: Not installed
2024-08-31 13:43:44,163:INFO:             uvicorn: Not installed
2024-08-31 13:43:44,163:INFO:              m2cgen: Not installed
2024-08-31 13:43:44,163:INFO:           evidently: Not installed
2024-08-31 13:43:44,163:INFO:               fugue: Not installed
2024-08-31 13:43:44,163:INFO:           streamlit: Not installed
2024-08-31 13:43:44,163:INFO:             prophet: Not installed
2024-08-31 13:43:44,163:INFO:None
2024-08-31 13:43:44,163:INFO:Set up data.
2024-08-31 13:44:37,840:INFO:PyCaret ClassificationExperiment
2024-08-31 13:44:37,840:INFO:Logging name: clf-default-name
2024-08-31 13:44:37,841:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 13:44:37,841:INFO:version 3.3.2
2024-08-31 13:44:37,841:INFO:Initializing setup()
2024-08-31 13:44:37,841:INFO:self.USI: cfcc
2024-08-31 13:44:37,841:INFO:self._variable_keys: {'USI', 'fold_shuffle_param', 'gpu_param', 'html_param', 'memory', 'gpu_n_jobs_param', 'fold_generator', 'is_multiclass', 'fold_groups_param', 'data', 'fix_imbalance', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'seed', 'idx', 'X', 'y_train', '_ml_usecase', 'y_test', 'X_train', 'exp_id', 'logging_param', 'y', '_available_plots', 'target_param', 'X_test', 'pipeline'}
2024-08-31 13:44:37,841:INFO:Checking environment
2024-08-31 13:44:37,841:INFO:python_version: 3.11.5
2024-08-31 13:44:37,841:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-08-31 13:44:37,841:INFO:machine: AMD64
2024-08-31 13:44:37,841:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 13:44:37,841:INFO:Memory: svmem(total=8269512704, available=499904512, percent=94.0, used=7769608192, free=499904512)
2024-08-31 13:44:37,841:INFO:Physical Core: 4
2024-08-31 13:44:37,841:INFO:Logical Core: 8
2024-08-31 13:44:37,842:INFO:Checking libraries
2024-08-31 13:44:37,842:INFO:System:
2024-08-31 13:44:37,842:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-08-31 13:44:37,842:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-08-31 13:44:37,842:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 13:44:37,842:INFO:PyCaret required dependencies:
2024-08-31 13:44:37,842:INFO:                 pip: 23.2.1
2024-08-31 13:44:37,842:INFO:          setuptools: 68.0.0
2024-08-31 13:44:37,842:INFO:             pycaret: 3.3.2
2024-08-31 13:44:37,842:INFO:             IPython: 8.15.0
2024-08-31 13:44:37,842:INFO:          ipywidgets: 8.0.4
2024-08-31 13:44:37,842:INFO:                tqdm: 4.65.0
2024-08-31 13:44:37,842:INFO:               numpy: 1.24.3
2024-08-31 13:44:37,842:INFO:              pandas: 2.0.3
2024-08-31 13:44:37,842:INFO:              jinja2: 3.1.2
2024-08-31 13:44:37,842:INFO:               scipy: 1.11.1
2024-08-31 13:44:37,842:INFO:              joblib: 1.2.0
2024-08-31 13:44:37,842:INFO:             sklearn: 1.4.2
2024-08-31 13:44:37,842:INFO:                pyod: 2.0.1
2024-08-31 13:44:37,842:INFO:            imblearn: 0.12.3
2024-08-31 13:44:37,842:INFO:   category_encoders: 2.6.3
2024-08-31 13:44:37,842:INFO:            lightgbm: 4.5.0
2024-08-31 13:44:37,842:INFO:               numba: 0.57.1
2024-08-31 13:44:37,842:INFO:            requests: 2.31.0
2024-08-31 13:44:37,842:INFO:          matplotlib: 3.7.2
2024-08-31 13:44:37,842:INFO:          scikitplot: 0.3.7
2024-08-31 13:44:37,843:INFO:         yellowbrick: 1.5
2024-08-31 13:44:37,843:INFO:              plotly: 5.24.0
2024-08-31 13:44:37,843:INFO:    plotly-resampler: Not installed
2024-08-31 13:44:37,843:INFO:             kaleido: 0.2.1
2024-08-31 13:44:37,843:INFO:           schemdraw: 0.15
2024-08-31 13:44:37,843:INFO:         statsmodels: 0.14.0
2024-08-31 13:44:37,843:INFO:              sktime: 0.26.0
2024-08-31 13:44:37,843:INFO:               tbats: 1.1.3
2024-08-31 13:44:37,843:INFO:            pmdarima: 2.0.4
2024-08-31 13:44:37,843:INFO:              psutil: 5.9.0
2024-08-31 13:44:37,843:INFO:          markupsafe: 2.1.1
2024-08-31 13:44:37,843:INFO:             pickle5: Not installed
2024-08-31 13:44:37,843:INFO:         cloudpickle: 2.2.1
2024-08-31 13:44:37,843:INFO:         deprecation: 2.1.0
2024-08-31 13:44:37,843:INFO:              xxhash: 2.0.2
2024-08-31 13:44:37,843:INFO:           wurlitzer: Not installed
2024-08-31 13:44:37,843:INFO:PyCaret optional dependencies:
2024-08-31 13:44:37,843:INFO:                shap: Not installed
2024-08-31 13:44:37,843:INFO:           interpret: Not installed
2024-08-31 13:44:37,843:INFO:                umap: Not installed
2024-08-31 13:44:37,843:INFO:     ydata_profiling: Not installed
2024-08-31 13:44:37,843:INFO:  explainerdashboard: Not installed
2024-08-31 13:44:37,843:INFO:             autoviz: Not installed
2024-08-31 13:44:37,843:INFO:           fairlearn: Not installed
2024-08-31 13:44:37,843:INFO:          deepchecks: Not installed
2024-08-31 13:44:37,843:INFO:             xgboost: Not installed
2024-08-31 13:44:37,843:INFO:            catboost: Not installed
2024-08-31 13:44:37,843:INFO:              kmodes: Not installed
2024-08-31 13:44:37,843:INFO:             mlxtend: 0.22.0
2024-08-31 13:44:37,843:INFO:       statsforecast: Not installed
2024-08-31 13:44:37,844:INFO:        tune_sklearn: Not installed
2024-08-31 13:44:37,844:INFO:                 ray: Not installed
2024-08-31 13:44:37,844:INFO:            hyperopt: Not installed
2024-08-31 13:44:37,844:INFO:              optuna: Not installed
2024-08-31 13:44:37,844:INFO:               skopt: Not installed
2024-08-31 13:44:37,844:INFO:              mlflow: Not installed
2024-08-31 13:44:37,844:INFO:              gradio: Not installed
2024-08-31 13:44:37,844:INFO:             fastapi: Not installed
2024-08-31 13:44:37,844:INFO:             uvicorn: Not installed
2024-08-31 13:44:37,844:INFO:              m2cgen: Not installed
2024-08-31 13:44:37,844:INFO:           evidently: Not installed
2024-08-31 13:44:37,844:INFO:               fugue: Not installed
2024-08-31 13:44:37,844:INFO:           streamlit: Not installed
2024-08-31 13:44:37,844:INFO:             prophet: Not installed
2024-08-31 13:44:37,844:INFO:None
2024-08-31 13:44:37,844:INFO:Set up data.
2024-08-31 13:44:37,881:INFO:Set up folding strategy.
2024-08-31 13:44:37,882:INFO:Set up train/test split.
2024-08-31 13:44:37,907:INFO:Set up index.
2024-08-31 13:44:37,907:INFO:Assigning column types.
2024-08-31 13:44:37,915:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 13:44:37,963:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 13:44:37,967:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,001:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,040:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,040:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,068:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 13:44:38,108:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,136:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,177:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,202:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,202:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,202:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 13:44:38,268:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,268:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,336:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,336:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,340:INFO:Preparing preprocessing pipeline...
2024-08-31 13:44:38,343:INFO:Set up label encoding.
2024-08-31 13:44:38,344:INFO:Set up simple imputation.
2024-08-31 13:44:38,349:INFO:Set up encoding of categorical features.
2024-08-31 13:44:38,350:INFO:Set up column name cleaning.
2024-08-31 13:44:38,506:INFO:Finished creating preprocessing pipeline.
2024-08-31 13:44:38,514:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 13:44:38,515:INFO:Creating final display dataframe.
2024-08-31 13:44:38,898:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16               Fold Generator   
17                  Fold Number   
18                     CPU Jobs   
19                      Use GPU   
20               Log Experiment   
21              Experiment Name   
22                          USI   

                                                Value  
0                                                  42  
1                                        Failure Type  
2                                          Multiclass  
3   Heat Dissipation Failure: 0, No Failure: 1, Ov...  
4                                          (9973, 21)  
5                                          (9973, 23)  
6                                          (6981, 23)  
7                                          (2992, 23)  
8                                                  18  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                    StratifiedKFold  
17                                                 10  
18                                                 -1  
19                                              False  
20                                              False  
21                                   clf-default-name  
22                                               cfcc  
2024-08-31 13:44:39,045:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,113:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,113:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,114:INFO:setup() successfully completed in 1.27s...............
2024-08-31 13:49:30,581:INFO:Initializing compare_models()
2024-08-31 13:49:30,581:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 13:49:30,582:INFO:Checking exceptions
2024-08-31 13:49:30,593:INFO:Preparing display monitor
2024-08-31 13:49:30,631:INFO:Initializing Logistic Regression
2024-08-31 13:49:30,631:INFO:Total runtime is 1.668532689412435e-05 minutes
2024-08-31 13:49:30,635:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:30,639:INFO:Initializing create_model()
2024-08-31 13:49:30,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:30,639:INFO:Checking exceptions
2024-08-31 13:49:30,639:INFO:Importing libraries
2024-08-31 13:49:30,640:INFO:Copying training dataset
2024-08-31 13:49:30,654:INFO:Defining folds
2024-08-31 13:49:30,655:INFO:Declaring metric variables
2024-08-31 13:49:30,659:INFO:Importing untrained model
2024-08-31 13:49:30,664:INFO:Logistic Regression Imported successfully
2024-08-31 13:49:30,673:INFO:Starting cross validation
2024-08-31 13:49:30,677:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:44,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,361:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,445:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,446:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,470:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:45,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:45,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:45,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:45,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:45,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,908:INFO:Calculating mean and std
2024-08-31 13:49:45,910:INFO:Creating metrics dataframe
2024-08-31 13:49:45,915:INFO:Uploading results into container
2024-08-31 13:49:45,916:INFO:Uploading model into container now
2024-08-31 13:49:45,917:INFO:_master_model_container: 1
2024-08-31 13:49:45,917:INFO:_display_container: 2
2024-08-31 13:49:45,919:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:49:45,919:INFO:create_model() successfully completed......................................
2024-08-31 13:49:48,115:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:48,116:INFO:Creating metrics dataframe
2024-08-31 13:49:48,127:INFO:Initializing K Neighbors Classifier
2024-08-31 13:49:48,127:INFO:Total runtime is 0.2916130820910136 minutes
2024-08-31 13:49:48,130:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:48,131:INFO:Initializing create_model()
2024-08-31 13:49:48,131:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:48,131:INFO:Checking exceptions
2024-08-31 13:49:48,131:INFO:Importing libraries
2024-08-31 13:49:48,131:INFO:Copying training dataset
2024-08-31 13:49:48,144:INFO:Defining folds
2024-08-31 13:49:48,144:INFO:Declaring metric variables
2024-08-31 13:49:48,148:INFO:Importing untrained model
2024-08-31 13:49:48,151:INFO:K Neighbors Classifier Imported successfully
2024-08-31 13:49:48,161:INFO:Starting cross validation
2024-08-31 13:49:48,163:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:48,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,925:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,941:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,287:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,290:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,300:INFO:Calculating mean and std
2024-08-31 13:49:49,302:INFO:Creating metrics dataframe
2024-08-31 13:49:49,306:INFO:Uploading results into container
2024-08-31 13:49:49,306:INFO:Uploading model into container now
2024-08-31 13:49:49,307:INFO:_master_model_container: 2
2024-08-31 13:49:49,307:INFO:_display_container: 2
2024-08-31 13:49:49,308:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 13:49:49,308:INFO:create_model() successfully completed......................................
2024-08-31 13:49:49,460:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:49,460:INFO:Creating metrics dataframe
2024-08-31 13:49:49,469:INFO:Initializing Naive Bayes
2024-08-31 13:49:49,469:INFO:Total runtime is 0.31398691336313883 minutes
2024-08-31 13:49:49,472:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:49,472:INFO:Initializing create_model()
2024-08-31 13:49:49,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:49,473:INFO:Checking exceptions
2024-08-31 13:49:49,473:INFO:Importing libraries
2024-08-31 13:49:49,473:INFO:Copying training dataset
2024-08-31 13:49:49,483:INFO:Defining folds
2024-08-31 13:49:49,483:INFO:Declaring metric variables
2024-08-31 13:49:49,486:INFO:Importing untrained model
2024-08-31 13:49:49,489:INFO:Naive Bayes Imported successfully
2024-08-31 13:49:49,498:INFO:Starting cross validation
2024-08-31 13:49:49,499:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:49,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,756:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,772:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,941:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,961:INFO:Calculating mean and std
2024-08-31 13:49:49,963:INFO:Creating metrics dataframe
2024-08-31 13:49:49,963:INFO:Uploading results into container
2024-08-31 13:49:49,964:INFO:Uploading model into container now
2024-08-31 13:49:49,964:INFO:_master_model_container: 3
2024-08-31 13:49:49,965:INFO:_display_container: 2
2024-08-31 13:49:49,965:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:49:49,965:INFO:create_model() successfully completed......................................
2024-08-31 13:49:50,103:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:50,104:INFO:Creating metrics dataframe
2024-08-31 13:49:50,113:INFO:Initializing Decision Tree Classifier
2024-08-31 13:49:50,113:INFO:Total runtime is 0.324717378616333 minutes
2024-08-31 13:49:50,116:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:50,116:INFO:Initializing create_model()
2024-08-31 13:49:50,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:50,116:INFO:Checking exceptions
2024-08-31 13:49:50,116:INFO:Importing libraries
2024-08-31 13:49:50,116:INFO:Copying training dataset
2024-08-31 13:49:50,129:INFO:Defining folds
2024-08-31 13:49:50,129:INFO:Declaring metric variables
2024-08-31 13:49:50,133:INFO:Importing untrained model
2024-08-31 13:49:50,137:INFO:Decision Tree Classifier Imported successfully
2024-08-31 13:49:50,147:INFO:Starting cross validation
2024-08-31 13:49:50,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:50,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,439:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,448:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,666:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,676:INFO:Calculating mean and std
2024-08-31 13:49:50,678:INFO:Creating metrics dataframe
2024-08-31 13:49:50,680:INFO:Uploading results into container
2024-08-31 13:49:50,680:INFO:Uploading model into container now
2024-08-31 13:49:50,681:INFO:_master_model_container: 4
2024-08-31 13:49:50,681:INFO:_display_container: 2
2024-08-31 13:49:50,682:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-08-31 13:49:50,682:INFO:create_model() successfully completed......................................
2024-08-31 13:49:50,831:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:50,831:INFO:Creating metrics dataframe
2024-08-31 13:49:50,841:INFO:Initializing SVM - Linear Kernel
2024-08-31 13:49:50,841:INFO:Total runtime is 0.3368482788403829 minutes
2024-08-31 13:49:50,843:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:50,844:INFO:Initializing create_model()
2024-08-31 13:49:50,844:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:50,844:INFO:Checking exceptions
2024-08-31 13:49:50,844:INFO:Importing libraries
2024-08-31 13:49:50,844:INFO:Copying training dataset
2024-08-31 13:49:50,856:INFO:Defining folds
2024-08-31 13:49:50,856:INFO:Declaring metric variables
2024-08-31 13:49:50,861:INFO:Importing untrained model
2024-08-31 13:49:50,866:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 13:49:50,874:INFO:Starting cross validation
2024-08-31 13:49:50,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:51,368:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,445:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,728:INFO:Calculating mean and std
2024-08-31 13:49:51,730:INFO:Creating metrics dataframe
2024-08-31 13:49:51,731:INFO:Uploading results into container
2024-08-31 13:49:51,732:INFO:Uploading model into container now
2024-08-31 13:49:51,733:INFO:_master_model_container: 5
2024-08-31 13:49:51,733:INFO:_display_container: 2
2024-08-31 13:49:51,734:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 13:49:51,734:INFO:create_model() successfully completed......................................
2024-08-31 13:49:51,879:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:51,879:INFO:Creating metrics dataframe
2024-08-31 13:49:51,895:INFO:Initializing Ridge Classifier
2024-08-31 13:49:51,896:INFO:Total runtime is 0.35444366931915283 minutes
2024-08-31 13:49:51,904:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:51,904:INFO:Initializing create_model()
2024-08-31 13:49:51,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:51,905:INFO:Checking exceptions
2024-08-31 13:49:51,905:INFO:Importing libraries
2024-08-31 13:49:51,905:INFO:Copying training dataset
2024-08-31 13:49:51,958:INFO:Defining folds
2024-08-31 13:49:51,960:INFO:Declaring metric variables
2024-08-31 13:49:51,969:INFO:Importing untrained model
2024-08-31 13:49:51,987:INFO:Ridge Classifier Imported successfully
2024-08-31 13:49:52,002:INFO:Starting cross validation
2024-08-31 13:49:52,006:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:52,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,312:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,315:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,318:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,329:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,336:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,523:INFO:Calculating mean and std
2024-08-31 13:49:52,524:INFO:Creating metrics dataframe
2024-08-31 13:49:52,526:INFO:Uploading results into container
2024-08-31 13:49:52,527:INFO:Uploading model into container now
2024-08-31 13:49:52,527:INFO:_master_model_container: 6
2024-08-31 13:49:52,527:INFO:_display_container: 2
2024-08-31 13:49:52,527:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-08-31 13:49:52,527:INFO:create_model() successfully completed......................................
2024-08-31 13:49:52,667:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:52,667:INFO:Creating metrics dataframe
2024-08-31 13:49:52,676:INFO:Initializing Random Forest Classifier
2024-08-31 13:49:52,676:INFO:Total runtime is 0.36743351221084597 minutes
2024-08-31 13:49:52,679:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:52,679:INFO:Initializing create_model()
2024-08-31 13:49:52,680:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:52,680:INFO:Checking exceptions
2024-08-31 13:49:52,680:INFO:Importing libraries
2024-08-31 13:49:52,680:INFO:Copying training dataset
2024-08-31 13:49:52,691:INFO:Defining folds
2024-08-31 13:49:52,691:INFO:Declaring metric variables
2024-08-31 13:49:52,694:INFO:Importing untrained model
2024-08-31 13:49:52,699:INFO:Random Forest Classifier Imported successfully
2024-08-31 13:49:52,709:INFO:Starting cross validation
2024-08-31 13:49:52,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:53,992:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:53,992:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:53,997:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,047:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-08-31 13:49:54,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,068:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,073:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,077:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,103:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,519:INFO:Calculating mean and std
2024-08-31 13:49:54,520:INFO:Creating metrics dataframe
2024-08-31 13:49:54,522:INFO:Uploading results into container
2024-08-31 13:49:54,522:INFO:Uploading model into container now
2024-08-31 13:49:54,523:INFO:_master_model_container: 7
2024-08-31 13:49:54,523:INFO:_display_container: 2
2024-08-31 13:49:54,523:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-08-31 13:49:54,523:INFO:create_model() successfully completed......................................
2024-08-31 13:49:54,663:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:54,663:INFO:Creating metrics dataframe
2024-08-31 13:49:54,674:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 13:49:54,674:INFO:Total runtime is 0.40073900222778325 minutes
2024-08-31 13:49:54,677:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:54,678:INFO:Initializing create_model()
2024-08-31 13:49:54,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:54,678:INFO:Checking exceptions
2024-08-31 13:49:54,678:INFO:Importing libraries
2024-08-31 13:49:54,678:INFO:Copying training dataset
2024-08-31 13:49:54,690:INFO:Defining folds
2024-08-31 13:49:54,690:INFO:Declaring metric variables
2024-08-31 13:49:54,693:INFO:Importing untrained model
2024-08-31 13:49:54,700:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 13:49:54,707:INFO:Starting cross validation
2024-08-31 13:49:54,709:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:54,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,003:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,008:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,008:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,059:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,216:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,217:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,220:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,221:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,223:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,233:INFO:Calculating mean and std
2024-08-31 13:49:55,234:INFO:Creating metrics dataframe
2024-08-31 13:49:55,236:INFO:Uploading results into container
2024-08-31 13:49:55,237:INFO:Uploading model into container now
2024-08-31 13:49:55,237:INFO:_master_model_container: 8
2024-08-31 13:49:55,237:INFO:_display_container: 2
2024-08-31 13:49:55,237:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 13:49:55,238:INFO:create_model() successfully completed......................................
2024-08-31 13:49:55,376:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:55,376:INFO:Creating metrics dataframe
2024-08-31 13:49:55,386:INFO:Initializing Ada Boost Classifier
2024-08-31 13:49:55,386:INFO:Total runtime is 0.4125955939292908 minutes
2024-08-31 13:49:55,389:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:55,389:INFO:Initializing create_model()
2024-08-31 13:49:55,389:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:55,389:INFO:Checking exceptions
2024-08-31 13:49:55,390:INFO:Importing libraries
2024-08-31 13:49:55,390:INFO:Copying training dataset
2024-08-31 13:49:55,401:INFO:Defining folds
2024-08-31 13:49:55,401:INFO:Declaring metric variables
2024-08-31 13:49:55,406:INFO:Importing untrained model
2024-08-31 13:49:55,410:INFO:Ada Boost Classifier Imported successfully
2024-08-31 13:49:55,419:INFO:Starting cross validation
2024-08-31 13:49:55,422:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:55,606:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,613:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,621:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,659:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,665:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,679:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:56,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,840:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,850:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,869:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:56,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:57,677:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:57,680:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,682:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:57,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,685:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:57,689:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:57,694:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,702:INFO:Calculating mean and std
2024-08-31 13:49:57,703:INFO:Creating metrics dataframe
2024-08-31 13:49:57,706:INFO:Uploading results into container
2024-08-31 13:49:57,706:INFO:Uploading model into container now
2024-08-31 13:49:57,707:INFO:_master_model_container: 9
2024-08-31 13:49:57,707:INFO:_display_container: 2
2024-08-31 13:49:57,707:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-08-31 13:49:57,707:INFO:create_model() successfully completed......................................
2024-08-31 13:49:57,844:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:57,844:INFO:Creating metrics dataframe
2024-08-31 13:49:57,854:INFO:Initializing Gradient Boosting Classifier
2024-08-31 13:49:57,854:INFO:Total runtime is 0.45373980998992924 minutes
2024-08-31 13:49:57,857:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:57,857:INFO:Initializing create_model()
2024-08-31 13:49:57,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:57,857:INFO:Checking exceptions
2024-08-31 13:49:57,857:INFO:Importing libraries
2024-08-31 13:49:57,857:INFO:Copying training dataset
2024-08-31 13:49:57,868:INFO:Defining folds
2024-08-31 13:49:57,868:INFO:Declaring metric variables
2024-08-31 13:49:57,872:INFO:Importing untrained model
2024-08-31 13:49:57,877:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 13:49:57,887:INFO:Starting cross validation
2024-08-31 13:49:57,889:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:04,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,595:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,599:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,610:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,677:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,212:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,246:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,266:INFO:Calculating mean and std
2024-08-31 13:50:09,268:INFO:Creating metrics dataframe
2024-08-31 13:50:09,271:INFO:Uploading results into container
2024-08-31 13:50:09,272:INFO:Uploading model into container now
2024-08-31 13:50:09,272:INFO:_master_model_container: 10
2024-08-31 13:50:09,272:INFO:_display_container: 2
2024-08-31 13:50:09,273:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 13:50:09,273:INFO:create_model() successfully completed......................................
2024-08-31 13:50:09,406:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:09,407:INFO:Creating metrics dataframe
2024-08-31 13:50:09,416:INFO:Initializing Linear Discriminant Analysis
2024-08-31 13:50:09,416:INFO:Total runtime is 0.6464404980341594 minutes
2024-08-31 13:50:09,419:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:09,420:INFO:Initializing create_model()
2024-08-31 13:50:09,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:09,420:INFO:Checking exceptions
2024-08-31 13:50:09,420:INFO:Importing libraries
2024-08-31 13:50:09,421:INFO:Copying training dataset
2024-08-31 13:50:09,432:INFO:Defining folds
2024-08-31 13:50:09,432:INFO:Declaring metric variables
2024-08-31 13:50:09,437:INFO:Importing untrained model
2024-08-31 13:50:09,442:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 13:50:09,450:INFO:Starting cross validation
2024-08-31 13:50:09,452:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:09,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,828:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,861:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,877:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,887:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,998:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:10,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,014:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,029:INFO:Calculating mean and std
2024-08-31 13:50:10,030:INFO:Creating metrics dataframe
2024-08-31 13:50:10,031:INFO:Uploading results into container
2024-08-31 13:50:10,032:INFO:Uploading model into container now
2024-08-31 13:50:10,032:INFO:_master_model_container: 11
2024-08-31 13:50:10,033:INFO:_display_container: 2
2024-08-31 13:50:10,033:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 13:50:10,034:INFO:create_model() successfully completed......................................
2024-08-31 13:50:10,163:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:10,163:INFO:Creating metrics dataframe
2024-08-31 13:50:10,174:INFO:Initializing Extra Trees Classifier
2024-08-31 13:50:10,174:INFO:Total runtime is 0.6590776006380717 minutes
2024-08-31 13:50:10,177:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:10,177:INFO:Initializing create_model()
2024-08-31 13:50:10,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:10,177:INFO:Checking exceptions
2024-08-31 13:50:10,177:INFO:Importing libraries
2024-08-31 13:50:10,178:INFO:Copying training dataset
2024-08-31 13:50:10,190:INFO:Defining folds
2024-08-31 13:50:10,190:INFO:Declaring metric variables
2024-08-31 13:50:10,192:INFO:Importing untrained model
2024-08-31 13:50:10,196:INFO:Extra Trees Classifier Imported successfully
2024-08-31 13:50:10,204:INFO:Starting cross validation
2024-08-31 13:50:10,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:10,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,928:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,052:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,077:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,118:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,508:INFO:Calculating mean and std
2024-08-31 13:50:11,509:INFO:Creating metrics dataframe
2024-08-31 13:50:11,511:INFO:Uploading results into container
2024-08-31 13:50:11,512:INFO:Uploading model into container now
2024-08-31 13:50:11,512:INFO:_master_model_container: 12
2024-08-31 13:50:11,512:INFO:_display_container: 2
2024-08-31 13:50:11,513:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-08-31 13:50:11,513:INFO:create_model() successfully completed......................................
2024-08-31 13:50:11,656:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:11,656:INFO:Creating metrics dataframe
2024-08-31 13:50:11,666:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 13:50:11,667:INFO:Total runtime is 0.6839614311854045 minutes
2024-08-31 13:50:11,671:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:11,671:INFO:Initializing create_model()
2024-08-31 13:50:11,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:11,671:INFO:Checking exceptions
2024-08-31 13:50:11,671:INFO:Importing libraries
2024-08-31 13:50:11,671:INFO:Copying training dataset
2024-08-31 13:50:11,682:INFO:Defining folds
2024-08-31 13:50:11,684:INFO:Declaring metric variables
2024-08-31 13:50:11,688:INFO:Importing untrained model
2024-08-31 13:50:11,692:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 13:50:11,701:INFO:Starting cross validation
2024-08-31 13:50:11,704:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:13,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,874:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,887:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,898:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,059:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,130:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,143:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,429:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,986:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,998:INFO:Calculating mean and std
2024-08-31 13:50:14,999:INFO:Creating metrics dataframe
2024-08-31 13:50:15,002:INFO:Uploading results into container
2024-08-31 13:50:15,003:INFO:Uploading model into container now
2024-08-31 13:50:15,003:INFO:_master_model_container: 13
2024-08-31 13:50:15,003:INFO:_display_container: 2
2024-08-31 13:50:15,004:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 13:50:15,004:INFO:create_model() successfully completed......................................
2024-08-31 13:50:15,150:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:15,150:INFO:Creating metrics dataframe
2024-08-31 13:50:15,160:INFO:Initializing Dummy Classifier
2024-08-31 13:50:15,161:INFO:Total runtime is 0.7421804189682008 minutes
2024-08-31 13:50:15,164:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:15,165:INFO:Initializing create_model()
2024-08-31 13:50:15,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:15,165:INFO:Checking exceptions
2024-08-31 13:50:15,165:INFO:Importing libraries
2024-08-31 13:50:15,165:INFO:Copying training dataset
2024-08-31 13:50:15,174:INFO:Defining folds
2024-08-31 13:50:15,174:INFO:Declaring metric variables
2024-08-31 13:50:15,177:INFO:Importing untrained model
2024-08-31 13:50:15,181:INFO:Dummy Classifier Imported successfully
2024-08-31 13:50:15,187:INFO:Starting cross validation
2024-08-31 13:50:15,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:15,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,441:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,448:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,486:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,608:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,615:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,620:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,622:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,624:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,634:INFO:Calculating mean and std
2024-08-31 13:50:15,635:INFO:Creating metrics dataframe
2024-08-31 13:50:15,636:INFO:Uploading results into container
2024-08-31 13:50:15,637:INFO:Uploading model into container now
2024-08-31 13:50:15,637:INFO:_master_model_container: 14
2024-08-31 13:50:15,637:INFO:_display_container: 2
2024-08-31 13:50:15,637:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-08-31 13:50:15,637:INFO:create_model() successfully completed......................................
2024-08-31 13:50:15,777:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:15,777:INFO:Creating metrics dataframe
2024-08-31 13:50:15,801:INFO:Initializing create_model()
2024-08-31 13:50:15,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:15,802:INFO:Checking exceptions
2024-08-31 13:50:15,803:INFO:Importing libraries
2024-08-31 13:50:15,803:INFO:Copying training dataset
2024-08-31 13:50:15,816:INFO:Defining folds
2024-08-31 13:50:15,816:INFO:Declaring metric variables
2024-08-31 13:50:15,816:INFO:Importing untrained model
2024-08-31 13:50:15,816:INFO:Declaring custom model
2024-08-31 13:50:15,817:INFO:Naive Bayes Imported successfully
2024-08-31 13:50:15,818:INFO:Cross validation set to False
2024-08-31 13:50:15,818:INFO:Fitting Model
2024-08-31 13:50:15,910:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:50:15,910:INFO:create_model() successfully completed......................................
2024-08-31 13:50:16,081:INFO:_master_model_container: 14
2024-08-31 13:50:16,083:INFO:_display_container: 2
2024-08-31 13:50:16,083:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:50:16,083:INFO:compare_models() successfully completed......................................
2024-08-31 13:50:48,594:INFO:Initializing plot_model()
2024-08-31 13:50:48,594:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:50:48,594:INFO:Checking exceptions
2024-08-31 13:50:48,606:INFO:Preloading libraries
2024-08-31 13:50:48,607:INFO:Copying training dataset
2024-08-31 13:50:48,607:INFO:Plot type: class_report
2024-08-31 13:50:49,156:INFO:Fitting Model
2024-08-31 13:50:49,157:INFO:Scoring test/hold-out set
2024-08-31 13:50:49,447:INFO:Visual Rendered Successfully
2024-08-31 13:50:49,660:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:11,425:INFO:Initializing plot_model()
2024-08-31 13:51:11,425:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:11,425:INFO:Checking exceptions
2024-08-31 13:51:11,435:INFO:Preloading libraries
2024-08-31 13:51:11,436:INFO:Copying training dataset
2024-08-31 13:51:11,436:INFO:Plot type: confusion_matrix
2024-08-31 13:51:11,776:INFO:Fitting Model
2024-08-31 13:51:11,777:INFO:Scoring test/hold-out set
2024-08-31 13:51:11,940:INFO:Visual Rendered Successfully
2024-08-31 13:51:12,102:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:23,536:INFO:Initializing plot_model()
2024-08-31 13:51:23,536:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:23,536:INFO:Checking exceptions
2024-08-31 13:51:23,541:INFO:Preloading libraries
2024-08-31 13:51:23,542:INFO:Copying training dataset
2024-08-31 13:51:23,542:INFO:Plot type: auc
2024-08-31 13:51:23,771:INFO:Fitting Model
2024-08-31 13:51:23,771:INFO:Scoring test/hold-out set
2024-08-31 13:51:23,937:INFO:Visual Rendered Successfully
2024-08-31 13:51:24,109:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:33,796:INFO:Initializing plot_model()
2024-08-31 13:51:33,796:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:33,796:INFO:Checking exceptions
2024-08-31 13:51:33,802:INFO:Preloading libraries
2024-08-31 13:51:33,803:INFO:Copying training dataset
2024-08-31 13:51:33,803:INFO:Plot type: learning
2024-08-31 13:51:34,101:INFO:Fitting Model
2024-08-31 13:51:34,925:INFO:Visual Rendered Successfully
2024-08-31 13:51:35,050:INFO:plot_model() successfully completed......................................
2024-08-31 13:52:13,459:INFO:Initializing compare_models()
2024-08-31 13:52:13,459:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Cost-Benefit, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Cost-Benefit', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 13:52:13,460:INFO:Checking exceptions
2024-08-31 13:52:13,466:INFO:Preparing display monitor
2024-08-31 13:52:13,496:INFO:Initializing Logistic Regression
2024-08-31 13:52:13,496:INFO:Total runtime is 0.0 minutes
2024-08-31 13:52:13,503:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:13,504:INFO:Initializing create_model()
2024-08-31 13:52:13,504:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:13,504:INFO:Checking exceptions
2024-08-31 13:52:13,504:INFO:Importing libraries
2024-08-31 13:52:13,504:INFO:Copying training dataset
2024-08-31 13:52:13,521:INFO:Defining folds
2024-08-31 13:52:13,521:INFO:Declaring metric variables
2024-08-31 13:52:13,524:INFO:Importing untrained model
2024-08-31 13:52:13,535:INFO:Logistic Regression Imported successfully
2024-08-31 13:52:13,563:INFO:Starting cross validation
2024-08-31 13:52:13,569:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:16,222:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,352:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,371:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,379:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,439:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,441:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,646:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:17,656:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:17,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:17,676:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:17,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:17,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,698:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,703:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,714:INFO:Calculating mean and std
2024-08-31 13:52:17,716:INFO:Creating metrics dataframe
2024-08-31 13:52:17,717:INFO:Uploading results into container
2024-08-31 13:52:17,718:INFO:Uploading model into container now
2024-08-31 13:52:17,718:INFO:_master_model_container: 15
2024-08-31 13:52:17,718:INFO:_display_container: 3
2024-08-31 13:52:17,718:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:17,718:INFO:create_model() successfully completed......................................
2024-08-31 13:52:17,848:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:17,849:INFO:Creating metrics dataframe
2024-08-31 13:52:17,855:INFO:Initializing K Neighbors Classifier
2024-08-31 13:52:17,856:INFO:Total runtime is 0.07266997893651327 minutes
2024-08-31 13:52:17,859:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:17,859:INFO:Initializing create_model()
2024-08-31 13:52:17,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:17,860:INFO:Checking exceptions
2024-08-31 13:52:17,860:INFO:Importing libraries
2024-08-31 13:52:17,860:INFO:Copying training dataset
2024-08-31 13:52:17,872:INFO:Defining folds
2024-08-31 13:52:17,872:INFO:Declaring metric variables
2024-08-31 13:52:17,875:INFO:Importing untrained model
2024-08-31 13:52:17,878:INFO:K Neighbors Classifier Imported successfully
2024-08-31 13:52:17,889:INFO:Starting cross validation
2024-08-31 13:52:17,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:18,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-08-31 13:52:18,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,549:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,559:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,850:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,852:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,869:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,871:INFO:Calculating mean and std
2024-08-31 13:52:18,872:INFO:Creating metrics dataframe
2024-08-31 13:52:18,874:INFO:Uploading results into container
2024-08-31 13:52:18,874:INFO:Uploading model into container now
2024-08-31 13:52:18,875:INFO:_master_model_container: 16
2024-08-31 13:52:18,875:INFO:_display_container: 3
2024-08-31 13:52:18,876:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 13:52:18,876:INFO:create_model() successfully completed......................................
2024-08-31 13:52:19,058:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:19,058:INFO:Creating metrics dataframe
2024-08-31 13:52:19,073:INFO:Initializing Naive Bayes
2024-08-31 13:52:19,075:INFO:Total runtime is 0.09298098882039388 minutes
2024-08-31 13:52:19,080:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:19,081:INFO:Initializing create_model()
2024-08-31 13:52:19,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:19,081:INFO:Checking exceptions
2024-08-31 13:52:19,081:INFO:Importing libraries
2024-08-31 13:52:19,081:INFO:Copying training dataset
2024-08-31 13:52:19,100:INFO:Defining folds
2024-08-31 13:52:19,100:INFO:Declaring metric variables
2024-08-31 13:52:19,104:INFO:Importing untrained model
2024-08-31 13:52:19,107:INFO:Naive Bayes Imported successfully
2024-08-31 13:52:19,117:INFO:Starting cross validation
2024-08-31 13:52:19,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:19,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,479:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,483:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,615:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,620:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,637:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,650:INFO:Calculating mean and std
2024-08-31 13:52:19,651:INFO:Creating metrics dataframe
2024-08-31 13:52:19,653:INFO:Uploading results into container
2024-08-31 13:52:19,654:INFO:Uploading model into container now
2024-08-31 13:52:19,654:INFO:_master_model_container: 17
2024-08-31 13:52:19,654:INFO:_display_container: 3
2024-08-31 13:52:19,654:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:52:19,654:INFO:create_model() successfully completed......................................
2024-08-31 13:52:19,787:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:19,787:INFO:Creating metrics dataframe
2024-08-31 13:52:19,798:INFO:Initializing Decision Tree Classifier
2024-08-31 13:52:19,798:INFO:Total runtime is 0.10503343343734742 minutes
2024-08-31 13:52:19,802:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:19,802:INFO:Initializing create_model()
2024-08-31 13:52:19,802:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:19,802:INFO:Checking exceptions
2024-08-31 13:52:19,802:INFO:Importing libraries
2024-08-31 13:52:19,802:INFO:Copying training dataset
2024-08-31 13:52:19,813:INFO:Defining folds
2024-08-31 13:52:19,814:INFO:Declaring metric variables
2024-08-31 13:52:19,816:INFO:Importing untrained model
2024-08-31 13:52:19,820:INFO:Decision Tree Classifier Imported successfully
2024-08-31 13:52:19,828:INFO:Starting cross validation
2024-08-31 13:52:19,830:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:20,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,103:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,105:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,116:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,117:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,121:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,122:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,127:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,186:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,204:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,331:INFO:Calculating mean and std
2024-08-31 13:52:20,332:INFO:Creating metrics dataframe
2024-08-31 13:52:20,336:INFO:Uploading results into container
2024-08-31 13:52:20,336:INFO:Uploading model into container now
2024-08-31 13:52:20,337:INFO:_master_model_container: 18
2024-08-31 13:52:20,337:INFO:_display_container: 3
2024-08-31 13:52:20,337:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-08-31 13:52:20,337:INFO:create_model() successfully completed......................................
2024-08-31 13:52:20,492:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:20,492:INFO:Creating metrics dataframe
2024-08-31 13:52:20,504:INFO:Initializing SVM - Linear Kernel
2024-08-31 13:52:20,504:INFO:Total runtime is 0.11679850816726685 minutes
2024-08-31 13:52:20,506:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:20,507:INFO:Initializing create_model()
2024-08-31 13:52:20,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:20,507:INFO:Checking exceptions
2024-08-31 13:52:20,507:INFO:Importing libraries
2024-08-31 13:52:20,507:INFO:Copying training dataset
2024-08-31 13:52:20,520:INFO:Defining folds
2024-08-31 13:52:20,520:INFO:Declaring metric variables
2024-08-31 13:52:20,523:INFO:Importing untrained model
2024-08-31 13:52:20,530:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 13:52:20,540:INFO:Starting cross validation
2024-08-31 13:52:20,542:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:21,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,114:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,190:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,237:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,247:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,249:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,250:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,446:INFO:Calculating mean and std
2024-08-31 13:52:21,447:INFO:Creating metrics dataframe
2024-08-31 13:52:21,448:INFO:Uploading results into container
2024-08-31 13:52:21,449:INFO:Uploading model into container now
2024-08-31 13:52:21,449:INFO:_master_model_container: 19
2024-08-31 13:52:21,450:INFO:_display_container: 3
2024-08-31 13:52:21,450:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 13:52:21,450:INFO:create_model() successfully completed......................................
2024-08-31 13:52:21,629:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:21,629:INFO:Creating metrics dataframe
2024-08-31 13:52:21,643:INFO:Initializing Ridge Classifier
2024-08-31 13:52:21,644:INFO:Total runtime is 0.13580156962076823 minutes
2024-08-31 13:52:21,652:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:21,653:INFO:Initializing create_model()
2024-08-31 13:52:21,653:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:21,653:INFO:Checking exceptions
2024-08-31 13:52:21,654:INFO:Importing libraries
2024-08-31 13:52:21,654:INFO:Copying training dataset
2024-08-31 13:52:21,669:INFO:Defining folds
2024-08-31 13:52:21,669:INFO:Declaring metric variables
2024-08-31 13:52:21,673:INFO:Importing untrained model
2024-08-31 13:52:21,678:INFO:Ridge Classifier Imported successfully
2024-08-31 13:52:21,686:INFO:Starting cross validation
2024-08-31 13:52:21,688:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:21,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,989:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,000:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,000:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,047:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,063:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,197:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,202:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,204:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,216:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,229:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,231:INFO:Calculating mean and std
2024-08-31 13:52:22,232:INFO:Creating metrics dataframe
2024-08-31 13:52:22,233:INFO:Uploading results into container
2024-08-31 13:52:22,234:INFO:Uploading model into container now
2024-08-31 13:52:22,235:INFO:_master_model_container: 20
2024-08-31 13:52:22,235:INFO:_display_container: 3
2024-08-31 13:52:22,235:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-08-31 13:52:22,235:INFO:create_model() successfully completed......................................
2024-08-31 13:52:22,368:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:22,370:INFO:Creating metrics dataframe
2024-08-31 13:52:22,382:INFO:Initializing Random Forest Classifier
2024-08-31 13:52:22,382:INFO:Total runtime is 0.1481105923652649 minutes
2024-08-31 13:52:22,386:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:22,387:INFO:Initializing create_model()
2024-08-31 13:52:22,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:22,387:INFO:Checking exceptions
2024-08-31 13:52:22,387:INFO:Importing libraries
2024-08-31 13:52:22,387:INFO:Copying training dataset
2024-08-31 13:52:22,397:INFO:Defining folds
2024-08-31 13:52:22,397:INFO:Declaring metric variables
2024-08-31 13:52:22,401:INFO:Importing untrained model
2024-08-31 13:52:22,405:INFO:Random Forest Classifier Imported successfully
2024-08-31 13:52:22,411:INFO:Starting cross validation
2024-08-31 13:52:22,414:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:23,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,837:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,849:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,855:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,863:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,866:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,281:INFO:Calculating mean and std
2024-08-31 13:52:24,282:INFO:Creating metrics dataframe
2024-08-31 13:52:24,283:INFO:Uploading results into container
2024-08-31 13:52:24,284:INFO:Uploading model into container now
2024-08-31 13:52:24,284:INFO:_master_model_container: 21
2024-08-31 13:52:24,284:INFO:_display_container: 3
2024-08-31 13:52:24,285:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-08-31 13:52:24,285:INFO:create_model() successfully completed......................................
2024-08-31 13:52:24,409:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:24,409:INFO:Creating metrics dataframe
2024-08-31 13:52:24,417:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 13:52:24,417:INFO:Total runtime is 0.18202539682388305 minutes
2024-08-31 13:52:24,421:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:24,421:INFO:Initializing create_model()
2024-08-31 13:52:24,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:24,421:INFO:Checking exceptions
2024-08-31 13:52:24,421:INFO:Importing libraries
2024-08-31 13:52:24,421:INFO:Copying training dataset
2024-08-31 13:52:24,432:INFO:Defining folds
2024-08-31 13:52:24,432:INFO:Declaring metric variables
2024-08-31 13:52:24,435:INFO:Importing untrained model
2024-08-31 13:52:24,438:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 13:52:24,445:INFO:Starting cross validation
2024-08-31 13:52:24,447:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:24,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,727:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,759:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,828:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,838:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,866:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,902:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,915:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,934:INFO:Calculating mean and std
2024-08-31 13:52:24,935:INFO:Creating metrics dataframe
2024-08-31 13:52:24,938:INFO:Uploading results into container
2024-08-31 13:52:24,938:INFO:Uploading model into container now
2024-08-31 13:52:24,939:INFO:_master_model_container: 22
2024-08-31 13:52:24,939:INFO:_display_container: 3
2024-08-31 13:52:24,939:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 13:52:24,940:INFO:create_model() successfully completed......................................
2024-08-31 13:52:25,072:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:25,072:INFO:Creating metrics dataframe
2024-08-31 13:52:25,081:INFO:Initializing Ada Boost Classifier
2024-08-31 13:52:25,081:INFO:Total runtime is 0.1930871605873108 minutes
2024-08-31 13:52:25,084:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:25,084:INFO:Initializing create_model()
2024-08-31 13:52:25,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:25,084:INFO:Checking exceptions
2024-08-31 13:52:25,084:INFO:Importing libraries
2024-08-31 13:52:25,084:INFO:Copying training dataset
2024-08-31 13:52:25,095:INFO:Defining folds
2024-08-31 13:52:25,095:INFO:Declaring metric variables
2024-08-31 13:52:25,098:INFO:Importing untrained model
2024-08-31 13:52:25,102:INFO:Ada Boost Classifier Imported successfully
2024-08-31 13:52:25,110:INFO:Starting cross validation
2024-08-31 13:52:25,111:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:25,293:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,326:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,424:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,567:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,573:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,575:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,578:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,593:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,597:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,598:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,612:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,616:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,670:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,676:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:27,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:27,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,479:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:27,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:27,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,516:INFO:Calculating mean and std
2024-08-31 13:52:27,516:INFO:Creating metrics dataframe
2024-08-31 13:52:27,518:INFO:Uploading results into container
2024-08-31 13:52:27,518:INFO:Uploading model into container now
2024-08-31 13:52:27,519:INFO:_master_model_container: 23
2024-08-31 13:52:27,519:INFO:_display_container: 3
2024-08-31 13:52:27,519:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-08-31 13:52:27,519:INFO:create_model() successfully completed......................................
2024-08-31 13:52:27,660:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:27,660:INFO:Creating metrics dataframe
2024-08-31 13:52:27,672:INFO:Initializing Gradient Boosting Classifier
2024-08-31 13:52:27,672:INFO:Total runtime is 0.23626784483591715 minutes
2024-08-31 13:52:27,676:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:27,676:INFO:Initializing create_model()
2024-08-31 13:52:27,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:27,676:INFO:Checking exceptions
2024-08-31 13:52:27,676:INFO:Importing libraries
2024-08-31 13:52:27,676:INFO:Copying training dataset
2024-08-31 13:52:27,685:INFO:Defining folds
2024-08-31 13:52:27,686:INFO:Declaring metric variables
2024-08-31 13:52:27,692:INFO:Importing untrained model
2024-08-31 13:52:27,698:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 13:52:27,709:INFO:Starting cross validation
2024-08-31 13:52:27,710:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:34,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,878:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,883:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,078:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,084:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,089:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,109:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:39,721:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:39,731:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:39,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:39,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,785:INFO:Calculating mean and std
2024-08-31 13:52:39,786:INFO:Creating metrics dataframe
2024-08-31 13:52:39,788:INFO:Uploading results into container
2024-08-31 13:52:39,788:INFO:Uploading model into container now
2024-08-31 13:52:39,788:INFO:_master_model_container: 24
2024-08-31 13:52:39,788:INFO:_display_container: 3
2024-08-31 13:52:39,789:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 13:52:39,789:INFO:create_model() successfully completed......................................
2024-08-31 13:52:39,908:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:39,909:INFO:Creating metrics dataframe
2024-08-31 13:52:39,921:INFO:Initializing Linear Discriminant Analysis
2024-08-31 13:52:39,921:INFO:Total runtime is 0.4404159188270569 minutes
2024-08-31 13:52:39,924:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:39,924:INFO:Initializing create_model()
2024-08-31 13:52:39,924:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:39,924:INFO:Checking exceptions
2024-08-31 13:52:39,924:INFO:Importing libraries
2024-08-31 13:52:39,924:INFO:Copying training dataset
2024-08-31 13:52:39,935:INFO:Defining folds
2024-08-31 13:52:39,935:INFO:Declaring metric variables
2024-08-31 13:52:39,938:INFO:Importing untrained model
2024-08-31 13:52:39,942:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 13:52:39,950:INFO:Starting cross validation
2024-08-31 13:52:39,952:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:40,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,221:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,222:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,228:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,232:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,234:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,240:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,245:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,246:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,278:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,312:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,413:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,430:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,438:INFO:Calculating mean and std
2024-08-31 13:52:40,439:INFO:Creating metrics dataframe
2024-08-31 13:52:40,442:INFO:Uploading results into container
2024-08-31 13:52:40,443:INFO:Uploading model into container now
2024-08-31 13:52:40,443:INFO:_master_model_container: 25
2024-08-31 13:52:40,443:INFO:_display_container: 3
2024-08-31 13:52:40,444:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 13:52:40,444:INFO:create_model() successfully completed......................................
2024-08-31 13:52:40,570:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:40,570:INFO:Creating metrics dataframe
2024-08-31 13:52:40,581:INFO:Initializing Extra Trees Classifier
2024-08-31 13:52:40,581:INFO:Total runtime is 0.45142091512680055 minutes
2024-08-31 13:52:40,585:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:40,585:INFO:Initializing create_model()
2024-08-31 13:52:40,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:40,585:INFO:Checking exceptions
2024-08-31 13:52:40,585:INFO:Importing libraries
2024-08-31 13:52:40,585:INFO:Copying training dataset
2024-08-31 13:52:40,597:INFO:Defining folds
2024-08-31 13:52:40,597:INFO:Declaring metric variables
2024-08-31 13:52:40,600:INFO:Importing untrained model
2024-08-31 13:52:40,603:INFO:Extra Trees Classifier Imported successfully
2024-08-31 13:52:40,610:INFO:Starting cross validation
2024-08-31 13:52:40,612:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:41,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,278:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,289:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,357:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,370:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,621:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,630:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,637:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,648:INFO:Calculating mean and std
2024-08-31 13:52:41,649:INFO:Creating metrics dataframe
2024-08-31 13:52:41,651:INFO:Uploading results into container
2024-08-31 13:52:41,652:INFO:Uploading model into container now
2024-08-31 13:52:41,652:INFO:_master_model_container: 26
2024-08-31 13:52:41,652:INFO:_display_container: 3
2024-08-31 13:52:41,652:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-08-31 13:52:41,652:INFO:create_model() successfully completed......................................
2024-08-31 13:52:41,796:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:41,797:INFO:Creating metrics dataframe
2024-08-31 13:52:41,813:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 13:52:41,813:INFO:Total runtime is 0.4719454884529114 minutes
2024-08-31 13:52:41,816:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:41,816:INFO:Initializing create_model()
2024-08-31 13:52:41,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:41,817:INFO:Checking exceptions
2024-08-31 13:52:41,817:INFO:Importing libraries
2024-08-31 13:52:41,817:INFO:Copying training dataset
2024-08-31 13:52:41,826:INFO:Defining folds
2024-08-31 13:52:41,826:INFO:Declaring metric variables
2024-08-31 13:52:41,830:INFO:Importing untrained model
2024-08-31 13:52:41,833:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 13:52:41,838:INFO:Starting cross validation
2024-08-31 13:52:41,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:43,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,116:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,120:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,319:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,532:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,636:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,645:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,675:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,698:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,350:INFO:Calculating mean and std
2024-08-31 13:52:45,352:INFO:Creating metrics dataframe
2024-08-31 13:52:45,355:INFO:Uploading results into container
2024-08-31 13:52:45,356:INFO:Uploading model into container now
2024-08-31 13:52:45,356:INFO:_master_model_container: 27
2024-08-31 13:52:45,356:INFO:_display_container: 3
2024-08-31 13:52:45,357:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 13:52:45,357:INFO:create_model() successfully completed......................................
2024-08-31 13:52:45,504:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:45,505:INFO:Creating metrics dataframe
2024-08-31 13:52:45,514:INFO:Initializing Dummy Classifier
2024-08-31 13:52:45,514:INFO:Total runtime is 0.5336297750473022 minutes
2024-08-31 13:52:45,517:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:45,517:INFO:Initializing create_model()
2024-08-31 13:52:45,517:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:45,517:INFO:Checking exceptions
2024-08-31 13:52:45,517:INFO:Importing libraries
2024-08-31 13:52:45,517:INFO:Copying training dataset
2024-08-31 13:52:45,527:INFO:Defining folds
2024-08-31 13:52:45,528:INFO:Declaring metric variables
2024-08-31 13:52:45,530:INFO:Importing untrained model
2024-08-31 13:52:45,534:INFO:Dummy Classifier Imported successfully
2024-08-31 13:52:45,541:INFO:Starting cross validation
2024-08-31 13:52:45,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:45,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,841:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,844:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,844:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,847:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,857:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,861:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,974:INFO:Calculating mean and std
2024-08-31 13:52:45,975:INFO:Creating metrics dataframe
2024-08-31 13:52:45,979:INFO:Uploading results into container
2024-08-31 13:52:45,980:INFO:Uploading model into container now
2024-08-31 13:52:45,981:INFO:_master_model_container: 28
2024-08-31 13:52:45,981:INFO:_display_container: 3
2024-08-31 13:52:45,981:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-08-31 13:52:45,981:INFO:create_model() successfully completed......................................
2024-08-31 13:52:46,107:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:46,107:INFO:Creating metrics dataframe
2024-08-31 13:52:46,125:INFO:Initializing create_model()
2024-08-31 13:52:46,125:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:46,125:INFO:Checking exceptions
2024-08-31 13:52:46,127:INFO:Importing libraries
2024-08-31 13:52:46,127:INFO:Copying training dataset
2024-08-31 13:52:46,137:INFO:Defining folds
2024-08-31 13:52:46,137:INFO:Declaring metric variables
2024-08-31 13:52:46,137:INFO:Importing untrained model
2024-08-31 13:52:46,137:INFO:Declaring custom model
2024-08-31 13:52:46,137:INFO:Logistic Regression Imported successfully
2024-08-31 13:52:46,139:INFO:Cross validation set to False
2024-08-31 13:52:46,139:INFO:Fitting Model
2024-08-31 13:52:47,386:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:47,386:INFO:create_model() successfully completed......................................
2024-08-31 13:52:47,570:INFO:_master_model_container: 28
2024-08-31 13:52:47,571:INFO:_display_container: 3
2024-08-31 13:52:47,571:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:47,571:INFO:compare_models() successfully completed......................................
2024-08-31 13:52:47,609:INFO:Initializing plot_model()
2024-08-31 13:52:47,609:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:52:47,609:INFO:Checking exceptions
2024-08-31 13:52:47,615:INFO:Preloading libraries
2024-08-31 13:52:47,616:INFO:Copying training dataset
2024-08-31 13:52:47,616:INFO:Plot type: class_report
2024-08-31 13:52:47,860:INFO:Fitting Model
2024-08-31 13:52:47,861:INFO:Scoring test/hold-out set
2024-08-31 13:52:48,063:INFO:Visual Rendered Successfully
2024-08-31 13:52:48,194:INFO:plot_model() successfully completed......................................
2024-08-31 13:52:50,847:INFO:Initializing plot_model()
2024-08-31 13:52:50,847:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:52:50,847:INFO:Checking exceptions
2024-08-31 13:52:50,854:INFO:Preloading libraries
2024-08-31 13:52:50,855:INFO:Copying training dataset
2024-08-31 13:52:50,855:INFO:Plot type: confusion_matrix
2024-08-31 13:52:51,095:INFO:Fitting Model
2024-08-31 13:52:51,096:INFO:Scoring test/hold-out set
2024-08-31 13:52:51,227:INFO:Visual Rendered Successfully
2024-08-31 13:52:51,372:INFO:plot_model() successfully completed......................................
2024-08-31 13:53:02,652:INFO:Initializing plot_model()
2024-08-31 13:53:02,653:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:53:02,653:INFO:Checking exceptions
2024-08-31 13:53:02,660:INFO:Preloading libraries
2024-08-31 13:53:02,660:INFO:Copying training dataset
2024-08-31 13:53:02,661:INFO:Plot type: auc
2024-08-31 13:53:02,932:INFO:Fitting Model
2024-08-31 13:53:02,933:INFO:Scoring test/hold-out set
2024-08-31 13:53:03,220:INFO:Visual Rendered Successfully
2024-08-31 13:53:03,362:INFO:plot_model() successfully completed......................................
2024-08-31 13:53:11,618:INFO:Initializing plot_model()
2024-08-31 13:53:11,618:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:53:11,618:INFO:Checking exceptions
2024-08-31 13:53:11,625:INFO:Preloading libraries
2024-08-31 13:53:11,625:INFO:Copying training dataset
2024-08-31 13:53:11,625:INFO:Plot type: learning
2024-08-31 13:53:11,861:INFO:Fitting Model
2024-08-31 13:54:23,593:INFO:Visual Rendered Successfully
2024-08-31 13:54:23,727:INFO:plot_model() successfully completed......................................
2024-08-31 13:54:29,063:INFO:Initializing save_model()
2024-08-31 13:54:29,063:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=predictive_maintenance.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-08-31 13:54:29,064:INFO:Adding model into prep_pipe
2024-08-31 13:54:29,081:INFO:predictive_maintenance.pkl.pkl saved in current working directory
2024-08-31 13:54:29,089:INFO:Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]',
                                             'Target', 'operational_hours',
                                             'RUL', 'Vi...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=42,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-08-31 13:54:29,089:INFO:save_model() successfully completed......................................
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:08,516:INFO:PyCaret ClassificationExperiment
2024-09-01 00:27:08,516:INFO:Logging name: clf-default-name
2024-09-01 00:27:08,516:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-01 00:27:08,516:INFO:version 3.3.2
2024-09-01 00:27:08,517:INFO:Initializing setup()
2024-09-01 00:27:08,517:INFO:self.USI: 9902
2024-09-01 00:27:08,517:INFO:self._variable_keys: {'_available_plots', 'exp_id', 'gpu_n_jobs_param', 'X', 'fold_generator', 'fix_imbalance', 'X_test', 'USI', 'X_train', 'log_plots_param', 'target_param', 'fold_shuffle_param', 'n_jobs_param', 'gpu_param', 'idx', 'exp_name_log', 'y', 'y_train', 'data', 'is_multiclass', 'pipeline', '_ml_usecase', 'seed', 'logging_param', 'fold_groups_param', 'memory', 'y_test', 'html_param'}
2024-09-01 00:27:08,517:INFO:Checking environment
2024-09-01 00:27:08,517:INFO:python_version: 3.11.5
2024-09-01 00:27:08,517:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-09-01 00:27:08,517:INFO:machine: AMD64
2024-09-01 00:27:08,517:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-01 00:27:08,517:INFO:Memory: svmem(total=8269512704, available=1388056576, percent=83.2, used=6881456128, free=1388056576)
2024-09-01 00:27:08,517:INFO:Physical Core: 4
2024-09-01 00:27:08,517:INFO:Logical Core: 8
2024-09-01 00:27:08,517:INFO:Checking libraries
2024-09-01 00:27:08,517:INFO:System:
2024-09-01 00:27:08,517:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-09-01 00:27:08,517:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-09-01 00:27:08,517:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-01 00:27:08,517:INFO:PyCaret required dependencies:
2024-09-01 00:27:10,003:INFO:                 pip: 23.2.1
2024-09-01 00:27:10,003:INFO:          setuptools: 68.0.0
2024-09-01 00:27:10,003:INFO:             pycaret: 3.3.2
2024-09-01 00:27:10,003:INFO:             IPython: 8.15.0
2024-09-01 00:27:10,003:INFO:          ipywidgets: 8.0.4
2024-09-01 00:27:10,003:INFO:                tqdm: 4.65.0
2024-09-01 00:27:10,003:INFO:               numpy: 1.24.3
2024-09-01 00:27:10,003:INFO:              pandas: 2.0.3
2024-09-01 00:27:10,003:INFO:              jinja2: 3.1.2
2024-09-01 00:27:10,003:INFO:               scipy: 1.11.1
2024-09-01 00:27:10,003:INFO:              joblib: 1.2.0
2024-09-01 00:27:10,003:INFO:             sklearn: 1.4.2
2024-09-01 00:27:10,003:INFO:                pyod: 2.0.1
2024-09-01 00:27:10,003:INFO:            imblearn: 0.12.3
2024-09-01 00:27:10,003:INFO:   category_encoders: 2.6.3
2024-09-01 00:27:10,003:INFO:            lightgbm: 4.5.0
2024-09-01 00:27:10,003:INFO:               numba: 0.57.1
2024-09-01 00:27:10,004:INFO:            requests: 2.31.0
2024-09-01 00:27:10,004:INFO:          matplotlib: 3.7.2
2024-09-01 00:27:10,004:INFO:          scikitplot: 0.3.7
2024-09-01 00:27:10,004:INFO:         yellowbrick: 1.5
2024-09-01 00:27:10,004:INFO:              plotly: 5.24.0
2024-09-01 00:27:10,004:INFO:    plotly-resampler: Not installed
2024-09-01 00:27:10,004:INFO:             kaleido: 0.2.1
2024-09-01 00:27:10,004:INFO:           schemdraw: 0.15
2024-09-01 00:27:10,005:INFO:         statsmodels: 0.14.0
2024-09-01 00:27:10,005:INFO:              sktime: 0.26.0
2024-09-01 00:27:10,005:INFO:               tbats: 1.1.3
2024-09-01 00:27:10,005:INFO:            pmdarima: 2.0.4
2024-09-01 00:27:10,005:INFO:              psutil: 5.9.0
2024-09-01 00:27:10,005:INFO:          markupsafe: 2.1.1
2024-09-01 00:27:10,005:INFO:             pickle5: Not installed
2024-09-01 00:27:10,005:INFO:         cloudpickle: 2.2.1
2024-09-01 00:27:10,005:INFO:         deprecation: 2.1.0
2024-09-01 00:27:10,006:INFO:              xxhash: 2.0.2
2024-09-01 00:27:10,006:INFO:           wurlitzer: Not installed
2024-09-01 00:27:10,006:INFO:PyCaret optional dependencies:
2024-09-01 00:27:10,019:INFO:                shap: Not installed
2024-09-01 00:27:10,019:INFO:           interpret: Not installed
2024-09-01 00:27:10,019:INFO:                umap: Not installed
2024-09-01 00:27:10,019:INFO:     ydata_profiling: Not installed
2024-09-01 00:27:10,019:INFO:  explainerdashboard: Not installed
2024-09-01 00:27:10,019:INFO:             autoviz: Not installed
2024-09-01 00:27:10,019:INFO:           fairlearn: Not installed
2024-09-01 00:27:10,019:INFO:          deepchecks: Not installed
2024-09-01 00:27:10,019:INFO:             xgboost: Not installed
2024-09-01 00:27:10,020:INFO:            catboost: Not installed
2024-09-01 00:27:10,020:INFO:              kmodes: Not installed
2024-09-01 00:27:10,020:INFO:             mlxtend: 0.22.0
2024-09-01 00:27:10,020:INFO:       statsforecast: Not installed
2024-09-01 00:27:10,020:INFO:        tune_sklearn: Not installed
2024-09-01 00:27:10,020:INFO:                 ray: Not installed
2024-09-01 00:27:10,020:INFO:            hyperopt: Not installed
2024-09-01 00:27:10,020:INFO:              optuna: Not installed
2024-09-01 00:27:10,020:INFO:               skopt: Not installed
2024-09-01 00:27:10,020:INFO:              mlflow: Not installed
2024-09-01 00:27:10,020:INFO:              gradio: Not installed
2024-09-01 00:27:10,020:INFO:             fastapi: Not installed
2024-09-01 00:27:10,020:INFO:             uvicorn: Not installed
2024-09-01 00:27:10,020:INFO:              m2cgen: Not installed
2024-09-01 00:27:10,020:INFO:           evidently: Not installed
2024-09-01 00:27:10,020:INFO:               fugue: Not installed
2024-09-01 00:27:10,020:INFO:           streamlit: 1.38.0
2024-09-01 00:27:10,020:INFO:             prophet: Not installed
2024-09-01 00:27:10,020:INFO:None
2024-09-01 00:27:10,021:INFO:Set up data.
2024-09-01 00:27:10,045:INFO:Set up folding strategy.
2024-09-01 00:27:10,045:INFO:Set up train/test split.
2024-09-01 00:27:10,065:INFO:Set up index.
2024-09-01 00:27:10,066:INFO:Assigning column types.
2024-09-01 00:27:10,074:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-01 00:27:10,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,124:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,173:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,224:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,226:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,259:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,260:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-01 00:27:10,311:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,342:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,390:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,417:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,417:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,417:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-01 00:27:10,493:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,493:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,569:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,575:INFO:Preparing preprocessing pipeline...
2024-09-01 00:27:10,578:INFO:Set up label encoding.
2024-09-01 00:27:10,578:INFO:Set up simple imputation.
2024-09-01 00:27:10,582:INFO:Set up encoding of categorical features.
2024-09-01 00:27:10,583:INFO:Set up column name cleaning.
2024-09-01 00:27:10,746:INFO:Finished creating preprocessing pipeline.
2024-09-01 00:27:10,761:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-01 00:27:10,761:INFO:Creating final display dataframe.
2024-09-01 00:27:11,184:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16               Fold Generator   
17                  Fold Number   
18                     CPU Jobs   
19                      Use GPU   
20               Log Experiment   
21              Experiment Name   
22                          USI   

                                                Value  
0                                                  42  
1                                        Failure Type  
2                                          Multiclass  
3   Heat Dissipation Failure: 0, No Failure: 1, Ov...  
4                                          (6879, 21)  
5                                          (6879, 23)  
6                                          (4815, 23)  
7                                          (2064, 23)  
8                                                  18  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                    StratifiedKFold  
17                                                 10  
18                                                 -1  
19                                              False  
20                                              False  
21                                   clf-default-name  
22                                               9902  
2024-09-01 00:27:11,290:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,373:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,373:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,375:INFO:setup() successfully completed in 2.86s...............
2024-09-01 00:27:11,384:INFO:Initializing compare_models()
2024-09-01 00:27:11,384:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 00:27:11,385:INFO:Checking exceptions
2024-09-01 00:27:11,397:INFO:Preparing display monitor
2024-09-01 00:27:11,439:INFO:Initializing Logistic Regression
2024-09-01 00:27:11,439:INFO:Total runtime is 1.6748905181884766e-05 minutes
2024-09-01 00:27:11,445:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:11,446:INFO:Initializing create_model()
2024-09-01 00:27:11,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:11,446:INFO:Checking exceptions
2024-09-01 00:27:11,446:INFO:Importing libraries
2024-09-01 00:27:11,446:INFO:Copying training dataset
2024-09-01 00:27:11,457:INFO:Defining folds
2024-09-01 00:27:11,457:INFO:Declaring metric variables
2024-09-01 00:27:11,461:INFO:Importing untrained model
2024-09-01 00:27:11,466:INFO:Logistic Regression Imported successfully
2024-09-01 00:27:11,474:INFO:Starting cross validation
2024-09-01 00:27:11,476:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:21,237:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,274:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,565:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,689:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,692:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:22,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:22,650:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:22,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,662:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:22,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:22,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,703:INFO:Calculating mean and std
2024-09-01 00:27:22,705:INFO:Creating metrics dataframe
2024-09-01 00:27:22,707:INFO:Uploading results into container
2024-09-01 00:27:22,708:INFO:Uploading model into container now
2024-09-01 00:27:22,708:INFO:_master_model_container: 1
2024-09-01 00:27:22,708:INFO:_display_container: 2
2024-09-01 00:27:22,709:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:27:22,710:INFO:create_model() successfully completed......................................
2024-09-01 00:27:22,850:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:22,850:INFO:Creating metrics dataframe
2024-09-01 00:27:22,857:INFO:Initializing K Neighbors Classifier
2024-09-01 00:27:22,857:INFO:Total runtime is 0.19031020005544028 minutes
2024-09-01 00:27:22,861:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:22,862:INFO:Initializing create_model()
2024-09-01 00:27:22,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:22,863:INFO:Checking exceptions
2024-09-01 00:27:22,863:INFO:Importing libraries
2024-09-01 00:27:22,863:INFO:Copying training dataset
2024-09-01 00:27:22,872:INFO:Defining folds
2024-09-01 00:27:22,872:INFO:Declaring metric variables
2024-09-01 00:27:22,879:INFO:Importing untrained model
2024-09-01 00:27:22,883:INFO:K Neighbors Classifier Imported successfully
2024-09-01 00:27:22,889:INFO:Starting cross validation
2024-09-01 00:27:22,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:23,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,536:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,810:INFO:Calculating mean and std
2024-09-01 00:27:23,812:INFO:Creating metrics dataframe
2024-09-01 00:27:23,814:INFO:Uploading results into container
2024-09-01 00:27:23,815:INFO:Uploading model into container now
2024-09-01 00:27:23,815:INFO:_master_model_container: 2
2024-09-01 00:27:23,816:INFO:_display_container: 2
2024-09-01 00:27:23,817:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 00:27:23,817:INFO:create_model() successfully completed......................................
2024-09-01 00:27:23,945:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:23,945:INFO:Creating metrics dataframe
2024-09-01 00:27:23,954:INFO:Initializing Naive Bayes
2024-09-01 00:27:23,954:INFO:Total runtime is 0.20858569145202638 minutes
2024-09-01 00:27:23,957:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:23,958:INFO:Initializing create_model()
2024-09-01 00:27:23,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:23,960:INFO:Checking exceptions
2024-09-01 00:27:23,960:INFO:Importing libraries
2024-09-01 00:27:23,960:INFO:Copying training dataset
2024-09-01 00:27:24,012:INFO:Defining folds
2024-09-01 00:27:24,013:INFO:Declaring metric variables
2024-09-01 00:27:24,016:INFO:Importing untrained model
2024-09-01 00:27:24,020:INFO:Naive Bayes Imported successfully
2024-09-01 00:27:24,033:INFO:Starting cross validation
2024-09-01 00:27:24,035:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:24,290:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 00:27:24,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,340:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,474:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,488:INFO:Calculating mean and std
2024-09-01 00:27:24,489:INFO:Creating metrics dataframe
2024-09-01 00:27:24,493:INFO:Uploading results into container
2024-09-01 00:27:24,493:INFO:Uploading model into container now
2024-09-01 00:27:24,494:INFO:_master_model_container: 3
2024-09-01 00:27:24,494:INFO:_display_container: 2
2024-09-01 00:27:24,494:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 00:27:24,495:INFO:create_model() successfully completed......................................
2024-09-01 00:27:24,619:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:24,619:INFO:Creating metrics dataframe
2024-09-01 00:27:24,630:INFO:Initializing Decision Tree Classifier
2024-09-01 00:27:24,630:INFO:Total runtime is 0.21985390981038413 minutes
2024-09-01 00:27:24,634:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:24,634:INFO:Initializing create_model()
2024-09-01 00:27:24,634:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:24,635:INFO:Checking exceptions
2024-09-01 00:27:24,635:INFO:Importing libraries
2024-09-01 00:27:24,635:INFO:Copying training dataset
2024-09-01 00:27:24,646:INFO:Defining folds
2024-09-01 00:27:24,646:INFO:Declaring metric variables
2024-09-01 00:27:24,650:INFO:Importing untrained model
2024-09-01 00:27:24,655:INFO:Decision Tree Classifier Imported successfully
2024-09-01 00:27:24,664:INFO:Starting cross validation
2024-09-01 00:27:24,666:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:24,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,952:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,954:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,118:INFO:Calculating mean and std
2024-09-01 00:27:25,119:INFO:Creating metrics dataframe
2024-09-01 00:27:25,121:INFO:Uploading results into container
2024-09-01 00:27:25,122:INFO:Uploading model into container now
2024-09-01 00:27:25,122:INFO:_master_model_container: 4
2024-09-01 00:27:25,122:INFO:_display_container: 2
2024-09-01 00:27:25,123:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 00:27:25,123:INFO:create_model() successfully completed......................................
2024-09-01 00:27:25,252:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:25,252:INFO:Creating metrics dataframe
2024-09-01 00:27:25,263:INFO:Initializing SVM - Linear Kernel
2024-09-01 00:27:25,263:INFO:Total runtime is 0.23041404883066816 minutes
2024-09-01 00:27:25,266:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:25,267:INFO:Initializing create_model()
2024-09-01 00:27:25,267:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:25,267:INFO:Checking exceptions
2024-09-01 00:27:25,267:INFO:Importing libraries
2024-09-01 00:27:25,267:INFO:Copying training dataset
2024-09-01 00:27:25,278:INFO:Defining folds
2024-09-01 00:27:25,279:INFO:Declaring metric variables
2024-09-01 00:27:25,283:INFO:Importing untrained model
2024-09-01 00:27:25,289:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 00:27:25,297:INFO:Starting cross validation
2024-09-01 00:27:25,299:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:25,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,738:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,756:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,771:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,019:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,048:INFO:Calculating mean and std
2024-09-01 00:27:26,049:INFO:Creating metrics dataframe
2024-09-01 00:27:26,051:INFO:Uploading results into container
2024-09-01 00:27:26,051:INFO:Uploading model into container now
2024-09-01 00:27:26,052:INFO:_master_model_container: 5
2024-09-01 00:27:26,052:INFO:_display_container: 2
2024-09-01 00:27:26,053:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 00:27:26,053:INFO:create_model() successfully completed......................................
2024-09-01 00:27:26,186:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:26,186:INFO:Creating metrics dataframe
2024-09-01 00:27:26,197:INFO:Initializing Ridge Classifier
2024-09-01 00:27:26,197:INFO:Total runtime is 0.24597050746281945 minutes
2024-09-01 00:27:26,199:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:26,200:INFO:Initializing create_model()
2024-09-01 00:27:26,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:26,200:INFO:Checking exceptions
2024-09-01 00:27:26,200:INFO:Importing libraries
2024-09-01 00:27:26,200:INFO:Copying training dataset
2024-09-01 00:27:26,211:INFO:Defining folds
2024-09-01 00:27:26,211:INFO:Declaring metric variables
2024-09-01 00:27:26,215:INFO:Importing untrained model
2024-09-01 00:27:26,220:INFO:Ridge Classifier Imported successfully
2024-09-01 00:27:26,227:INFO:Starting cross validation
2024-09-01 00:27:26,230:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:26,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,528:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,529:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,543:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,699:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,703:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,722:INFO:Calculating mean and std
2024-09-01 00:27:26,724:INFO:Creating metrics dataframe
2024-09-01 00:27:26,726:INFO:Uploading results into container
2024-09-01 00:27:26,726:INFO:Uploading model into container now
2024-09-01 00:27:26,727:INFO:_master_model_container: 6
2024-09-01 00:27:26,727:INFO:_display_container: 2
2024-09-01 00:27:26,728:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 00:27:26,728:INFO:create_model() successfully completed......................................
2024-09-01 00:27:26,852:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:26,853:INFO:Creating metrics dataframe
2024-09-01 00:27:26,863:INFO:Initializing Random Forest Classifier
2024-09-01 00:27:26,863:INFO:Total runtime is 0.25708246628443404 minutes
2024-09-01 00:27:26,866:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:26,867:INFO:Initializing create_model()
2024-09-01 00:27:26,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:26,867:INFO:Checking exceptions
2024-09-01 00:27:26,867:INFO:Importing libraries
2024-09-01 00:27:26,867:INFO:Copying training dataset
2024-09-01 00:27:26,878:INFO:Defining folds
2024-09-01 00:27:26,879:INFO:Declaring metric variables
2024-09-01 00:27:26,884:INFO:Importing untrained model
2024-09-01 00:27:26,890:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:27:26,899:INFO:Starting cross validation
2024-09-01 00:27:26,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,124:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,616:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,631:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,633:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,639:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,642:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,644:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,651:INFO:Calculating mean and std
2024-09-01 00:27:28,652:INFO:Creating metrics dataframe
2024-09-01 00:27:28,654:INFO:Uploading results into container
2024-09-01 00:27:28,656:INFO:Uploading model into container now
2024-09-01 00:27:28,656:INFO:_master_model_container: 7
2024-09-01 00:27:28,657:INFO:_display_container: 2
2024-09-01 00:27:28,657:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:28,657:INFO:create_model() successfully completed......................................
2024-09-01 00:27:28,784:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:28,785:INFO:Creating metrics dataframe
2024-09-01 00:27:28,808:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 00:27:28,809:INFO:Total runtime is 0.2895180741945903 minutes
2024-09-01 00:27:28,814:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:28,814:INFO:Initializing create_model()
2024-09-01 00:27:28,814:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:28,814:INFO:Checking exceptions
2024-09-01 00:27:28,814:INFO:Importing libraries
2024-09-01 00:27:28,814:INFO:Copying training dataset
2024-09-01 00:27:28,834:INFO:Defining folds
2024-09-01 00:27:28,834:INFO:Declaring metric variables
2024-09-01 00:27:28,842:INFO:Importing untrained model
2024-09-01 00:27:28,847:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 00:27:28,859:INFO:Starting cross validation
2024-09-01 00:27:28,863:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,132:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,133:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,134:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,147:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,149:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,150:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,250:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,289:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,313:INFO:Calculating mean and std
2024-09-01 00:27:29,315:INFO:Creating metrics dataframe
2024-09-01 00:27:29,317:INFO:Uploading results into container
2024-09-01 00:27:29,318:INFO:Uploading model into container now
2024-09-01 00:27:29,318:INFO:_master_model_container: 8
2024-09-01 00:27:29,318:INFO:_display_container: 2
2024-09-01 00:27:29,319:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 00:27:29,319:INFO:create_model() successfully completed......................................
2024-09-01 00:27:29,457:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:29,457:INFO:Creating metrics dataframe
2024-09-01 00:27:29,466:INFO:Initializing Ada Boost Classifier
2024-09-01 00:27:29,466:INFO:Total runtime is 0.300457243124644 minutes
2024-09-01 00:27:29,470:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:29,471:INFO:Initializing create_model()
2024-09-01 00:27:29,471:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:29,471:INFO:Checking exceptions
2024-09-01 00:27:29,472:INFO:Importing libraries
2024-09-01 00:27:29,472:INFO:Copying training dataset
2024-09-01 00:27:29,482:INFO:Defining folds
2024-09-01 00:27:29,483:INFO:Declaring metric variables
2024-09-01 00:27:29,492:INFO:Importing untrained model
2024-09-01 00:27:29,498:INFO:Ada Boost Classifier Imported successfully
2024-09-01 00:27:29,513:INFO:Starting cross validation
2024-09-01 00:27:29,516:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:29,702:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:30,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,759:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,840:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,848:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:30,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:31,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:31,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:31,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:31,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:31,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,571:INFO:Calculating mean and std
2024-09-01 00:27:31,573:INFO:Creating metrics dataframe
2024-09-01 00:27:31,575:INFO:Uploading results into container
2024-09-01 00:27:31,576:INFO:Uploading model into container now
2024-09-01 00:27:31,576:INFO:_master_model_container: 9
2024-09-01 00:27:31,576:INFO:_display_container: 2
2024-09-01 00:27:31,576:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 00:27:31,576:INFO:create_model() successfully completed......................................
2024-09-01 00:27:31,704:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:31,704:INFO:Creating metrics dataframe
2024-09-01 00:27:31,713:INFO:Initializing Gradient Boosting Classifier
2024-09-01 00:27:31,713:INFO:Total runtime is 0.33791769742965705 minutes
2024-09-01 00:27:31,718:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:31,720:INFO:Initializing create_model()
2024-09-01 00:27:31,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:31,720:INFO:Checking exceptions
2024-09-01 00:27:31,720:INFO:Importing libraries
2024-09-01 00:27:31,720:INFO:Copying training dataset
2024-09-01 00:27:31,728:INFO:Defining folds
2024-09-01 00:27:31,728:INFO:Declaring metric variables
2024-09-01 00:27:31,732:INFO:Importing untrained model
2024-09-01 00:27:31,739:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 00:27:31,749:INFO:Starting cross validation
2024-09-01 00:27:31,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:40,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,201:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,215:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,223:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,229:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,230:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,233:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,240:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,274:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,573:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,584:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,588:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,624:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,255:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,280:INFO:Calculating mean and std
2024-09-01 00:27:46,281:INFO:Creating metrics dataframe
2024-09-01 00:27:46,283:INFO:Uploading results into container
2024-09-01 00:27:46,284:INFO:Uploading model into container now
2024-09-01 00:27:46,284:INFO:_master_model_container: 10
2024-09-01 00:27:46,284:INFO:_display_container: 2
2024-09-01 00:27:46,285:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 00:27:46,285:INFO:create_model() successfully completed......................................
2024-09-01 00:27:46,413:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:46,414:INFO:Creating metrics dataframe
2024-09-01 00:27:46,424:INFO:Initializing Linear Discriminant Analysis
2024-09-01 00:27:46,424:INFO:Total runtime is 0.5830902179082236 minutes
2024-09-01 00:27:46,429:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:46,429:INFO:Initializing create_model()
2024-09-01 00:27:46,429:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:46,430:INFO:Checking exceptions
2024-09-01 00:27:46,430:INFO:Importing libraries
2024-09-01 00:27:46,430:INFO:Copying training dataset
2024-09-01 00:27:46,441:INFO:Defining folds
2024-09-01 00:27:46,441:INFO:Declaring metric variables
2024-09-01 00:27:46,445:INFO:Importing untrained model
2024-09-01 00:27:46,450:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 00:27:46,458:INFO:Starting cross validation
2024-09-01 00:27:46,492:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:46,750:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,952:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,969:INFO:Calculating mean and std
2024-09-01 00:27:46,971:INFO:Creating metrics dataframe
2024-09-01 00:27:46,974:INFO:Uploading results into container
2024-09-01 00:27:46,974:INFO:Uploading model into container now
2024-09-01 00:27:46,975:INFO:_master_model_container: 11
2024-09-01 00:27:46,975:INFO:_display_container: 2
2024-09-01 00:27:46,975:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 00:27:46,975:INFO:create_model() successfully completed......................................
2024-09-01 00:27:47,104:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:47,104:INFO:Creating metrics dataframe
2024-09-01 00:27:47,115:INFO:Initializing Extra Trees Classifier
2024-09-01 00:27:47,115:INFO:Total runtime is 0.5946038087209067 minutes
2024-09-01 00:27:47,118:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:47,119:INFO:Initializing create_model()
2024-09-01 00:27:47,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:47,120:INFO:Checking exceptions
2024-09-01 00:27:47,120:INFO:Importing libraries
2024-09-01 00:27:47,120:INFO:Copying training dataset
2024-09-01 00:27:47,130:INFO:Defining folds
2024-09-01 00:27:47,131:INFO:Declaring metric variables
2024-09-01 00:27:47,135:INFO:Importing untrained model
2024-09-01 00:27:47,141:INFO:Extra Trees Classifier Imported successfully
2024-09-01 00:27:47,148:INFO:Starting cross validation
2024-09-01 00:27:47,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:47,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,877:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,895:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,983:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,252:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:48,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:48,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,274:INFO:Calculating mean and std
2024-09-01 00:27:48,275:INFO:Creating metrics dataframe
2024-09-01 00:27:48,277:INFO:Uploading results into container
2024-09-01 00:27:48,278:INFO:Uploading model into container now
2024-09-01 00:27:48,278:INFO:_master_model_container: 12
2024-09-01 00:27:48,278:INFO:_display_container: 2
2024-09-01 00:27:48,279:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 00:27:48,280:INFO:create_model() successfully completed......................................
2024-09-01 00:27:48,404:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:48,404:INFO:Creating metrics dataframe
2024-09-01 00:27:48,414:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 00:27:48,414:INFO:Total runtime is 0.6162563602129619 minutes
2024-09-01 00:27:48,417:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:48,418:INFO:Initializing create_model()
2024-09-01 00:27:48,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:48,419:INFO:Checking exceptions
2024-09-01 00:27:48,419:INFO:Importing libraries
2024-09-01 00:27:48,419:INFO:Copying training dataset
2024-09-01 00:27:48,430:INFO:Defining folds
2024-09-01 00:27:48,430:INFO:Declaring metric variables
2024-09-01 00:27:48,433:INFO:Importing untrained model
2024-09-01 00:27:48,440:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 00:27:48,447:INFO:Starting cross validation
2024-09-01 00:27:48,449:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:50,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,361:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,370:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,593:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,603:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,608:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,612:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,659:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,669:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,511:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,560:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,574:INFO:Calculating mean and std
2024-09-01 00:27:51,576:INFO:Creating metrics dataframe
2024-09-01 00:27:51,578:INFO:Uploading results into container
2024-09-01 00:27:51,579:INFO:Uploading model into container now
2024-09-01 00:27:51,580:INFO:_master_model_container: 13
2024-09-01 00:27:51,580:INFO:_display_container: 2
2024-09-01 00:27:51,580:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 00:27:51,580:INFO:create_model() successfully completed......................................
2024-09-01 00:27:51,720:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:51,720:INFO:Creating metrics dataframe
2024-09-01 00:27:51,739:INFO:Initializing Dummy Classifier
2024-09-01 00:27:51,740:INFO:Total runtime is 0.6717009663581849 minutes
2024-09-01 00:27:51,746:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:51,747:INFO:Initializing create_model()
2024-09-01 00:27:51,747:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:51,747:INFO:Checking exceptions
2024-09-01 00:27:51,748:INFO:Importing libraries
2024-09-01 00:27:51,748:INFO:Copying training dataset
2024-09-01 00:27:51,761:INFO:Defining folds
2024-09-01 00:27:51,761:INFO:Declaring metric variables
2024-09-01 00:27:51,767:INFO:Importing untrained model
2024-09-01 00:27:51,774:INFO:Dummy Classifier Imported successfully
2024-09-01 00:27:51,784:INFO:Starting cross validation
2024-09-01 00:27:51,787:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:52,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,058:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,079:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,086:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,097:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,207:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,215:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,224:INFO:Calculating mean and std
2024-09-01 00:27:52,225:INFO:Creating metrics dataframe
2024-09-01 00:27:52,227:INFO:Uploading results into container
2024-09-01 00:27:52,227:INFO:Uploading model into container now
2024-09-01 00:27:52,228:INFO:_master_model_container: 14
2024-09-01 00:27:52,228:INFO:_display_container: 2
2024-09-01 00:27:52,228:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 00:27:52,228:INFO:create_model() successfully completed......................................
2024-09-01 00:27:52,357:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:52,357:INFO:Creating metrics dataframe
2024-09-01 00:27:52,379:INFO:Initializing create_model()
2024-09-01 00:27:52,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:52,379:INFO:Checking exceptions
2024-09-01 00:27:52,381:INFO:Importing libraries
2024-09-01 00:27:52,381:INFO:Copying training dataset
2024-09-01 00:27:52,389:INFO:Defining folds
2024-09-01 00:27:52,389:INFO:Declaring metric variables
2024-09-01 00:27:52,389:INFO:Importing untrained model
2024-09-01 00:27:52,389:INFO:Declaring custom model
2024-09-01 00:27:52,390:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:27:52,391:INFO:Cross validation set to False
2024-09-01 00:27:52,391:INFO:Fitting Model
2024-09-01 00:27:52,690:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:52,690:INFO:create_model() successfully completed......................................
2024-09-01 00:27:52,854:INFO:_master_model_container: 14
2024-09-01 00:27:52,854:INFO:_display_container: 2
2024-09-01 00:27:52,855:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:52,855:INFO:compare_models() successfully completed......................................
2024-09-01 00:27:52,905:INFO:Initializing plot_model()
2024-09-01 00:27:52,905:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:52,905:INFO:Checking exceptions
2024-09-01 00:27:52,931:INFO:Preloading libraries
2024-09-01 00:27:52,940:INFO:Copying training dataset
2024-09-01 00:27:52,940:INFO:Plot type: class_report
2024-09-01 00:27:53,354:INFO:Fitting Model
2024-09-01 00:27:53,355:INFO:Scoring test/hold-out set
2024-09-01 00:27:53,677:INFO:Visual Rendered Successfully
2024-09-01 00:27:53,810:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:53,825:INFO:Initializing plot_model()
2024-09-01 00:27:53,825:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:53,826:INFO:Checking exceptions
2024-09-01 00:27:53,854:INFO:Preloading libraries
2024-09-01 00:27:53,861:INFO:Copying training dataset
2024-09-01 00:27:53,861:INFO:Plot type: confusion_matrix
2024-09-01 00:27:54,278:INFO:Fitting Model
2024-09-01 00:27:54,278:INFO:Scoring test/hold-out set
2024-09-01 00:27:54,500:INFO:Visual Rendered Successfully
2024-09-01 00:27:54,630:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:54,650:INFO:Initializing plot_model()
2024-09-01 00:27:54,650:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:54,650:INFO:Checking exceptions
2024-09-01 00:27:54,679:INFO:Preloading libraries
2024-09-01 00:27:54,691:INFO:Copying training dataset
2024-09-01 00:27:54,691:INFO:Plot type: auc
2024-09-01 00:27:55,103:INFO:Fitting Model
2024-09-01 00:27:55,104:INFO:Scoring test/hold-out set
2024-09-01 00:27:55,405:INFO:Visual Rendered Successfully
2024-09-01 00:27:55,534:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:55,553:INFO:Initializing plot_model()
2024-09-01 00:27:55,553:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:55,553:INFO:Checking exceptions
2024-09-01 00:27:55,582:INFO:Preloading libraries
2024-09-01 00:27:55,590:INFO:Copying training dataset
2024-09-01 00:27:55,591:INFO:Plot type: learning
2024-09-01 00:27:56,035:INFO:Fitting Model
2024-09-01 00:28:07,483:INFO:Visual Rendered Successfully
2024-09-01 00:28:07,622:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:07,664:INFO:Initializing compare_models()
2024-09-01 00:28:07,665:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Cost-Benefit, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Cost-Benefit', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 00:28:07,665:INFO:Checking exceptions
2024-09-01 00:28:07,672:INFO:Preparing display monitor
2024-09-01 00:28:07,709:INFO:Initializing Logistic Regression
2024-09-01 00:28:07,709:INFO:Total runtime is 1.6669432322184246e-05 minutes
2024-09-01 00:28:07,712:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:07,713:INFO:Initializing create_model()
2024-09-01 00:28:07,713:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:07,713:INFO:Checking exceptions
2024-09-01 00:28:07,713:INFO:Importing libraries
2024-09-01 00:28:07,713:INFO:Copying training dataset
2024-09-01 00:28:07,730:INFO:Defining folds
2024-09-01 00:28:07,730:INFO:Declaring metric variables
2024-09-01 00:28:07,734:INFO:Importing untrained model
2024-09-01 00:28:07,740:INFO:Logistic Regression Imported successfully
2024-09-01 00:28:07,749:INFO:Starting cross validation
2024-09-01 00:28:07,752:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:09,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,837:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,841:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,890:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,972:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,979:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,989:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,994:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,994:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,995:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:10,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,014:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:10,019:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:11,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:11,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:11,068:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:11,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,074:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,078:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,087:INFO:Calculating mean and std
2024-09-01 00:28:11,088:INFO:Creating metrics dataframe
2024-09-01 00:28:11,090:INFO:Uploading results into container
2024-09-01 00:28:11,090:INFO:Uploading model into container now
2024-09-01 00:28:11,091:INFO:_master_model_container: 15
2024-09-01 00:28:11,091:INFO:_display_container: 3
2024-09-01 00:28:11,092:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:11,092:INFO:create_model() successfully completed......................................
2024-09-01 00:28:11,229:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:11,229:INFO:Creating metrics dataframe
2024-09-01 00:28:11,252:INFO:Initializing K Neighbors Classifier
2024-09-01 00:28:11,253:INFO:Total runtime is 0.059084888299306235 minutes
2024-09-01 00:28:11,258:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:11,258:INFO:Initializing create_model()
2024-09-01 00:28:11,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:11,258:INFO:Checking exceptions
2024-09-01 00:28:11,258:INFO:Importing libraries
2024-09-01 00:28:11,258:INFO:Copying training dataset
2024-09-01 00:28:11,272:INFO:Defining folds
2024-09-01 00:28:11,272:INFO:Declaring metric variables
2024-09-01 00:28:11,275:INFO:Importing untrained model
2024-09-01 00:28:11,283:INFO:K Neighbors Classifier Imported successfully
2024-09-01 00:28:11,292:INFO:Starting cross validation
2024-09-01 00:28:11,294:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:11,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,732:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,755:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,760:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,762:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,766:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,038:INFO:Calculating mean and std
2024-09-01 00:28:12,039:INFO:Creating metrics dataframe
2024-09-01 00:28:12,041:INFO:Uploading results into container
2024-09-01 00:28:12,042:INFO:Uploading model into container now
2024-09-01 00:28:12,042:INFO:_master_model_container: 16
2024-09-01 00:28:12,042:INFO:_display_container: 3
2024-09-01 00:28:12,043:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 00:28:12,043:INFO:create_model() successfully completed......................................
2024-09-01 00:28:12,175:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:12,175:INFO:Creating metrics dataframe
2024-09-01 00:28:12,185:INFO:Initializing Naive Bayes
2024-09-01 00:28:12,185:INFO:Total runtime is 0.07461739778518676 minutes
2024-09-01 00:28:12,189:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:12,190:INFO:Initializing create_model()
2024-09-01 00:28:12,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:12,190:INFO:Checking exceptions
2024-09-01 00:28:12,190:INFO:Importing libraries
2024-09-01 00:28:12,190:INFO:Copying training dataset
2024-09-01 00:28:12,201:INFO:Defining folds
2024-09-01 00:28:12,201:INFO:Declaring metric variables
2024-09-01 00:28:12,205:INFO:Importing untrained model
2024-09-01 00:28:12,209:INFO:Naive Bayes Imported successfully
2024-09-01 00:28:12,218:INFO:Starting cross validation
2024-09-01 00:28:12,220:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:12,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,484:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,511:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,532:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 00:28:12,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,551:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,654:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,660:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,663:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,670:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,675:INFO:Calculating mean and std
2024-09-01 00:28:12,677:INFO:Creating metrics dataframe
2024-09-01 00:28:12,681:INFO:Uploading results into container
2024-09-01 00:28:12,682:INFO:Uploading model into container now
2024-09-01 00:28:12,683:INFO:_master_model_container: 17
2024-09-01 00:28:12,683:INFO:_display_container: 3
2024-09-01 00:28:12,683:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 00:28:12,683:INFO:create_model() successfully completed......................................
2024-09-01 00:28:12,815:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:12,816:INFO:Creating metrics dataframe
2024-09-01 00:28:12,826:INFO:Initializing Decision Tree Classifier
2024-09-01 00:28:12,826:INFO:Total runtime is 0.08528912464777628 minutes
2024-09-01 00:28:12,830:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:12,831:INFO:Initializing create_model()
2024-09-01 00:28:12,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:12,831:INFO:Checking exceptions
2024-09-01 00:28:12,831:INFO:Importing libraries
2024-09-01 00:28:12,831:INFO:Copying training dataset
2024-09-01 00:28:12,842:INFO:Defining folds
2024-09-01 00:28:12,842:INFO:Declaring metric variables
2024-09-01 00:28:12,847:INFO:Importing untrained model
2024-09-01 00:28:12,853:INFO:Decision Tree Classifier Imported successfully
2024-09-01 00:28:12,861:INFO:Starting cross validation
2024-09-01 00:28:12,864:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:13,110:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,120:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,143:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,149:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,190:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,193:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,196:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,207:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,330:INFO:Calculating mean and std
2024-09-01 00:28:13,332:INFO:Creating metrics dataframe
2024-09-01 00:28:13,334:INFO:Uploading results into container
2024-09-01 00:28:13,335:INFO:Uploading model into container now
2024-09-01 00:28:13,335:INFO:_master_model_container: 18
2024-09-01 00:28:13,335:INFO:_display_container: 3
2024-09-01 00:28:13,336:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 00:28:13,336:INFO:create_model() successfully completed......................................
2024-09-01 00:28:13,468:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:13,468:INFO:Creating metrics dataframe
2024-09-01 00:28:13,476:INFO:Initializing SVM - Linear Kernel
2024-09-01 00:28:13,476:INFO:Total runtime is 0.09613619645436605 minutes
2024-09-01 00:28:13,482:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:13,482:INFO:Initializing create_model()
2024-09-01 00:28:13,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:13,482:INFO:Checking exceptions
2024-09-01 00:28:13,483:INFO:Importing libraries
2024-09-01 00:28:13,483:INFO:Copying training dataset
2024-09-01 00:28:13,493:INFO:Defining folds
2024-09-01 00:28:13,493:INFO:Declaring metric variables
2024-09-01 00:28:13,498:INFO:Importing untrained model
2024-09-01 00:28:13,503:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 00:28:13,510:INFO:Starting cross validation
2024-09-01 00:28:13,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:13,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,983:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,986:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,988:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,988:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,173:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,194:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,198:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,198:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,206:INFO:Calculating mean and std
2024-09-01 00:28:14,207:INFO:Creating metrics dataframe
2024-09-01 00:28:14,209:INFO:Uploading results into container
2024-09-01 00:28:14,210:INFO:Uploading model into container now
2024-09-01 00:28:14,210:INFO:_master_model_container: 19
2024-09-01 00:28:14,211:INFO:_display_container: 3
2024-09-01 00:28:14,212:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 00:28:14,212:INFO:create_model() successfully completed......................................
2024-09-01 00:28:14,348:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:14,348:INFO:Creating metrics dataframe
2024-09-01 00:28:14,357:INFO:Initializing Ridge Classifier
2024-09-01 00:28:14,357:INFO:Total runtime is 0.11081366141637167 minutes
2024-09-01 00:28:14,362:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:14,363:INFO:Initializing create_model()
2024-09-01 00:28:14,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:14,363:INFO:Checking exceptions
2024-09-01 00:28:14,363:INFO:Importing libraries
2024-09-01 00:28:14,364:INFO:Copying training dataset
2024-09-01 00:28:14,373:INFO:Defining folds
2024-09-01 00:28:14,373:INFO:Declaring metric variables
2024-09-01 00:28:14,378:INFO:Importing untrained model
2024-09-01 00:28:14,385:INFO:Ridge Classifier Imported successfully
2024-09-01 00:28:14,391:INFO:Starting cross validation
2024-09-01 00:28:14,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:14,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,638:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,641:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,643:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,646:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,653:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,653:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,665:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,673:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,678:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,685:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,692:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,699:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,702:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,710:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,722:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,733:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,823:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,840:INFO:Calculating mean and std
2024-09-01 00:28:14,841:INFO:Creating metrics dataframe
2024-09-01 00:28:14,843:INFO:Uploading results into container
2024-09-01 00:28:14,845:INFO:Uploading model into container now
2024-09-01 00:28:14,846:INFO:_master_model_container: 20
2024-09-01 00:28:14,846:INFO:_display_container: 3
2024-09-01 00:28:14,847:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 00:28:14,847:INFO:create_model() successfully completed......................................
2024-09-01 00:28:14,987:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:14,988:INFO:Creating metrics dataframe
2024-09-01 00:28:15,000:INFO:Initializing Random Forest Classifier
2024-09-01 00:28:15,000:INFO:Total runtime is 0.1215261975924174 minutes
2024-09-01 00:28:15,003:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:15,003:INFO:Initializing create_model()
2024-09-01 00:28:15,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:15,004:INFO:Checking exceptions
2024-09-01 00:28:15,004:INFO:Importing libraries
2024-09-01 00:28:15,004:INFO:Copying training dataset
2024-09-01 00:28:15,015:INFO:Defining folds
2024-09-01 00:28:15,015:INFO:Declaring metric variables
2024-09-01 00:28:15,018:INFO:Importing untrained model
2024-09-01 00:28:15,024:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:28:15,032:INFO:Starting cross validation
2024-09-01 00:28:15,035:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:16,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,357:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,424:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,876:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,892:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,898:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,900:INFO:Calculating mean and std
2024-09-01 00:28:16,901:INFO:Creating metrics dataframe
2024-09-01 00:28:16,903:INFO:Uploading results into container
2024-09-01 00:28:16,903:INFO:Uploading model into container now
2024-09-01 00:28:16,903:INFO:_master_model_container: 21
2024-09-01 00:28:16,904:INFO:_display_container: 3
2024-09-01 00:28:16,904:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:28:16,904:INFO:create_model() successfully completed......................................
2024-09-01 00:28:17,047:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:17,047:INFO:Creating metrics dataframe
2024-09-01 00:28:17,056:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 00:28:17,056:INFO:Total runtime is 0.15578850507736205 minutes
2024-09-01 00:28:17,062:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:17,062:INFO:Initializing create_model()
2024-09-01 00:28:17,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:17,062:INFO:Checking exceptions
2024-09-01 00:28:17,063:INFO:Importing libraries
2024-09-01 00:28:17,063:INFO:Copying training dataset
2024-09-01 00:28:17,071:INFO:Defining folds
2024-09-01 00:28:17,071:INFO:Declaring metric variables
2024-09-01 00:28:17,075:INFO:Importing untrained model
2024-09-01 00:28:17,083:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 00:28:17,089:INFO:Starting cross validation
2024-09-01 00:28:17,091:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:17,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,298:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,329:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,413:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,430:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,528:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,536:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,546:INFO:Calculating mean and std
2024-09-01 00:28:17,548:INFO:Creating metrics dataframe
2024-09-01 00:28:17,551:INFO:Uploading results into container
2024-09-01 00:28:17,552:INFO:Uploading model into container now
2024-09-01 00:28:17,553:INFO:_master_model_container: 22
2024-09-01 00:28:17,554:INFO:_display_container: 3
2024-09-01 00:28:17,554:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 00:28:17,554:INFO:create_model() successfully completed......................................
2024-09-01 00:28:17,689:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:17,689:INFO:Creating metrics dataframe
2024-09-01 00:28:17,700:INFO:Initializing Ada Boost Classifier
2024-09-01 00:28:17,701:INFO:Total runtime is 0.16654818852742512 minutes
2024-09-01 00:28:17,705:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:17,705:INFO:Initializing create_model()
2024-09-01 00:28:17,705:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:17,705:INFO:Checking exceptions
2024-09-01 00:28:17,705:INFO:Importing libraries
2024-09-01 00:28:17,705:INFO:Copying training dataset
2024-09-01 00:28:17,716:INFO:Defining folds
2024-09-01 00:28:17,716:INFO:Declaring metric variables
2024-09-01 00:28:17,720:INFO:Importing untrained model
2024-09-01 00:28:17,727:INFO:Ada Boost Classifier Imported successfully
2024-09-01 00:28:17,734:INFO:Starting cross validation
2024-09-01 00:28:17,736:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:17,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:18,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:18,883:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,892:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:18,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:18,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:18,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:18,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,050:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,058:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:19,087:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,114:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,138:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:19,736:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,739:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,816:INFO:Calculating mean and std
2024-09-01 00:28:19,817:INFO:Creating metrics dataframe
2024-09-01 00:28:19,819:INFO:Uploading results into container
2024-09-01 00:28:19,820:INFO:Uploading model into container now
2024-09-01 00:28:19,820:INFO:_master_model_container: 23
2024-09-01 00:28:19,820:INFO:_display_container: 3
2024-09-01 00:28:19,822:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 00:28:19,822:INFO:create_model() successfully completed......................................
2024-09-01 00:28:19,952:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:19,952:INFO:Creating metrics dataframe
2024-09-01 00:28:19,964:INFO:Initializing Gradient Boosting Classifier
2024-09-01 00:28:19,964:INFO:Total runtime is 0.20426334142684935 minutes
2024-09-01 00:28:19,968:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:19,968:INFO:Initializing create_model()
2024-09-01 00:28:19,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:19,968:INFO:Checking exceptions
2024-09-01 00:28:19,968:INFO:Importing libraries
2024-09-01 00:28:19,968:INFO:Copying training dataset
2024-09-01 00:28:19,980:INFO:Defining folds
2024-09-01 00:28:19,980:INFO:Declaring metric variables
2024-09-01 00:28:19,983:INFO:Importing untrained model
2024-09-01 00:28:19,989:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 00:28:19,997:INFO:Starting cross validation
2024-09-01 00:28:20,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:28,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,122:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,127:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,212:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,220:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,245:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,287:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:33,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:33,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:33,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,855:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:33,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,873:INFO:Calculating mean and std
2024-09-01 00:28:33,874:INFO:Creating metrics dataframe
2024-09-01 00:28:33,879:INFO:Uploading results into container
2024-09-01 00:28:33,880:INFO:Uploading model into container now
2024-09-01 00:28:33,880:INFO:_master_model_container: 24
2024-09-01 00:28:33,880:INFO:_display_container: 3
2024-09-01 00:28:33,881:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 00:28:33,881:INFO:create_model() successfully completed......................................
2024-09-01 00:28:34,042:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:34,042:INFO:Creating metrics dataframe
2024-09-01 00:28:34,053:INFO:Initializing Linear Discriminant Analysis
2024-09-01 00:28:34,053:INFO:Total runtime is 0.43907773892084756 minutes
2024-09-01 00:28:34,056:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:34,056:INFO:Initializing create_model()
2024-09-01 00:28:34,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:34,057:INFO:Checking exceptions
2024-09-01 00:28:34,057:INFO:Importing libraries
2024-09-01 00:28:34,057:INFO:Copying training dataset
2024-09-01 00:28:34,068:INFO:Defining folds
2024-09-01 00:28:34,068:INFO:Declaring metric variables
2024-09-01 00:28:34,071:INFO:Importing untrained model
2024-09-01 00:28:34,077:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 00:28:34,086:INFO:Starting cross validation
2024-09-01 00:28:34,088:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:34,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,358:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,535:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,571:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,577:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,587:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,589:INFO:Calculating mean and std
2024-09-01 00:28:34,590:INFO:Creating metrics dataframe
2024-09-01 00:28:34,593:INFO:Uploading results into container
2024-09-01 00:28:34,595:INFO:Uploading model into container now
2024-09-01 00:28:34,595:INFO:_master_model_container: 25
2024-09-01 00:28:34,595:INFO:_display_container: 3
2024-09-01 00:28:34,596:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 00:28:34,596:INFO:create_model() successfully completed......................................
2024-09-01 00:28:34,725:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:34,725:INFO:Creating metrics dataframe
2024-09-01 00:28:34,739:INFO:Initializing Extra Trees Classifier
2024-09-01 00:28:34,739:INFO:Total runtime is 0.4505175789197286 minutes
2024-09-01 00:28:34,744:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:34,744:INFO:Initializing create_model()
2024-09-01 00:28:34,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:34,744:INFO:Checking exceptions
2024-09-01 00:28:34,745:INFO:Importing libraries
2024-09-01 00:28:34,745:INFO:Copying training dataset
2024-09-01 00:28:34,754:INFO:Defining folds
2024-09-01 00:28:34,754:INFO:Declaring metric variables
2024-09-01 00:28:34,759:INFO:Importing untrained model
2024-09-01 00:28:34,766:INFO:Extra Trees Classifier Imported successfully
2024-09-01 00:28:34,772:INFO:Starting cross validation
2024-09-01 00:28:34,776:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:35,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,484:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,529:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,572:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,589:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,595:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,610:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,625:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,652:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,654:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,656:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,929:INFO:Calculating mean and std
2024-09-01 00:28:35,930:INFO:Creating metrics dataframe
2024-09-01 00:28:35,932:INFO:Uploading results into container
2024-09-01 00:28:35,932:INFO:Uploading model into container now
2024-09-01 00:28:35,933:INFO:_master_model_container: 26
2024-09-01 00:28:35,933:INFO:_display_container: 3
2024-09-01 00:28:35,934:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 00:28:35,934:INFO:create_model() successfully completed......................................
2024-09-01 00:28:36,106:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:36,106:INFO:Creating metrics dataframe
2024-09-01 00:28:36,121:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 00:28:36,121:INFO:Total runtime is 0.47353917360305786 minutes
2024-09-01 00:28:36,124:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:36,126:INFO:Initializing create_model()
2024-09-01 00:28:36,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:36,126:INFO:Checking exceptions
2024-09-01 00:28:36,126:INFO:Importing libraries
2024-09-01 00:28:36,126:INFO:Copying training dataset
2024-09-01 00:28:36,136:INFO:Defining folds
2024-09-01 00:28:36,136:INFO:Declaring metric variables
2024-09-01 00:28:36,140:INFO:Importing untrained model
2024-09-01 00:28:36,147:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 00:28:36,154:INFO:Starting cross validation
2024-09-01 00:28:36,157:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:38,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,050:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,055:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,293:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,618:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,630:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,650:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,671:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,693:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,755:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,760:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,293:INFO:Calculating mean and std
2024-09-01 00:28:39,295:INFO:Creating metrics dataframe
2024-09-01 00:28:39,297:INFO:Uploading results into container
2024-09-01 00:28:39,298:INFO:Uploading model into container now
2024-09-01 00:28:39,299:INFO:_master_model_container: 27
2024-09-01 00:28:39,299:INFO:_display_container: 3
2024-09-01 00:28:39,299:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 00:28:39,299:INFO:create_model() successfully completed......................................
2024-09-01 00:28:39,443:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:39,443:INFO:Creating metrics dataframe
2024-09-01 00:28:39,466:INFO:Initializing Dummy Classifier
2024-09-01 00:28:39,466:INFO:Total runtime is 0.5293021082878113 minutes
2024-09-01 00:28:39,473:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:39,474:INFO:Initializing create_model()
2024-09-01 00:28:39,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:39,474:INFO:Checking exceptions
2024-09-01 00:28:39,475:INFO:Importing libraries
2024-09-01 00:28:39,475:INFO:Copying training dataset
2024-09-01 00:28:39,486:INFO:Defining folds
2024-09-01 00:28:39,486:INFO:Declaring metric variables
2024-09-01 00:28:39,491:INFO:Importing untrained model
2024-09-01 00:28:39,496:INFO:Dummy Classifier Imported successfully
2024-09-01 00:28:39,504:INFO:Starting cross validation
2024-09-01 00:28:39,507:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:39,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,745:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,766:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,772:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,823:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,922:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,928:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,929:INFO:Calculating mean and std
2024-09-01 00:28:39,930:INFO:Creating metrics dataframe
2024-09-01 00:28:39,932:INFO:Uploading results into container
2024-09-01 00:28:39,933:INFO:Uploading model into container now
2024-09-01 00:28:39,933:INFO:_master_model_container: 28
2024-09-01 00:28:39,933:INFO:_display_container: 3
2024-09-01 00:28:39,933:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 00:28:39,933:INFO:create_model() successfully completed......................................
2024-09-01 00:28:40,063:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:40,064:INFO:Creating metrics dataframe
2024-09-01 00:28:40,094:INFO:Initializing create_model()
2024-09-01 00:28:40,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:40,094:INFO:Checking exceptions
2024-09-01 00:28:40,097:INFO:Importing libraries
2024-09-01 00:28:40,097:INFO:Copying training dataset
2024-09-01 00:28:40,130:INFO:Defining folds
2024-09-01 00:28:40,131:INFO:Declaring metric variables
2024-09-01 00:28:40,131:INFO:Importing untrained model
2024-09-01 00:28:40,131:INFO:Declaring custom model
2024-09-01 00:28:40,132:INFO:Logistic Regression Imported successfully
2024-09-01 00:28:40,134:INFO:Cross validation set to False
2024-09-01 00:28:40,134:INFO:Fitting Model
2024-09-01 00:28:41,264:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:41,264:INFO:create_model() successfully completed......................................
2024-09-01 00:28:41,441:INFO:_master_model_container: 28
2024-09-01 00:28:41,441:INFO:_display_container: 3
2024-09-01 00:28:41,442:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:41,442:INFO:compare_models() successfully completed......................................
2024-09-01 00:28:41,509:INFO:Initializing plot_model()
2024-09-01 00:28:41,509:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:41,509:INFO:Checking exceptions
2024-09-01 00:28:41,519:INFO:Preloading libraries
2024-09-01 00:28:41,520:INFO:Copying training dataset
2024-09-01 00:28:41,520:INFO:Plot type: class_report
2024-09-01 00:28:41,931:INFO:Fitting Model
2024-09-01 00:28:41,931:INFO:Scoring test/hold-out set
2024-09-01 00:28:42,184:INFO:Visual Rendered Successfully
2024-09-01 00:28:42,317:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:42,331:INFO:Initializing plot_model()
2024-09-01 00:28:42,331:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:42,332:INFO:Checking exceptions
2024-09-01 00:28:42,341:INFO:Preloading libraries
2024-09-01 00:28:42,343:INFO:Copying training dataset
2024-09-01 00:28:42,343:INFO:Plot type: confusion_matrix
2024-09-01 00:28:42,889:INFO:Fitting Model
2024-09-01 00:28:42,889:INFO:Scoring test/hold-out set
2024-09-01 00:28:43,062:INFO:Visual Rendered Successfully
2024-09-01 00:28:43,199:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:43,214:INFO:Initializing plot_model()
2024-09-01 00:28:43,214:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:43,214:INFO:Checking exceptions
2024-09-01 00:28:43,225:INFO:Preloading libraries
2024-09-01 00:28:43,225:INFO:Copying training dataset
2024-09-01 00:28:43,225:INFO:Plot type: auc
2024-09-01 00:28:43,508:INFO:Fitting Model
2024-09-01 00:28:43,511:INFO:Scoring test/hold-out set
2024-09-01 00:28:43,764:INFO:Visual Rendered Successfully
2024-09-01 00:28:43,898:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:43,917:INFO:Initializing plot_model()
2024-09-01 00:28:43,917:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:43,917:INFO:Checking exceptions
2024-09-01 00:28:43,927:INFO:Preloading libraries
2024-09-01 00:28:43,928:INFO:Copying training dataset
2024-09-01 00:28:43,928:INFO:Plot type: learning
2024-09-01 00:28:44,228:INFO:Fitting Model
2024-09-01 00:32:01,214:INFO:Visual Rendered Successfully
2024-09-01 00:32:01,345:INFO:plot_model() successfully completed......................................
