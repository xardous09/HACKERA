2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:41:46,841:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-08-31 13:43:41,558:INFO:PyCaret ClassificationExperiment
2024-08-31 13:43:41,559:INFO:Logging name: clf-default-name
2024-08-31 13:43:41,559:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 13:43:41,560:INFO:version 3.3.2
2024-08-31 13:43:41,560:INFO:Initializing setup()
2024-08-31 13:43:41,560:INFO:self.USI: 90ce
2024-08-31 13:43:41,560:INFO:self._variable_keys: {'USI', 'fold_shuffle_param', 'gpu_param', 'html_param', 'memory', 'gpu_n_jobs_param', 'fold_generator', 'is_multiclass', 'fold_groups_param', 'data', 'fix_imbalance', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'seed', 'idx', 'X', 'y_train', '_ml_usecase', 'y_test', 'X_train', 'exp_id', 'logging_param', 'y', '_available_plots', 'target_param', 'X_test', 'pipeline'}
2024-08-31 13:43:41,560:INFO:Checking environment
2024-08-31 13:43:41,560:INFO:python_version: 3.11.5
2024-08-31 13:43:41,560:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-08-31 13:43:41,560:INFO:machine: AMD64
2024-08-31 13:43:41,560:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 13:43:41,560:INFO:Memory: svmem(total=8269512704, available=419684352, percent=94.9, used=7849828352, free=419684352)
2024-08-31 13:43:41,560:INFO:Physical Core: 4
2024-08-31 13:43:41,560:INFO:Logical Core: 8
2024-08-31 13:43:41,560:INFO:Checking libraries
2024-08-31 13:43:41,560:INFO:System:
2024-08-31 13:43:41,560:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-08-31 13:43:41,560:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-08-31 13:43:41,560:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 13:43:41,560:INFO:PyCaret required dependencies:
2024-08-31 13:43:44,146:INFO:                 pip: 23.2.1
2024-08-31 13:43:44,147:INFO:          setuptools: 68.0.0
2024-08-31 13:43:44,147:INFO:             pycaret: 3.3.2
2024-08-31 13:43:44,147:INFO:             IPython: 8.15.0
2024-08-31 13:43:44,147:INFO:          ipywidgets: 8.0.4
2024-08-31 13:43:44,147:INFO:                tqdm: 4.65.0
2024-08-31 13:43:44,147:INFO:               numpy: 1.24.3
2024-08-31 13:43:44,147:INFO:              pandas: 2.0.3
2024-08-31 13:43:44,147:INFO:              jinja2: 3.1.2
2024-08-31 13:43:44,147:INFO:               scipy: 1.11.1
2024-08-31 13:43:44,147:INFO:              joblib: 1.2.0
2024-08-31 13:43:44,148:INFO:             sklearn: 1.4.2
2024-08-31 13:43:44,148:INFO:                pyod: 2.0.1
2024-08-31 13:43:44,148:INFO:            imblearn: 0.12.3
2024-08-31 13:43:44,148:INFO:   category_encoders: 2.6.3
2024-08-31 13:43:44,148:INFO:            lightgbm: 4.5.0
2024-08-31 13:43:44,148:INFO:               numba: 0.57.1
2024-08-31 13:43:44,148:INFO:            requests: 2.31.0
2024-08-31 13:43:44,148:INFO:          matplotlib: 3.7.2
2024-08-31 13:43:44,148:INFO:          scikitplot: 0.3.7
2024-08-31 13:43:44,148:INFO:         yellowbrick: 1.5
2024-08-31 13:43:44,148:INFO:              plotly: 5.24.0
2024-08-31 13:43:44,148:INFO:    plotly-resampler: Not installed
2024-08-31 13:43:44,148:INFO:             kaleido: 0.2.1
2024-08-31 13:43:44,148:INFO:           schemdraw: 0.15
2024-08-31 13:43:44,148:INFO:         statsmodels: 0.14.0
2024-08-31 13:43:44,148:INFO:              sktime: 0.26.0
2024-08-31 13:43:44,148:INFO:               tbats: 1.1.3
2024-08-31 13:43:44,148:INFO:            pmdarima: 2.0.4
2024-08-31 13:43:44,148:INFO:              psutil: 5.9.0
2024-08-31 13:43:44,148:INFO:          markupsafe: 2.1.1
2024-08-31 13:43:44,149:INFO:             pickle5: Not installed
2024-08-31 13:43:44,149:INFO:         cloudpickle: 2.2.1
2024-08-31 13:43:44,149:INFO:         deprecation: 2.1.0
2024-08-31 13:43:44,149:INFO:              xxhash: 2.0.2
2024-08-31 13:43:44,149:INFO:           wurlitzer: Not installed
2024-08-31 13:43:44,149:INFO:PyCaret optional dependencies:
2024-08-31 13:43:44,162:INFO:                shap: Not installed
2024-08-31 13:43:44,162:INFO:           interpret: Not installed
2024-08-31 13:43:44,162:INFO:                umap: Not installed
2024-08-31 13:43:44,162:INFO:     ydata_profiling: Not installed
2024-08-31 13:43:44,162:INFO:  explainerdashboard: Not installed
2024-08-31 13:43:44,162:INFO:             autoviz: Not installed
2024-08-31 13:43:44,162:INFO:           fairlearn: Not installed
2024-08-31 13:43:44,162:INFO:          deepchecks: Not installed
2024-08-31 13:43:44,162:INFO:             xgboost: Not installed
2024-08-31 13:43:44,162:INFO:            catboost: Not installed
2024-08-31 13:43:44,162:INFO:              kmodes: Not installed
2024-08-31 13:43:44,162:INFO:             mlxtend: 0.22.0
2024-08-31 13:43:44,162:INFO:       statsforecast: Not installed
2024-08-31 13:43:44,162:INFO:        tune_sklearn: Not installed
2024-08-31 13:43:44,162:INFO:                 ray: Not installed
2024-08-31 13:43:44,162:INFO:            hyperopt: Not installed
2024-08-31 13:43:44,162:INFO:              optuna: Not installed
2024-08-31 13:43:44,163:INFO:               skopt: Not installed
2024-08-31 13:43:44,163:INFO:              mlflow: Not installed
2024-08-31 13:43:44,163:INFO:              gradio: Not installed
2024-08-31 13:43:44,163:INFO:             fastapi: Not installed
2024-08-31 13:43:44,163:INFO:             uvicorn: Not installed
2024-08-31 13:43:44,163:INFO:              m2cgen: Not installed
2024-08-31 13:43:44,163:INFO:           evidently: Not installed
2024-08-31 13:43:44,163:INFO:               fugue: Not installed
2024-08-31 13:43:44,163:INFO:           streamlit: Not installed
2024-08-31 13:43:44,163:INFO:             prophet: Not installed
2024-08-31 13:43:44,163:INFO:None
2024-08-31 13:43:44,163:INFO:Set up data.
2024-08-31 13:44:37,840:INFO:PyCaret ClassificationExperiment
2024-08-31 13:44:37,840:INFO:Logging name: clf-default-name
2024-08-31 13:44:37,841:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-08-31 13:44:37,841:INFO:version 3.3.2
2024-08-31 13:44:37,841:INFO:Initializing setup()
2024-08-31 13:44:37,841:INFO:self.USI: cfcc
2024-08-31 13:44:37,841:INFO:self._variable_keys: {'USI', 'fold_shuffle_param', 'gpu_param', 'html_param', 'memory', 'gpu_n_jobs_param', 'fold_generator', 'is_multiclass', 'fold_groups_param', 'data', 'fix_imbalance', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'seed', 'idx', 'X', 'y_train', '_ml_usecase', 'y_test', 'X_train', 'exp_id', 'logging_param', 'y', '_available_plots', 'target_param', 'X_test', 'pipeline'}
2024-08-31 13:44:37,841:INFO:Checking environment
2024-08-31 13:44:37,841:INFO:python_version: 3.11.5
2024-08-31 13:44:37,841:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-08-31 13:44:37,841:INFO:machine: AMD64
2024-08-31 13:44:37,841:INFO:platform: Windows-10-10.0.22631-SP0
2024-08-31 13:44:37,841:INFO:Memory: svmem(total=8269512704, available=499904512, percent=94.0, used=7769608192, free=499904512)
2024-08-31 13:44:37,841:INFO:Physical Core: 4
2024-08-31 13:44:37,841:INFO:Logical Core: 8
2024-08-31 13:44:37,842:INFO:Checking libraries
2024-08-31 13:44:37,842:INFO:System:
2024-08-31 13:44:37,842:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-08-31 13:44:37,842:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-08-31 13:44:37,842:INFO:   machine: Windows-10-10.0.22631-SP0
2024-08-31 13:44:37,842:INFO:PyCaret required dependencies:
2024-08-31 13:44:37,842:INFO:                 pip: 23.2.1
2024-08-31 13:44:37,842:INFO:          setuptools: 68.0.0
2024-08-31 13:44:37,842:INFO:             pycaret: 3.3.2
2024-08-31 13:44:37,842:INFO:             IPython: 8.15.0
2024-08-31 13:44:37,842:INFO:          ipywidgets: 8.0.4
2024-08-31 13:44:37,842:INFO:                tqdm: 4.65.0
2024-08-31 13:44:37,842:INFO:               numpy: 1.24.3
2024-08-31 13:44:37,842:INFO:              pandas: 2.0.3
2024-08-31 13:44:37,842:INFO:              jinja2: 3.1.2
2024-08-31 13:44:37,842:INFO:               scipy: 1.11.1
2024-08-31 13:44:37,842:INFO:              joblib: 1.2.0
2024-08-31 13:44:37,842:INFO:             sklearn: 1.4.2
2024-08-31 13:44:37,842:INFO:                pyod: 2.0.1
2024-08-31 13:44:37,842:INFO:            imblearn: 0.12.3
2024-08-31 13:44:37,842:INFO:   category_encoders: 2.6.3
2024-08-31 13:44:37,842:INFO:            lightgbm: 4.5.0
2024-08-31 13:44:37,842:INFO:               numba: 0.57.1
2024-08-31 13:44:37,842:INFO:            requests: 2.31.0
2024-08-31 13:44:37,842:INFO:          matplotlib: 3.7.2
2024-08-31 13:44:37,842:INFO:          scikitplot: 0.3.7
2024-08-31 13:44:37,843:INFO:         yellowbrick: 1.5
2024-08-31 13:44:37,843:INFO:              plotly: 5.24.0
2024-08-31 13:44:37,843:INFO:    plotly-resampler: Not installed
2024-08-31 13:44:37,843:INFO:             kaleido: 0.2.1
2024-08-31 13:44:37,843:INFO:           schemdraw: 0.15
2024-08-31 13:44:37,843:INFO:         statsmodels: 0.14.0
2024-08-31 13:44:37,843:INFO:              sktime: 0.26.0
2024-08-31 13:44:37,843:INFO:               tbats: 1.1.3
2024-08-31 13:44:37,843:INFO:            pmdarima: 2.0.4
2024-08-31 13:44:37,843:INFO:              psutil: 5.9.0
2024-08-31 13:44:37,843:INFO:          markupsafe: 2.1.1
2024-08-31 13:44:37,843:INFO:             pickle5: Not installed
2024-08-31 13:44:37,843:INFO:         cloudpickle: 2.2.1
2024-08-31 13:44:37,843:INFO:         deprecation: 2.1.0
2024-08-31 13:44:37,843:INFO:              xxhash: 2.0.2
2024-08-31 13:44:37,843:INFO:           wurlitzer: Not installed
2024-08-31 13:44:37,843:INFO:PyCaret optional dependencies:
2024-08-31 13:44:37,843:INFO:                shap: Not installed
2024-08-31 13:44:37,843:INFO:           interpret: Not installed
2024-08-31 13:44:37,843:INFO:                umap: Not installed
2024-08-31 13:44:37,843:INFO:     ydata_profiling: Not installed
2024-08-31 13:44:37,843:INFO:  explainerdashboard: Not installed
2024-08-31 13:44:37,843:INFO:             autoviz: Not installed
2024-08-31 13:44:37,843:INFO:           fairlearn: Not installed
2024-08-31 13:44:37,843:INFO:          deepchecks: Not installed
2024-08-31 13:44:37,843:INFO:             xgboost: Not installed
2024-08-31 13:44:37,843:INFO:            catboost: Not installed
2024-08-31 13:44:37,843:INFO:              kmodes: Not installed
2024-08-31 13:44:37,843:INFO:             mlxtend: 0.22.0
2024-08-31 13:44:37,843:INFO:       statsforecast: Not installed
2024-08-31 13:44:37,844:INFO:        tune_sklearn: Not installed
2024-08-31 13:44:37,844:INFO:                 ray: Not installed
2024-08-31 13:44:37,844:INFO:            hyperopt: Not installed
2024-08-31 13:44:37,844:INFO:              optuna: Not installed
2024-08-31 13:44:37,844:INFO:               skopt: Not installed
2024-08-31 13:44:37,844:INFO:              mlflow: Not installed
2024-08-31 13:44:37,844:INFO:              gradio: Not installed
2024-08-31 13:44:37,844:INFO:             fastapi: Not installed
2024-08-31 13:44:37,844:INFO:             uvicorn: Not installed
2024-08-31 13:44:37,844:INFO:              m2cgen: Not installed
2024-08-31 13:44:37,844:INFO:           evidently: Not installed
2024-08-31 13:44:37,844:INFO:               fugue: Not installed
2024-08-31 13:44:37,844:INFO:           streamlit: Not installed
2024-08-31 13:44:37,844:INFO:             prophet: Not installed
2024-08-31 13:44:37,844:INFO:None
2024-08-31 13:44:37,844:INFO:Set up data.
2024-08-31 13:44:37,881:INFO:Set up folding strategy.
2024-08-31 13:44:37,882:INFO:Set up train/test split.
2024-08-31 13:44:37,907:INFO:Set up index.
2024-08-31 13:44:37,907:INFO:Assigning column types.
2024-08-31 13:44:37,915:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-08-31 13:44:37,963:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 13:44:37,967:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,001:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,040:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,040:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,068:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-08-31 13:44:38,108:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,136:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,177:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-08-31 13:44:38,202:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,202:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,202:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-08-31 13:44:38,268:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,268:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,336:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,336:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:38,340:INFO:Preparing preprocessing pipeline...
2024-08-31 13:44:38,343:INFO:Set up label encoding.
2024-08-31 13:44:38,344:INFO:Set up simple imputation.
2024-08-31 13:44:38,349:INFO:Set up encoding of categorical features.
2024-08-31 13:44:38,350:INFO:Set up column name cleaning.
2024-08-31 13:44:38,506:INFO:Finished creating preprocessing pipeline.
2024-08-31 13:44:38,514:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-08-31 13:44:38,515:INFO:Creating final display dataframe.
2024-08-31 13:44:38,898:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16               Fold Generator   
17                  Fold Number   
18                     CPU Jobs   
19                      Use GPU   
20               Log Experiment   
21              Experiment Name   
22                          USI   

                                                Value  
0                                                  42  
1                                        Failure Type  
2                                          Multiclass  
3   Heat Dissipation Failure: 0, No Failure: 1, Ov...  
4                                          (9973, 21)  
5                                          (9973, 23)  
6                                          (6981, 23)  
7                                          (2992, 23)  
8                                                  18  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                    StratifiedKFold  
17                                                 10  
18                                                 -1  
19                                              False  
20                                              False  
21                                   clf-default-name  
22                                               cfcc  
2024-08-31 13:44:39,045:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,113:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,113:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-08-31 13:44:39,114:INFO:setup() successfully completed in 1.27s...............
2024-08-31 13:49:30,581:INFO:Initializing compare_models()
2024-08-31 13:49:30,581:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 13:49:30,582:INFO:Checking exceptions
2024-08-31 13:49:30,593:INFO:Preparing display monitor
2024-08-31 13:49:30,631:INFO:Initializing Logistic Regression
2024-08-31 13:49:30,631:INFO:Total runtime is 1.668532689412435e-05 minutes
2024-08-31 13:49:30,635:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:30,639:INFO:Initializing create_model()
2024-08-31 13:49:30,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:30,639:INFO:Checking exceptions
2024-08-31 13:49:30,639:INFO:Importing libraries
2024-08-31 13:49:30,640:INFO:Copying training dataset
2024-08-31 13:49:30,654:INFO:Defining folds
2024-08-31 13:49:30,655:INFO:Declaring metric variables
2024-08-31 13:49:30,659:INFO:Importing untrained model
2024-08-31 13:49:30,664:INFO:Logistic Regression Imported successfully
2024-08-31 13:49:30,673:INFO:Starting cross validation
2024-08-31 13:49:30,677:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:44,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,361:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:44,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,445:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,446:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,470:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:44,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:44,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:44,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:45,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:49:45,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:45,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:45,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:45,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:45,908:INFO:Calculating mean and std
2024-08-31 13:49:45,910:INFO:Creating metrics dataframe
2024-08-31 13:49:45,915:INFO:Uploading results into container
2024-08-31 13:49:45,916:INFO:Uploading model into container now
2024-08-31 13:49:45,917:INFO:_master_model_container: 1
2024-08-31 13:49:45,917:INFO:_display_container: 2
2024-08-31 13:49:45,919:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:49:45,919:INFO:create_model() successfully completed......................................
2024-08-31 13:49:48,115:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:48,116:INFO:Creating metrics dataframe
2024-08-31 13:49:48,127:INFO:Initializing K Neighbors Classifier
2024-08-31 13:49:48,127:INFO:Total runtime is 0.2916130820910136 minutes
2024-08-31 13:49:48,130:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:48,131:INFO:Initializing create_model()
2024-08-31 13:49:48,131:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:48,131:INFO:Checking exceptions
2024-08-31 13:49:48,131:INFO:Importing libraries
2024-08-31 13:49:48,131:INFO:Copying training dataset
2024-08-31 13:49:48,144:INFO:Defining folds
2024-08-31 13:49:48,144:INFO:Declaring metric variables
2024-08-31 13:49:48,148:INFO:Importing untrained model
2024-08-31 13:49:48,151:INFO:K Neighbors Classifier Imported successfully
2024-08-31 13:49:48,161:INFO:Starting cross validation
2024-08-31 13:49:48,163:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:48,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,925:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,941:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:48,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:48,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,287:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,290:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,300:INFO:Calculating mean and std
2024-08-31 13:49:49,302:INFO:Creating metrics dataframe
2024-08-31 13:49:49,306:INFO:Uploading results into container
2024-08-31 13:49:49,306:INFO:Uploading model into container now
2024-08-31 13:49:49,307:INFO:_master_model_container: 2
2024-08-31 13:49:49,307:INFO:_display_container: 2
2024-08-31 13:49:49,308:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 13:49:49,308:INFO:create_model() successfully completed......................................
2024-08-31 13:49:49,460:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:49,460:INFO:Creating metrics dataframe
2024-08-31 13:49:49,469:INFO:Initializing Naive Bayes
2024-08-31 13:49:49,469:INFO:Total runtime is 0.31398691336313883 minutes
2024-08-31 13:49:49,472:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:49,472:INFO:Initializing create_model()
2024-08-31 13:49:49,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:49,473:INFO:Checking exceptions
2024-08-31 13:49:49,473:INFO:Importing libraries
2024-08-31 13:49:49,473:INFO:Copying training dataset
2024-08-31 13:49:49,483:INFO:Defining folds
2024-08-31 13:49:49,483:INFO:Declaring metric variables
2024-08-31 13:49:49,486:INFO:Importing untrained model
2024-08-31 13:49:49,489:INFO:Naive Bayes Imported successfully
2024-08-31 13:49:49,498:INFO:Starting cross validation
2024-08-31 13:49:49,499:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:49,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,756:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,772:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,941:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:49,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:49,961:INFO:Calculating mean and std
2024-08-31 13:49:49,963:INFO:Creating metrics dataframe
2024-08-31 13:49:49,963:INFO:Uploading results into container
2024-08-31 13:49:49,964:INFO:Uploading model into container now
2024-08-31 13:49:49,964:INFO:_master_model_container: 3
2024-08-31 13:49:49,965:INFO:_display_container: 2
2024-08-31 13:49:49,965:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:49:49,965:INFO:create_model() successfully completed......................................
2024-08-31 13:49:50,103:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:50,104:INFO:Creating metrics dataframe
2024-08-31 13:49:50,113:INFO:Initializing Decision Tree Classifier
2024-08-31 13:49:50,113:INFO:Total runtime is 0.324717378616333 minutes
2024-08-31 13:49:50,116:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:50,116:INFO:Initializing create_model()
2024-08-31 13:49:50,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:50,116:INFO:Checking exceptions
2024-08-31 13:49:50,116:INFO:Importing libraries
2024-08-31 13:49:50,116:INFO:Copying training dataset
2024-08-31 13:49:50,129:INFO:Defining folds
2024-08-31 13:49:50,129:INFO:Declaring metric variables
2024-08-31 13:49:50,133:INFO:Importing untrained model
2024-08-31 13:49:50,137:INFO:Decision Tree Classifier Imported successfully
2024-08-31 13:49:50,147:INFO:Starting cross validation
2024-08-31 13:49:50,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:50,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,439:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,448:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:50,666:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:50,676:INFO:Calculating mean and std
2024-08-31 13:49:50,678:INFO:Creating metrics dataframe
2024-08-31 13:49:50,680:INFO:Uploading results into container
2024-08-31 13:49:50,680:INFO:Uploading model into container now
2024-08-31 13:49:50,681:INFO:_master_model_container: 4
2024-08-31 13:49:50,681:INFO:_display_container: 2
2024-08-31 13:49:50,682:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-08-31 13:49:50,682:INFO:create_model() successfully completed......................................
2024-08-31 13:49:50,831:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:50,831:INFO:Creating metrics dataframe
2024-08-31 13:49:50,841:INFO:Initializing SVM - Linear Kernel
2024-08-31 13:49:50,841:INFO:Total runtime is 0.3368482788403829 minutes
2024-08-31 13:49:50,843:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:50,844:INFO:Initializing create_model()
2024-08-31 13:49:50,844:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:50,844:INFO:Checking exceptions
2024-08-31 13:49:50,844:INFO:Importing libraries
2024-08-31 13:49:50,844:INFO:Copying training dataset
2024-08-31 13:49:50,856:INFO:Defining folds
2024-08-31 13:49:50,856:INFO:Declaring metric variables
2024-08-31 13:49:50,861:INFO:Importing untrained model
2024-08-31 13:49:50,866:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 13:49:50,874:INFO:Starting cross validation
2024-08-31 13:49:50,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:51,368:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,445:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:51,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:51,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:51,728:INFO:Calculating mean and std
2024-08-31 13:49:51,730:INFO:Creating metrics dataframe
2024-08-31 13:49:51,731:INFO:Uploading results into container
2024-08-31 13:49:51,732:INFO:Uploading model into container now
2024-08-31 13:49:51,733:INFO:_master_model_container: 5
2024-08-31 13:49:51,733:INFO:_display_container: 2
2024-08-31 13:49:51,734:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 13:49:51,734:INFO:create_model() successfully completed......................................
2024-08-31 13:49:51,879:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:51,879:INFO:Creating metrics dataframe
2024-08-31 13:49:51,895:INFO:Initializing Ridge Classifier
2024-08-31 13:49:51,896:INFO:Total runtime is 0.35444366931915283 minutes
2024-08-31 13:49:51,904:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:51,904:INFO:Initializing create_model()
2024-08-31 13:49:51,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:51,905:INFO:Checking exceptions
2024-08-31 13:49:51,905:INFO:Importing libraries
2024-08-31 13:49:51,905:INFO:Copying training dataset
2024-08-31 13:49:51,958:INFO:Defining folds
2024-08-31 13:49:51,960:INFO:Declaring metric variables
2024-08-31 13:49:51,969:INFO:Importing untrained model
2024-08-31 13:49:51,987:INFO:Ridge Classifier Imported successfully
2024-08-31 13:49:52,002:INFO:Starting cross validation
2024-08-31 13:49:52,006:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:52,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,312:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,315:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,318:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,329:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,336:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:52,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:52,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:52,523:INFO:Calculating mean and std
2024-08-31 13:49:52,524:INFO:Creating metrics dataframe
2024-08-31 13:49:52,526:INFO:Uploading results into container
2024-08-31 13:49:52,527:INFO:Uploading model into container now
2024-08-31 13:49:52,527:INFO:_master_model_container: 6
2024-08-31 13:49:52,527:INFO:_display_container: 2
2024-08-31 13:49:52,527:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-08-31 13:49:52,527:INFO:create_model() successfully completed......................................
2024-08-31 13:49:52,667:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:52,667:INFO:Creating metrics dataframe
2024-08-31 13:49:52,676:INFO:Initializing Random Forest Classifier
2024-08-31 13:49:52,676:INFO:Total runtime is 0.36743351221084597 minutes
2024-08-31 13:49:52,679:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:52,679:INFO:Initializing create_model()
2024-08-31 13:49:52,680:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:52,680:INFO:Checking exceptions
2024-08-31 13:49:52,680:INFO:Importing libraries
2024-08-31 13:49:52,680:INFO:Copying training dataset
2024-08-31 13:49:52,691:INFO:Defining folds
2024-08-31 13:49:52,691:INFO:Declaring metric variables
2024-08-31 13:49:52,694:INFO:Importing untrained model
2024-08-31 13:49:52,699:INFO:Random Forest Classifier Imported successfully
2024-08-31 13:49:52,709:INFO:Starting cross validation
2024-08-31 13:49:52,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:53,992:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:53,992:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:53,997:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,047:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-08-31 13:49:54,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,068:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,073:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,077:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,103:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:54,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:54,519:INFO:Calculating mean and std
2024-08-31 13:49:54,520:INFO:Creating metrics dataframe
2024-08-31 13:49:54,522:INFO:Uploading results into container
2024-08-31 13:49:54,522:INFO:Uploading model into container now
2024-08-31 13:49:54,523:INFO:_master_model_container: 7
2024-08-31 13:49:54,523:INFO:_display_container: 2
2024-08-31 13:49:54,523:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-08-31 13:49:54,523:INFO:create_model() successfully completed......................................
2024-08-31 13:49:54,663:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:54,663:INFO:Creating metrics dataframe
2024-08-31 13:49:54,674:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 13:49:54,674:INFO:Total runtime is 0.40073900222778325 minutes
2024-08-31 13:49:54,677:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:54,678:INFO:Initializing create_model()
2024-08-31 13:49:54,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:54,678:INFO:Checking exceptions
2024-08-31 13:49:54,678:INFO:Importing libraries
2024-08-31 13:49:54,678:INFO:Copying training dataset
2024-08-31 13:49:54,690:INFO:Defining folds
2024-08-31 13:49:54,690:INFO:Declaring metric variables
2024-08-31 13:49:54,693:INFO:Importing untrained model
2024-08-31 13:49:54,700:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 13:49:54,707:INFO:Starting cross validation
2024-08-31 13:49:54,709:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:54,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:54,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,003:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,008:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,008:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,059:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:49:55,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:55,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,216:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,217:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,220:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:55,221:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,223:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:55,233:INFO:Calculating mean and std
2024-08-31 13:49:55,234:INFO:Creating metrics dataframe
2024-08-31 13:49:55,236:INFO:Uploading results into container
2024-08-31 13:49:55,237:INFO:Uploading model into container now
2024-08-31 13:49:55,237:INFO:_master_model_container: 8
2024-08-31 13:49:55,237:INFO:_display_container: 2
2024-08-31 13:49:55,237:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 13:49:55,238:INFO:create_model() successfully completed......................................
2024-08-31 13:49:55,376:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:55,376:INFO:Creating metrics dataframe
2024-08-31 13:49:55,386:INFO:Initializing Ada Boost Classifier
2024-08-31 13:49:55,386:INFO:Total runtime is 0.4125955939292908 minutes
2024-08-31 13:49:55,389:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:55,389:INFO:Initializing create_model()
2024-08-31 13:49:55,389:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:55,389:INFO:Checking exceptions
2024-08-31 13:49:55,390:INFO:Importing libraries
2024-08-31 13:49:55,390:INFO:Copying training dataset
2024-08-31 13:49:55,401:INFO:Defining folds
2024-08-31 13:49:55,401:INFO:Declaring metric variables
2024-08-31 13:49:55,406:INFO:Importing untrained model
2024-08-31 13:49:55,410:INFO:Ada Boost Classifier Imported successfully
2024-08-31 13:49:55,419:INFO:Starting cross validation
2024-08-31 13:49:55,422:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:49:55,606:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,613:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,621:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,659:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,665:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:55,679:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:56,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,840:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:56,850:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:56,869:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:56,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:56,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:49:57,677:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:57,680:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,682:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:49:57,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,685:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:57,689:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:49:57,694:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:49:57,702:INFO:Calculating mean and std
2024-08-31 13:49:57,703:INFO:Creating metrics dataframe
2024-08-31 13:49:57,706:INFO:Uploading results into container
2024-08-31 13:49:57,706:INFO:Uploading model into container now
2024-08-31 13:49:57,707:INFO:_master_model_container: 9
2024-08-31 13:49:57,707:INFO:_display_container: 2
2024-08-31 13:49:57,707:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-08-31 13:49:57,707:INFO:create_model() successfully completed......................................
2024-08-31 13:49:57,844:INFO:SubProcess create_model() end ==================================
2024-08-31 13:49:57,844:INFO:Creating metrics dataframe
2024-08-31 13:49:57,854:INFO:Initializing Gradient Boosting Classifier
2024-08-31 13:49:57,854:INFO:Total runtime is 0.45373980998992924 minutes
2024-08-31 13:49:57,857:INFO:SubProcess create_model() called ==================================
2024-08-31 13:49:57,857:INFO:Initializing create_model()
2024-08-31 13:49:57,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:49:57,857:INFO:Checking exceptions
2024-08-31 13:49:57,857:INFO:Importing libraries
2024-08-31 13:49:57,857:INFO:Copying training dataset
2024-08-31 13:49:57,868:INFO:Defining folds
2024-08-31 13:49:57,868:INFO:Declaring metric variables
2024-08-31 13:49:57,872:INFO:Importing untrained model
2024-08-31 13:49:57,877:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 13:49:57,887:INFO:Starting cross validation
2024-08-31 13:49:57,889:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:04,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,595:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,599:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,610:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,677:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:04,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:04,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:04,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,212:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,246:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,266:INFO:Calculating mean and std
2024-08-31 13:50:09,268:INFO:Creating metrics dataframe
2024-08-31 13:50:09,271:INFO:Uploading results into container
2024-08-31 13:50:09,272:INFO:Uploading model into container now
2024-08-31 13:50:09,272:INFO:_master_model_container: 10
2024-08-31 13:50:09,272:INFO:_display_container: 2
2024-08-31 13:50:09,273:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 13:50:09,273:INFO:create_model() successfully completed......................................
2024-08-31 13:50:09,406:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:09,407:INFO:Creating metrics dataframe
2024-08-31 13:50:09,416:INFO:Initializing Linear Discriminant Analysis
2024-08-31 13:50:09,416:INFO:Total runtime is 0.6464404980341594 minutes
2024-08-31 13:50:09,419:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:09,420:INFO:Initializing create_model()
2024-08-31 13:50:09,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:09,420:INFO:Checking exceptions
2024-08-31 13:50:09,420:INFO:Importing libraries
2024-08-31 13:50:09,421:INFO:Copying training dataset
2024-08-31 13:50:09,432:INFO:Defining folds
2024-08-31 13:50:09,432:INFO:Declaring metric variables
2024-08-31 13:50:09,437:INFO:Importing untrained model
2024-08-31 13:50:09,442:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 13:50:09,450:INFO:Starting cross validation
2024-08-31 13:50:09,452:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:09,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,828:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,861:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,875:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,877:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,887:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:09,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:09,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:09,998:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:50:10,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,014:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,029:INFO:Calculating mean and std
2024-08-31 13:50:10,030:INFO:Creating metrics dataframe
2024-08-31 13:50:10,031:INFO:Uploading results into container
2024-08-31 13:50:10,032:INFO:Uploading model into container now
2024-08-31 13:50:10,032:INFO:_master_model_container: 11
2024-08-31 13:50:10,033:INFO:_display_container: 2
2024-08-31 13:50:10,033:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 13:50:10,034:INFO:create_model() successfully completed......................................
2024-08-31 13:50:10,163:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:10,163:INFO:Creating metrics dataframe
2024-08-31 13:50:10,174:INFO:Initializing Extra Trees Classifier
2024-08-31 13:50:10,174:INFO:Total runtime is 0.6590776006380717 minutes
2024-08-31 13:50:10,177:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:10,177:INFO:Initializing create_model()
2024-08-31 13:50:10,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:10,177:INFO:Checking exceptions
2024-08-31 13:50:10,177:INFO:Importing libraries
2024-08-31 13:50:10,178:INFO:Copying training dataset
2024-08-31 13:50:10,190:INFO:Defining folds
2024-08-31 13:50:10,190:INFO:Declaring metric variables
2024-08-31 13:50:10,192:INFO:Importing untrained model
2024-08-31 13:50:10,196:INFO:Extra Trees Classifier Imported successfully
2024-08-31 13:50:10,204:INFO:Starting cross validation
2024-08-31 13:50:10,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:10,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,928:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:10,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:10,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,052:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,077:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,118:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:11,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:11,508:INFO:Calculating mean and std
2024-08-31 13:50:11,509:INFO:Creating metrics dataframe
2024-08-31 13:50:11,511:INFO:Uploading results into container
2024-08-31 13:50:11,512:INFO:Uploading model into container now
2024-08-31 13:50:11,512:INFO:_master_model_container: 12
2024-08-31 13:50:11,512:INFO:_display_container: 2
2024-08-31 13:50:11,513:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-08-31 13:50:11,513:INFO:create_model() successfully completed......................................
2024-08-31 13:50:11,656:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:11,656:INFO:Creating metrics dataframe
2024-08-31 13:50:11,666:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 13:50:11,667:INFO:Total runtime is 0.6839614311854045 minutes
2024-08-31 13:50:11,671:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:11,671:INFO:Initializing create_model()
2024-08-31 13:50:11,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:11,671:INFO:Checking exceptions
2024-08-31 13:50:11,671:INFO:Importing libraries
2024-08-31 13:50:11,671:INFO:Copying training dataset
2024-08-31 13:50:11,682:INFO:Defining folds
2024-08-31 13:50:11,684:INFO:Declaring metric variables
2024-08-31 13:50:11,688:INFO:Importing untrained model
2024-08-31 13:50:11,692:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 13:50:11,701:INFO:Starting cross validation
2024-08-31 13:50:11,704:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:13,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,874:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,887:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,898:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:13,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:13,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,059:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,130:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,143:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,429:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:14,986:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:14,998:INFO:Calculating mean and std
2024-08-31 13:50:14,999:INFO:Creating metrics dataframe
2024-08-31 13:50:15,002:INFO:Uploading results into container
2024-08-31 13:50:15,003:INFO:Uploading model into container now
2024-08-31 13:50:15,003:INFO:_master_model_container: 13
2024-08-31 13:50:15,003:INFO:_display_container: 2
2024-08-31 13:50:15,004:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 13:50:15,004:INFO:create_model() successfully completed......................................
2024-08-31 13:50:15,150:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:15,150:INFO:Creating metrics dataframe
2024-08-31 13:50:15,160:INFO:Initializing Dummy Classifier
2024-08-31 13:50:15,161:INFO:Total runtime is 0.7421804189682008 minutes
2024-08-31 13:50:15,164:INFO:SubProcess create_model() called ==================================
2024-08-31 13:50:15,165:INFO:Initializing create_model()
2024-08-31 13:50:15,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002101E4ECDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:15,165:INFO:Checking exceptions
2024-08-31 13:50:15,165:INFO:Importing libraries
2024-08-31 13:50:15,165:INFO:Copying training dataset
2024-08-31 13:50:15,174:INFO:Defining folds
2024-08-31 13:50:15,174:INFO:Declaring metric variables
2024-08-31 13:50:15,177:INFO:Importing untrained model
2024-08-31 13:50:15,181:INFO:Dummy Classifier Imported successfully
2024-08-31 13:50:15,187:INFO:Starting cross validation
2024-08-31 13:50:15,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:50:15,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,441:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,448:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,456:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,486:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,608:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,615:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,620:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,622:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:50:15,624:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:50:15,634:INFO:Calculating mean and std
2024-08-31 13:50:15,635:INFO:Creating metrics dataframe
2024-08-31 13:50:15,636:INFO:Uploading results into container
2024-08-31 13:50:15,637:INFO:Uploading model into container now
2024-08-31 13:50:15,637:INFO:_master_model_container: 14
2024-08-31 13:50:15,637:INFO:_display_container: 2
2024-08-31 13:50:15,637:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-08-31 13:50:15,637:INFO:create_model() successfully completed......................................
2024-08-31 13:50:15,777:INFO:SubProcess create_model() end ==================================
2024-08-31 13:50:15,777:INFO:Creating metrics dataframe
2024-08-31 13:50:15,801:INFO:Initializing create_model()
2024-08-31 13:50:15,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:50:15,802:INFO:Checking exceptions
2024-08-31 13:50:15,803:INFO:Importing libraries
2024-08-31 13:50:15,803:INFO:Copying training dataset
2024-08-31 13:50:15,816:INFO:Defining folds
2024-08-31 13:50:15,816:INFO:Declaring metric variables
2024-08-31 13:50:15,816:INFO:Importing untrained model
2024-08-31 13:50:15,816:INFO:Declaring custom model
2024-08-31 13:50:15,817:INFO:Naive Bayes Imported successfully
2024-08-31 13:50:15,818:INFO:Cross validation set to False
2024-08-31 13:50:15,818:INFO:Fitting Model
2024-08-31 13:50:15,910:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:50:15,910:INFO:create_model() successfully completed......................................
2024-08-31 13:50:16,081:INFO:_master_model_container: 14
2024-08-31 13:50:16,083:INFO:_display_container: 2
2024-08-31 13:50:16,083:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:50:16,083:INFO:compare_models() successfully completed......................................
2024-08-31 13:50:48,594:INFO:Initializing plot_model()
2024-08-31 13:50:48,594:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:50:48,594:INFO:Checking exceptions
2024-08-31 13:50:48,606:INFO:Preloading libraries
2024-08-31 13:50:48,607:INFO:Copying training dataset
2024-08-31 13:50:48,607:INFO:Plot type: class_report
2024-08-31 13:50:49,156:INFO:Fitting Model
2024-08-31 13:50:49,157:INFO:Scoring test/hold-out set
2024-08-31 13:50:49,447:INFO:Visual Rendered Successfully
2024-08-31 13:50:49,660:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:11,425:INFO:Initializing plot_model()
2024-08-31 13:51:11,425:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:11,425:INFO:Checking exceptions
2024-08-31 13:51:11,435:INFO:Preloading libraries
2024-08-31 13:51:11,436:INFO:Copying training dataset
2024-08-31 13:51:11,436:INFO:Plot type: confusion_matrix
2024-08-31 13:51:11,776:INFO:Fitting Model
2024-08-31 13:51:11,777:INFO:Scoring test/hold-out set
2024-08-31 13:51:11,940:INFO:Visual Rendered Successfully
2024-08-31 13:51:12,102:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:23,536:INFO:Initializing plot_model()
2024-08-31 13:51:23,536:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:23,536:INFO:Checking exceptions
2024-08-31 13:51:23,541:INFO:Preloading libraries
2024-08-31 13:51:23,542:INFO:Copying training dataset
2024-08-31 13:51:23,542:INFO:Plot type: auc
2024-08-31 13:51:23,771:INFO:Fitting Model
2024-08-31 13:51:23,771:INFO:Scoring test/hold-out set
2024-08-31 13:51:23,937:INFO:Visual Rendered Successfully
2024-08-31 13:51:24,109:INFO:plot_model() successfully completed......................................
2024-08-31 13:51:33,796:INFO:Initializing plot_model()
2024-08-31 13:51:33,796:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:51:33,796:INFO:Checking exceptions
2024-08-31 13:51:33,802:INFO:Preloading libraries
2024-08-31 13:51:33,803:INFO:Copying training dataset
2024-08-31 13:51:33,803:INFO:Plot type: learning
2024-08-31 13:51:34,101:INFO:Fitting Model
2024-08-31 13:51:34,925:INFO:Visual Rendered Successfully
2024-08-31 13:51:35,050:INFO:plot_model() successfully completed......................................
2024-08-31 13:52:13,459:INFO:Initializing compare_models()
2024-08-31 13:52:13,459:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Cost-Benefit, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Cost-Benefit', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-08-31 13:52:13,460:INFO:Checking exceptions
2024-08-31 13:52:13,466:INFO:Preparing display monitor
2024-08-31 13:52:13,496:INFO:Initializing Logistic Regression
2024-08-31 13:52:13,496:INFO:Total runtime is 0.0 minutes
2024-08-31 13:52:13,503:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:13,504:INFO:Initializing create_model()
2024-08-31 13:52:13,504:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:13,504:INFO:Checking exceptions
2024-08-31 13:52:13,504:INFO:Importing libraries
2024-08-31 13:52:13,504:INFO:Copying training dataset
2024-08-31 13:52:13,521:INFO:Defining folds
2024-08-31 13:52:13,521:INFO:Declaring metric variables
2024-08-31 13:52:13,524:INFO:Importing untrained model
2024-08-31 13:52:13,535:INFO:Logistic Regression Imported successfully
2024-08-31 13:52:13,563:INFO:Starting cross validation
2024-08-31 13:52:13,569:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:16,222:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,352:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,371:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:16,379:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:16,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,439:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,441:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:16,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:16,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:16,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,646:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:17,656:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-08-31 13:52:17,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:17,676:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:17,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:17,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,698:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,703:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:17,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:17,714:INFO:Calculating mean and std
2024-08-31 13:52:17,716:INFO:Creating metrics dataframe
2024-08-31 13:52:17,717:INFO:Uploading results into container
2024-08-31 13:52:17,718:INFO:Uploading model into container now
2024-08-31 13:52:17,718:INFO:_master_model_container: 15
2024-08-31 13:52:17,718:INFO:_display_container: 3
2024-08-31 13:52:17,718:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:17,718:INFO:create_model() successfully completed......................................
2024-08-31 13:52:17,848:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:17,849:INFO:Creating metrics dataframe
2024-08-31 13:52:17,855:INFO:Initializing K Neighbors Classifier
2024-08-31 13:52:17,856:INFO:Total runtime is 0.07266997893651327 minutes
2024-08-31 13:52:17,859:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:17,859:INFO:Initializing create_model()
2024-08-31 13:52:17,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:17,860:INFO:Checking exceptions
2024-08-31 13:52:17,860:INFO:Importing libraries
2024-08-31 13:52:17,860:INFO:Copying training dataset
2024-08-31 13:52:17,872:INFO:Defining folds
2024-08-31 13:52:17,872:INFO:Declaring metric variables
2024-08-31 13:52:17,875:INFO:Importing untrained model
2024-08-31 13:52:17,878:INFO:K Neighbors Classifier Imported successfully
2024-08-31 13:52:17,889:INFO:Starting cross validation
2024-08-31 13:52:17,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:18,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,477:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-08-31 13:52:18,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,549:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,559:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,850:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,852:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:18,856:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:18,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,869:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:18,871:INFO:Calculating mean and std
2024-08-31 13:52:18,872:INFO:Creating metrics dataframe
2024-08-31 13:52:18,874:INFO:Uploading results into container
2024-08-31 13:52:18,874:INFO:Uploading model into container now
2024-08-31 13:52:18,875:INFO:_master_model_container: 16
2024-08-31 13:52:18,875:INFO:_display_container: 3
2024-08-31 13:52:18,876:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-08-31 13:52:18,876:INFO:create_model() successfully completed......................................
2024-08-31 13:52:19,058:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:19,058:INFO:Creating metrics dataframe
2024-08-31 13:52:19,073:INFO:Initializing Naive Bayes
2024-08-31 13:52:19,075:INFO:Total runtime is 0.09298098882039388 minutes
2024-08-31 13:52:19,080:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:19,081:INFO:Initializing create_model()
2024-08-31 13:52:19,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:19,081:INFO:Checking exceptions
2024-08-31 13:52:19,081:INFO:Importing libraries
2024-08-31 13:52:19,081:INFO:Copying training dataset
2024-08-31 13:52:19,100:INFO:Defining folds
2024-08-31 13:52:19,100:INFO:Declaring metric variables
2024-08-31 13:52:19,104:INFO:Importing untrained model
2024-08-31 13:52:19,107:INFO:Naive Bayes Imported successfully
2024-08-31 13:52:19,117:INFO:Starting cross validation
2024-08-31 13:52:19,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:19,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,463:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,479:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,483:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,507:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,615:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,620:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,637:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:19,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:19,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:19,650:INFO:Calculating mean and std
2024-08-31 13:52:19,651:INFO:Creating metrics dataframe
2024-08-31 13:52:19,653:INFO:Uploading results into container
2024-08-31 13:52:19,654:INFO:Uploading model into container now
2024-08-31 13:52:19,654:INFO:_master_model_container: 17
2024-08-31 13:52:19,654:INFO:_display_container: 3
2024-08-31 13:52:19,654:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-08-31 13:52:19,654:INFO:create_model() successfully completed......................................
2024-08-31 13:52:19,787:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:19,787:INFO:Creating metrics dataframe
2024-08-31 13:52:19,798:INFO:Initializing Decision Tree Classifier
2024-08-31 13:52:19,798:INFO:Total runtime is 0.10503343343734742 minutes
2024-08-31 13:52:19,802:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:19,802:INFO:Initializing create_model()
2024-08-31 13:52:19,802:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:19,802:INFO:Checking exceptions
2024-08-31 13:52:19,802:INFO:Importing libraries
2024-08-31 13:52:19,802:INFO:Copying training dataset
2024-08-31 13:52:19,813:INFO:Defining folds
2024-08-31 13:52:19,814:INFO:Declaring metric variables
2024-08-31 13:52:19,816:INFO:Importing untrained model
2024-08-31 13:52:19,820:INFO:Decision Tree Classifier Imported successfully
2024-08-31 13:52:19,828:INFO:Starting cross validation
2024-08-31 13:52:19,830:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:20,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,103:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,105:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,116:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,117:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,121:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,122:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,127:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,186:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,204:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:20,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:20,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:20,331:INFO:Calculating mean and std
2024-08-31 13:52:20,332:INFO:Creating metrics dataframe
2024-08-31 13:52:20,336:INFO:Uploading results into container
2024-08-31 13:52:20,336:INFO:Uploading model into container now
2024-08-31 13:52:20,337:INFO:_master_model_container: 18
2024-08-31 13:52:20,337:INFO:_display_container: 3
2024-08-31 13:52:20,337:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-08-31 13:52:20,337:INFO:create_model() successfully completed......................................
2024-08-31 13:52:20,492:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:20,492:INFO:Creating metrics dataframe
2024-08-31 13:52:20,504:INFO:Initializing SVM - Linear Kernel
2024-08-31 13:52:20,504:INFO:Total runtime is 0.11679850816726685 minutes
2024-08-31 13:52:20,506:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:20,507:INFO:Initializing create_model()
2024-08-31 13:52:20,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:20,507:INFO:Checking exceptions
2024-08-31 13:52:20,507:INFO:Importing libraries
2024-08-31 13:52:20,507:INFO:Copying training dataset
2024-08-31 13:52:20,520:INFO:Defining folds
2024-08-31 13:52:20,520:INFO:Declaring metric variables
2024-08-31 13:52:20,523:INFO:Importing untrained model
2024-08-31 13:52:20,530:INFO:SVM - Linear Kernel Imported successfully
2024-08-31 13:52:20,540:INFO:Starting cross validation
2024-08-31 13:52:20,542:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:21,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,114:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,190:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,237:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,247:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,249:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,250:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,254:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:21,446:INFO:Calculating mean and std
2024-08-31 13:52:21,447:INFO:Creating metrics dataframe
2024-08-31 13:52:21,448:INFO:Uploading results into container
2024-08-31 13:52:21,449:INFO:Uploading model into container now
2024-08-31 13:52:21,449:INFO:_master_model_container: 19
2024-08-31 13:52:21,450:INFO:_display_container: 3
2024-08-31 13:52:21,450:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-08-31 13:52:21,450:INFO:create_model() successfully completed......................................
2024-08-31 13:52:21,629:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:21,629:INFO:Creating metrics dataframe
2024-08-31 13:52:21,643:INFO:Initializing Ridge Classifier
2024-08-31 13:52:21,644:INFO:Total runtime is 0.13580156962076823 minutes
2024-08-31 13:52:21,652:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:21,653:INFO:Initializing create_model()
2024-08-31 13:52:21,653:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:21,653:INFO:Checking exceptions
2024-08-31 13:52:21,654:INFO:Importing libraries
2024-08-31 13:52:21,654:INFO:Copying training dataset
2024-08-31 13:52:21,669:INFO:Defining folds
2024-08-31 13:52:21,669:INFO:Declaring metric variables
2024-08-31 13:52:21,673:INFO:Importing untrained model
2024-08-31 13:52:21,678:INFO:Ridge Classifier Imported successfully
2024-08-31 13:52:21,686:INFO:Starting cross validation
2024-08-31 13:52:21,688:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:21,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:21,989:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:21,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:21,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,000:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,000:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,036:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,047:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,063:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,197:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,202:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,204:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:22,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,216:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:22,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:22,229:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:22,231:INFO:Calculating mean and std
2024-08-31 13:52:22,232:INFO:Creating metrics dataframe
2024-08-31 13:52:22,233:INFO:Uploading results into container
2024-08-31 13:52:22,234:INFO:Uploading model into container now
2024-08-31 13:52:22,235:INFO:_master_model_container: 20
2024-08-31 13:52:22,235:INFO:_display_container: 3
2024-08-31 13:52:22,235:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-08-31 13:52:22,235:INFO:create_model() successfully completed......................................
2024-08-31 13:52:22,368:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:22,370:INFO:Creating metrics dataframe
2024-08-31 13:52:22,382:INFO:Initializing Random Forest Classifier
2024-08-31 13:52:22,382:INFO:Total runtime is 0.1481105923652649 minutes
2024-08-31 13:52:22,386:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:22,387:INFO:Initializing create_model()
2024-08-31 13:52:22,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:22,387:INFO:Checking exceptions
2024-08-31 13:52:22,387:INFO:Importing libraries
2024-08-31 13:52:22,387:INFO:Copying training dataset
2024-08-31 13:52:22,397:INFO:Defining folds
2024-08-31 13:52:22,397:INFO:Declaring metric variables
2024-08-31 13:52:22,401:INFO:Importing untrained model
2024-08-31 13:52:22,405:INFO:Random Forest Classifier Imported successfully
2024-08-31 13:52:22,411:INFO:Starting cross validation
2024-08-31 13:52:22,414:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:23,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,837:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,849:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,855:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,863:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,866:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:23,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:23,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:23,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,281:INFO:Calculating mean and std
2024-08-31 13:52:24,282:INFO:Creating metrics dataframe
2024-08-31 13:52:24,283:INFO:Uploading results into container
2024-08-31 13:52:24,284:INFO:Uploading model into container now
2024-08-31 13:52:24,284:INFO:_master_model_container: 21
2024-08-31 13:52:24,284:INFO:_display_container: 3
2024-08-31 13:52:24,285:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-08-31 13:52:24,285:INFO:create_model() successfully completed......................................
2024-08-31 13:52:24,409:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:24,409:INFO:Creating metrics dataframe
2024-08-31 13:52:24,417:INFO:Initializing Quadratic Discriminant Analysis
2024-08-31 13:52:24,417:INFO:Total runtime is 0.18202539682388305 minutes
2024-08-31 13:52:24,421:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:24,421:INFO:Initializing create_model()
2024-08-31 13:52:24,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:24,421:INFO:Checking exceptions
2024-08-31 13:52:24,421:INFO:Importing libraries
2024-08-31 13:52:24,421:INFO:Copying training dataset
2024-08-31 13:52:24,432:INFO:Defining folds
2024-08-31 13:52:24,432:INFO:Declaring metric variables
2024-08-31 13:52:24,435:INFO:Importing untrained model
2024-08-31 13:52:24,438:INFO:Quadratic Discriminant Analysis Imported successfully
2024-08-31 13:52:24,445:INFO:Starting cross validation
2024-08-31 13:52:24,447:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:24,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,727:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,759:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,828:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,838:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,866:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-08-31 13:52:24,902:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:24,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,915:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:24,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:24,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:24,934:INFO:Calculating mean and std
2024-08-31 13:52:24,935:INFO:Creating metrics dataframe
2024-08-31 13:52:24,938:INFO:Uploading results into container
2024-08-31 13:52:24,938:INFO:Uploading model into container now
2024-08-31 13:52:24,939:INFO:_master_model_container: 22
2024-08-31 13:52:24,939:INFO:_display_container: 3
2024-08-31 13:52:24,939:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-08-31 13:52:24,940:INFO:create_model() successfully completed......................................
2024-08-31 13:52:25,072:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:25,072:INFO:Creating metrics dataframe
2024-08-31 13:52:25,081:INFO:Initializing Ada Boost Classifier
2024-08-31 13:52:25,081:INFO:Total runtime is 0.1930871605873108 minutes
2024-08-31 13:52:25,084:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:25,084:INFO:Initializing create_model()
2024-08-31 13:52:25,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:25,084:INFO:Checking exceptions
2024-08-31 13:52:25,084:INFO:Importing libraries
2024-08-31 13:52:25,084:INFO:Copying training dataset
2024-08-31 13:52:25,095:INFO:Defining folds
2024-08-31 13:52:25,095:INFO:Declaring metric variables
2024-08-31 13:52:25,098:INFO:Importing untrained model
2024-08-31 13:52:25,102:INFO:Ada Boost Classifier Imported successfully
2024-08-31 13:52:25,110:INFO:Starting cross validation
2024-08-31 13:52:25,111:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:25,293:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,326:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:25,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,424:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,444:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,567:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,573:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,575:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,578:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,593:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,597:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:26,598:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,612:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-08-31 13:52:26,616:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:26,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,670:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:26,676:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:26,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:27,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:27,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,479:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:27,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:27,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:27,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:27,516:INFO:Calculating mean and std
2024-08-31 13:52:27,516:INFO:Creating metrics dataframe
2024-08-31 13:52:27,518:INFO:Uploading results into container
2024-08-31 13:52:27,518:INFO:Uploading model into container now
2024-08-31 13:52:27,519:INFO:_master_model_container: 23
2024-08-31 13:52:27,519:INFO:_display_container: 3
2024-08-31 13:52:27,519:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-08-31 13:52:27,519:INFO:create_model() successfully completed......................................
2024-08-31 13:52:27,660:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:27,660:INFO:Creating metrics dataframe
2024-08-31 13:52:27,672:INFO:Initializing Gradient Boosting Classifier
2024-08-31 13:52:27,672:INFO:Total runtime is 0.23626784483591715 minutes
2024-08-31 13:52:27,676:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:27,676:INFO:Initializing create_model()
2024-08-31 13:52:27,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:27,676:INFO:Checking exceptions
2024-08-31 13:52:27,676:INFO:Importing libraries
2024-08-31 13:52:27,676:INFO:Copying training dataset
2024-08-31 13:52:27,685:INFO:Defining folds
2024-08-31 13:52:27,686:INFO:Declaring metric variables
2024-08-31 13:52:27,692:INFO:Importing untrained model
2024-08-31 13:52:27,698:INFO:Gradient Boosting Classifier Imported successfully
2024-08-31 13:52:27,709:INFO:Starting cross validation
2024-08-31 13:52:27,710:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:34,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,864:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,878:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,883:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:34,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:34,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:34,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:34,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,078:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:35,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,084:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,089:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:35,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:35,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:35,109:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:39,721:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:39,731:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:39,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:39,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:39,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:39,785:INFO:Calculating mean and std
2024-08-31 13:52:39,786:INFO:Creating metrics dataframe
2024-08-31 13:52:39,788:INFO:Uploading results into container
2024-08-31 13:52:39,788:INFO:Uploading model into container now
2024-08-31 13:52:39,788:INFO:_master_model_container: 24
2024-08-31 13:52:39,788:INFO:_display_container: 3
2024-08-31 13:52:39,789:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-08-31 13:52:39,789:INFO:create_model() successfully completed......................................
2024-08-31 13:52:39,908:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:39,909:INFO:Creating metrics dataframe
2024-08-31 13:52:39,921:INFO:Initializing Linear Discriminant Analysis
2024-08-31 13:52:39,921:INFO:Total runtime is 0.4404159188270569 minutes
2024-08-31 13:52:39,924:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:39,924:INFO:Initializing create_model()
2024-08-31 13:52:39,924:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:39,924:INFO:Checking exceptions
2024-08-31 13:52:39,924:INFO:Importing libraries
2024-08-31 13:52:39,924:INFO:Copying training dataset
2024-08-31 13:52:39,935:INFO:Defining folds
2024-08-31 13:52:39,935:INFO:Declaring metric variables
2024-08-31 13:52:39,938:INFO:Importing untrained model
2024-08-31 13:52:39,942:INFO:Linear Discriminant Analysis Imported successfully
2024-08-31 13:52:39,950:INFO:Starting cross validation
2024-08-31 13:52:39,952:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:40,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,214:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,221:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,222:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,228:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,232:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,234:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,240:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,245:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,246:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,278:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,282:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,286:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,312:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,413:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-08-31 13:52:40,416:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:40,427:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,430:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:40,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:40,438:INFO:Calculating mean and std
2024-08-31 13:52:40,439:INFO:Creating metrics dataframe
2024-08-31 13:52:40,442:INFO:Uploading results into container
2024-08-31 13:52:40,443:INFO:Uploading model into container now
2024-08-31 13:52:40,443:INFO:_master_model_container: 25
2024-08-31 13:52:40,443:INFO:_display_container: 3
2024-08-31 13:52:40,444:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-08-31 13:52:40,444:INFO:create_model() successfully completed......................................
2024-08-31 13:52:40,570:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:40,570:INFO:Creating metrics dataframe
2024-08-31 13:52:40,581:INFO:Initializing Extra Trees Classifier
2024-08-31 13:52:40,581:INFO:Total runtime is 0.45142091512680055 minutes
2024-08-31 13:52:40,585:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:40,585:INFO:Initializing create_model()
2024-08-31 13:52:40,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:40,585:INFO:Checking exceptions
2024-08-31 13:52:40,585:INFO:Importing libraries
2024-08-31 13:52:40,585:INFO:Copying training dataset
2024-08-31 13:52:40,597:INFO:Defining folds
2024-08-31 13:52:40,597:INFO:Declaring metric variables
2024-08-31 13:52:40,600:INFO:Importing untrained model
2024-08-31 13:52:40,603:INFO:Extra Trees Classifier Imported successfully
2024-08-31 13:52:40,610:INFO:Starting cross validation
2024-08-31 13:52:40,612:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:41,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,278:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,289:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,357:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,370:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,621:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,630:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:41,637:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:41,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:41,648:INFO:Calculating mean and std
2024-08-31 13:52:41,649:INFO:Creating metrics dataframe
2024-08-31 13:52:41,651:INFO:Uploading results into container
2024-08-31 13:52:41,652:INFO:Uploading model into container now
2024-08-31 13:52:41,652:INFO:_master_model_container: 26
2024-08-31 13:52:41,652:INFO:_display_container: 3
2024-08-31 13:52:41,652:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-08-31 13:52:41,652:INFO:create_model() successfully completed......................................
2024-08-31 13:52:41,796:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:41,797:INFO:Creating metrics dataframe
2024-08-31 13:52:41,813:INFO:Initializing Light Gradient Boosting Machine
2024-08-31 13:52:41,813:INFO:Total runtime is 0.4719454884529114 minutes
2024-08-31 13:52:41,816:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:41,816:INFO:Initializing create_model()
2024-08-31 13:52:41,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:41,817:INFO:Checking exceptions
2024-08-31 13:52:41,817:INFO:Importing libraries
2024-08-31 13:52:41,817:INFO:Copying training dataset
2024-08-31 13:52:41,826:INFO:Defining folds
2024-08-31 13:52:41,826:INFO:Declaring metric variables
2024-08-31 13:52:41,830:INFO:Importing untrained model
2024-08-31 13:52:41,833:INFO:Light Gradient Boosting Machine Imported successfully
2024-08-31 13:52:41,838:INFO:Starting cross validation
2024-08-31 13:52:41,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:43,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,116:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,120:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,319:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,532:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,636:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,645:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,675:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,698:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:44,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:44,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:44,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,332:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,350:INFO:Calculating mean and std
2024-08-31 13:52:45,352:INFO:Creating metrics dataframe
2024-08-31 13:52:45,355:INFO:Uploading results into container
2024-08-31 13:52:45,356:INFO:Uploading model into container now
2024-08-31 13:52:45,356:INFO:_master_model_container: 27
2024-08-31 13:52:45,356:INFO:_display_container: 3
2024-08-31 13:52:45,357:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-08-31 13:52:45,357:INFO:create_model() successfully completed......................................
2024-08-31 13:52:45,504:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:45,505:INFO:Creating metrics dataframe
2024-08-31 13:52:45,514:INFO:Initializing Dummy Classifier
2024-08-31 13:52:45,514:INFO:Total runtime is 0.5336297750473022 minutes
2024-08-31 13:52:45,517:INFO:SubProcess create_model() called ==================================
2024-08-31 13:52:45,517:INFO:Initializing create_model()
2024-08-31 13:52:45,517:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210165BC810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:45,517:INFO:Checking exceptions
2024-08-31 13:52:45,517:INFO:Importing libraries
2024-08-31 13:52:45,517:INFO:Copying training dataset
2024-08-31 13:52:45,527:INFO:Defining folds
2024-08-31 13:52:45,528:INFO:Declaring metric variables
2024-08-31 13:52:45,530:INFO:Importing untrained model
2024-08-31 13:52:45,534:INFO:Dummy Classifier Imported successfully
2024-08-31 13:52:45,541:INFO:Starting cross validation
2024-08-31 13:52:45,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-08-31 13:52:45,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,797:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,841:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,844:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,844:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,847:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,857:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,861:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-08-31 13:52:45,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-08-31 13:52:45,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-08-31 13:52:45,974:INFO:Calculating mean and std
2024-08-31 13:52:45,975:INFO:Creating metrics dataframe
2024-08-31 13:52:45,979:INFO:Uploading results into container
2024-08-31 13:52:45,980:INFO:Uploading model into container now
2024-08-31 13:52:45,981:INFO:_master_model_container: 28
2024-08-31 13:52:45,981:INFO:_display_container: 3
2024-08-31 13:52:45,981:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-08-31 13:52:45,981:INFO:create_model() successfully completed......................................
2024-08-31 13:52:46,107:INFO:SubProcess create_model() end ==================================
2024-08-31 13:52:46,107:INFO:Creating metrics dataframe
2024-08-31 13:52:46,125:INFO:Initializing create_model()
2024-08-31 13:52:46,125:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-08-31 13:52:46,125:INFO:Checking exceptions
2024-08-31 13:52:46,127:INFO:Importing libraries
2024-08-31 13:52:46,127:INFO:Copying training dataset
2024-08-31 13:52:46,137:INFO:Defining folds
2024-08-31 13:52:46,137:INFO:Declaring metric variables
2024-08-31 13:52:46,137:INFO:Importing untrained model
2024-08-31 13:52:46,137:INFO:Declaring custom model
2024-08-31 13:52:46,137:INFO:Logistic Regression Imported successfully
2024-08-31 13:52:46,139:INFO:Cross validation set to False
2024-08-31 13:52:46,139:INFO:Fitting Model
2024-08-31 13:52:47,386:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:47,386:INFO:create_model() successfully completed......................................
2024-08-31 13:52:47,570:INFO:_master_model_container: 28
2024-08-31 13:52:47,571:INFO:_display_container: 3
2024-08-31 13:52:47,571:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-08-31 13:52:47,571:INFO:compare_models() successfully completed......................................
2024-08-31 13:52:47,609:INFO:Initializing plot_model()
2024-08-31 13:52:47,609:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:52:47,609:INFO:Checking exceptions
2024-08-31 13:52:47,615:INFO:Preloading libraries
2024-08-31 13:52:47,616:INFO:Copying training dataset
2024-08-31 13:52:47,616:INFO:Plot type: class_report
2024-08-31 13:52:47,860:INFO:Fitting Model
2024-08-31 13:52:47,861:INFO:Scoring test/hold-out set
2024-08-31 13:52:48,063:INFO:Visual Rendered Successfully
2024-08-31 13:52:48,194:INFO:plot_model() successfully completed......................................
2024-08-31 13:52:50,847:INFO:Initializing plot_model()
2024-08-31 13:52:50,847:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:52:50,847:INFO:Checking exceptions
2024-08-31 13:52:50,854:INFO:Preloading libraries
2024-08-31 13:52:50,855:INFO:Copying training dataset
2024-08-31 13:52:50,855:INFO:Plot type: confusion_matrix
2024-08-31 13:52:51,095:INFO:Fitting Model
2024-08-31 13:52:51,096:INFO:Scoring test/hold-out set
2024-08-31 13:52:51,227:INFO:Visual Rendered Successfully
2024-08-31 13:52:51,372:INFO:plot_model() successfully completed......................................
2024-08-31 13:53:02,652:INFO:Initializing plot_model()
2024-08-31 13:53:02,653:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:53:02,653:INFO:Checking exceptions
2024-08-31 13:53:02,660:INFO:Preloading libraries
2024-08-31 13:53:02,660:INFO:Copying training dataset
2024-08-31 13:53:02,661:INFO:Plot type: auc
2024-08-31 13:53:02,932:INFO:Fitting Model
2024-08-31 13:53:02,933:INFO:Scoring test/hold-out set
2024-08-31 13:53:03,220:INFO:Visual Rendered Successfully
2024-08-31 13:53:03,362:INFO:plot_model() successfully completed......................................
2024-08-31 13:53:11,618:INFO:Initializing plot_model()
2024-08-31 13:53:11,618:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021029547DD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-08-31 13:53:11,618:INFO:Checking exceptions
2024-08-31 13:53:11,625:INFO:Preloading libraries
2024-08-31 13:53:11,625:INFO:Copying training dataset
2024-08-31 13:53:11,625:INFO:Plot type: learning
2024-08-31 13:53:11,861:INFO:Fitting Model
2024-08-31 13:54:23,593:INFO:Visual Rendered Successfully
2024-08-31 13:54:23,727:INFO:plot_model() successfully completed......................................
2024-08-31 13:54:29,063:INFO:Initializing save_model()
2024-08-31 13:54:29,063:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=predictive_maintenance.pkl, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-08-31 13:54:29,064:INFO:Adding model into prep_pipe
2024-08-31 13:54:29,081:INFO:predictive_maintenance.pkl.pkl saved in current working directory
2024-08-31 13:54:29,089:INFO:Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]',
                                             'Target', 'operational_hours',
                                             'RUL', 'Vi...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=42,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-08-31 13:54:29,089:INFO:save_model() successfully completed......................................
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:07,709:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 00:27:08,516:INFO:PyCaret ClassificationExperiment
2024-09-01 00:27:08,516:INFO:Logging name: clf-default-name
2024-09-01 00:27:08,516:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-01 00:27:08,516:INFO:version 3.3.2
2024-09-01 00:27:08,517:INFO:Initializing setup()
2024-09-01 00:27:08,517:INFO:self.USI: 9902
2024-09-01 00:27:08,517:INFO:self._variable_keys: {'_available_plots', 'exp_id', 'gpu_n_jobs_param', 'X', 'fold_generator', 'fix_imbalance', 'X_test', 'USI', 'X_train', 'log_plots_param', 'target_param', 'fold_shuffle_param', 'n_jobs_param', 'gpu_param', 'idx', 'exp_name_log', 'y', 'y_train', 'data', 'is_multiclass', 'pipeline', '_ml_usecase', 'seed', 'logging_param', 'fold_groups_param', 'memory', 'y_test', 'html_param'}
2024-09-01 00:27:08,517:INFO:Checking environment
2024-09-01 00:27:08,517:INFO:python_version: 3.11.5
2024-09-01 00:27:08,517:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-09-01 00:27:08,517:INFO:machine: AMD64
2024-09-01 00:27:08,517:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-01 00:27:08,517:INFO:Memory: svmem(total=8269512704, available=1388056576, percent=83.2, used=6881456128, free=1388056576)
2024-09-01 00:27:08,517:INFO:Physical Core: 4
2024-09-01 00:27:08,517:INFO:Logical Core: 8
2024-09-01 00:27:08,517:INFO:Checking libraries
2024-09-01 00:27:08,517:INFO:System:
2024-09-01 00:27:08,517:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-09-01 00:27:08,517:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-09-01 00:27:08,517:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-01 00:27:08,517:INFO:PyCaret required dependencies:
2024-09-01 00:27:10,003:INFO:                 pip: 23.2.1
2024-09-01 00:27:10,003:INFO:          setuptools: 68.0.0
2024-09-01 00:27:10,003:INFO:             pycaret: 3.3.2
2024-09-01 00:27:10,003:INFO:             IPython: 8.15.0
2024-09-01 00:27:10,003:INFO:          ipywidgets: 8.0.4
2024-09-01 00:27:10,003:INFO:                tqdm: 4.65.0
2024-09-01 00:27:10,003:INFO:               numpy: 1.24.3
2024-09-01 00:27:10,003:INFO:              pandas: 2.0.3
2024-09-01 00:27:10,003:INFO:              jinja2: 3.1.2
2024-09-01 00:27:10,003:INFO:               scipy: 1.11.1
2024-09-01 00:27:10,003:INFO:              joblib: 1.2.0
2024-09-01 00:27:10,003:INFO:             sklearn: 1.4.2
2024-09-01 00:27:10,003:INFO:                pyod: 2.0.1
2024-09-01 00:27:10,003:INFO:            imblearn: 0.12.3
2024-09-01 00:27:10,003:INFO:   category_encoders: 2.6.3
2024-09-01 00:27:10,003:INFO:            lightgbm: 4.5.0
2024-09-01 00:27:10,003:INFO:               numba: 0.57.1
2024-09-01 00:27:10,004:INFO:            requests: 2.31.0
2024-09-01 00:27:10,004:INFO:          matplotlib: 3.7.2
2024-09-01 00:27:10,004:INFO:          scikitplot: 0.3.7
2024-09-01 00:27:10,004:INFO:         yellowbrick: 1.5
2024-09-01 00:27:10,004:INFO:              plotly: 5.24.0
2024-09-01 00:27:10,004:INFO:    plotly-resampler: Not installed
2024-09-01 00:27:10,004:INFO:             kaleido: 0.2.1
2024-09-01 00:27:10,004:INFO:           schemdraw: 0.15
2024-09-01 00:27:10,005:INFO:         statsmodels: 0.14.0
2024-09-01 00:27:10,005:INFO:              sktime: 0.26.0
2024-09-01 00:27:10,005:INFO:               tbats: 1.1.3
2024-09-01 00:27:10,005:INFO:            pmdarima: 2.0.4
2024-09-01 00:27:10,005:INFO:              psutil: 5.9.0
2024-09-01 00:27:10,005:INFO:          markupsafe: 2.1.1
2024-09-01 00:27:10,005:INFO:             pickle5: Not installed
2024-09-01 00:27:10,005:INFO:         cloudpickle: 2.2.1
2024-09-01 00:27:10,005:INFO:         deprecation: 2.1.0
2024-09-01 00:27:10,006:INFO:              xxhash: 2.0.2
2024-09-01 00:27:10,006:INFO:           wurlitzer: Not installed
2024-09-01 00:27:10,006:INFO:PyCaret optional dependencies:
2024-09-01 00:27:10,019:INFO:                shap: Not installed
2024-09-01 00:27:10,019:INFO:           interpret: Not installed
2024-09-01 00:27:10,019:INFO:                umap: Not installed
2024-09-01 00:27:10,019:INFO:     ydata_profiling: Not installed
2024-09-01 00:27:10,019:INFO:  explainerdashboard: Not installed
2024-09-01 00:27:10,019:INFO:             autoviz: Not installed
2024-09-01 00:27:10,019:INFO:           fairlearn: Not installed
2024-09-01 00:27:10,019:INFO:          deepchecks: Not installed
2024-09-01 00:27:10,019:INFO:             xgboost: Not installed
2024-09-01 00:27:10,020:INFO:            catboost: Not installed
2024-09-01 00:27:10,020:INFO:              kmodes: Not installed
2024-09-01 00:27:10,020:INFO:             mlxtend: 0.22.0
2024-09-01 00:27:10,020:INFO:       statsforecast: Not installed
2024-09-01 00:27:10,020:INFO:        tune_sklearn: Not installed
2024-09-01 00:27:10,020:INFO:                 ray: Not installed
2024-09-01 00:27:10,020:INFO:            hyperopt: Not installed
2024-09-01 00:27:10,020:INFO:              optuna: Not installed
2024-09-01 00:27:10,020:INFO:               skopt: Not installed
2024-09-01 00:27:10,020:INFO:              mlflow: Not installed
2024-09-01 00:27:10,020:INFO:              gradio: Not installed
2024-09-01 00:27:10,020:INFO:             fastapi: Not installed
2024-09-01 00:27:10,020:INFO:             uvicorn: Not installed
2024-09-01 00:27:10,020:INFO:              m2cgen: Not installed
2024-09-01 00:27:10,020:INFO:           evidently: Not installed
2024-09-01 00:27:10,020:INFO:               fugue: Not installed
2024-09-01 00:27:10,020:INFO:           streamlit: 1.38.0
2024-09-01 00:27:10,020:INFO:             prophet: Not installed
2024-09-01 00:27:10,020:INFO:None
2024-09-01 00:27:10,021:INFO:Set up data.
2024-09-01 00:27:10,045:INFO:Set up folding strategy.
2024-09-01 00:27:10,045:INFO:Set up train/test split.
2024-09-01 00:27:10,065:INFO:Set up index.
2024-09-01 00:27:10,066:INFO:Assigning column types.
2024-09-01 00:27:10,074:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-01 00:27:10,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,124:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,173:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,224:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,226:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,259:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,260:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-01 00:27:10,311:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,342:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,390:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 00:27:10,417:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,417:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,417:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-01 00:27:10,493:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,493:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,569:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:10,575:INFO:Preparing preprocessing pipeline...
2024-09-01 00:27:10,578:INFO:Set up label encoding.
2024-09-01 00:27:10,578:INFO:Set up simple imputation.
2024-09-01 00:27:10,582:INFO:Set up encoding of categorical features.
2024-09-01 00:27:10,583:INFO:Set up column name cleaning.
2024-09-01 00:27:10,746:INFO:Finished creating preprocessing pipeline.
2024-09-01 00:27:10,761:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-01 00:27:10,761:INFO:Creating final display dataframe.
2024-09-01 00:27:11,184:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16               Fold Generator   
17                  Fold Number   
18                     CPU Jobs   
19                      Use GPU   
20               Log Experiment   
21              Experiment Name   
22                          USI   

                                                Value  
0                                                  42  
1                                        Failure Type  
2                                          Multiclass  
3   Heat Dissipation Failure: 0, No Failure: 1, Ov...  
4                                          (6879, 21)  
5                                          (6879, 23)  
6                                          (4815, 23)  
7                                          (2064, 23)  
8                                                  18  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                    StratifiedKFold  
17                                                 10  
18                                                 -1  
19                                              False  
20                                              False  
21                                   clf-default-name  
22                                               9902  
2024-09-01 00:27:11,290:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,373:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,373:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 00:27:11,375:INFO:setup() successfully completed in 2.86s...............
2024-09-01 00:27:11,384:INFO:Initializing compare_models()
2024-09-01 00:27:11,384:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 00:27:11,385:INFO:Checking exceptions
2024-09-01 00:27:11,397:INFO:Preparing display monitor
2024-09-01 00:27:11,439:INFO:Initializing Logistic Regression
2024-09-01 00:27:11,439:INFO:Total runtime is 1.6748905181884766e-05 minutes
2024-09-01 00:27:11,445:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:11,446:INFO:Initializing create_model()
2024-09-01 00:27:11,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:11,446:INFO:Checking exceptions
2024-09-01 00:27:11,446:INFO:Importing libraries
2024-09-01 00:27:11,446:INFO:Copying training dataset
2024-09-01 00:27:11,457:INFO:Defining folds
2024-09-01 00:27:11,457:INFO:Declaring metric variables
2024-09-01 00:27:11,461:INFO:Importing untrained model
2024-09-01 00:27:11,466:INFO:Logistic Regression Imported successfully
2024-09-01 00:27:11,474:INFO:Starting cross validation
2024-09-01 00:27:11,476:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:21,237:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,274:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:21,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,565:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:21,689:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:21,692:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:21,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:22,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:22,650:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:27:22,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,662:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:22,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:22,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:22,703:INFO:Calculating mean and std
2024-09-01 00:27:22,705:INFO:Creating metrics dataframe
2024-09-01 00:27:22,707:INFO:Uploading results into container
2024-09-01 00:27:22,708:INFO:Uploading model into container now
2024-09-01 00:27:22,708:INFO:_master_model_container: 1
2024-09-01 00:27:22,708:INFO:_display_container: 2
2024-09-01 00:27:22,709:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:27:22,710:INFO:create_model() successfully completed......................................
2024-09-01 00:27:22,850:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:22,850:INFO:Creating metrics dataframe
2024-09-01 00:27:22,857:INFO:Initializing K Neighbors Classifier
2024-09-01 00:27:22,857:INFO:Total runtime is 0.19031020005544028 minutes
2024-09-01 00:27:22,861:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:22,862:INFO:Initializing create_model()
2024-09-01 00:27:22,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:22,863:INFO:Checking exceptions
2024-09-01 00:27:22,863:INFO:Importing libraries
2024-09-01 00:27:22,863:INFO:Copying training dataset
2024-09-01 00:27:22,872:INFO:Defining folds
2024-09-01 00:27:22,872:INFO:Declaring metric variables
2024-09-01 00:27:22,879:INFO:Importing untrained model
2024-09-01 00:27:22,883:INFO:K Neighbors Classifier Imported successfully
2024-09-01 00:27:22,889:INFO:Starting cross validation
2024-09-01 00:27:22,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:23,487:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,501:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,518:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,536:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:23,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:23,810:INFO:Calculating mean and std
2024-09-01 00:27:23,812:INFO:Creating metrics dataframe
2024-09-01 00:27:23,814:INFO:Uploading results into container
2024-09-01 00:27:23,815:INFO:Uploading model into container now
2024-09-01 00:27:23,815:INFO:_master_model_container: 2
2024-09-01 00:27:23,816:INFO:_display_container: 2
2024-09-01 00:27:23,817:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 00:27:23,817:INFO:create_model() successfully completed......................................
2024-09-01 00:27:23,945:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:23,945:INFO:Creating metrics dataframe
2024-09-01 00:27:23,954:INFO:Initializing Naive Bayes
2024-09-01 00:27:23,954:INFO:Total runtime is 0.20858569145202638 minutes
2024-09-01 00:27:23,957:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:23,958:INFO:Initializing create_model()
2024-09-01 00:27:23,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:23,960:INFO:Checking exceptions
2024-09-01 00:27:23,960:INFO:Importing libraries
2024-09-01 00:27:23,960:INFO:Copying training dataset
2024-09-01 00:27:24,012:INFO:Defining folds
2024-09-01 00:27:24,013:INFO:Declaring metric variables
2024-09-01 00:27:24,016:INFO:Importing untrained model
2024-09-01 00:27:24,020:INFO:Naive Bayes Imported successfully
2024-09-01 00:27:24,033:INFO:Starting cross validation
2024-09-01 00:27:24,035:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:24,290:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 00:27:24,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,338:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,340:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,474:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,488:INFO:Calculating mean and std
2024-09-01 00:27:24,489:INFO:Creating metrics dataframe
2024-09-01 00:27:24,493:INFO:Uploading results into container
2024-09-01 00:27:24,493:INFO:Uploading model into container now
2024-09-01 00:27:24,494:INFO:_master_model_container: 3
2024-09-01 00:27:24,494:INFO:_display_container: 2
2024-09-01 00:27:24,494:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 00:27:24,495:INFO:create_model() successfully completed......................................
2024-09-01 00:27:24,619:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:24,619:INFO:Creating metrics dataframe
2024-09-01 00:27:24,630:INFO:Initializing Decision Tree Classifier
2024-09-01 00:27:24,630:INFO:Total runtime is 0.21985390981038413 minutes
2024-09-01 00:27:24,634:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:24,634:INFO:Initializing create_model()
2024-09-01 00:27:24,634:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:24,635:INFO:Checking exceptions
2024-09-01 00:27:24,635:INFO:Importing libraries
2024-09-01 00:27:24,635:INFO:Copying training dataset
2024-09-01 00:27:24,646:INFO:Defining folds
2024-09-01 00:27:24,646:INFO:Declaring metric variables
2024-09-01 00:27:24,650:INFO:Importing untrained model
2024-09-01 00:27:24,655:INFO:Decision Tree Classifier Imported successfully
2024-09-01 00:27:24,664:INFO:Starting cross validation
2024-09-01 00:27:24,666:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:24,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,952:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,954:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:24,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:24,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,118:INFO:Calculating mean and std
2024-09-01 00:27:25,119:INFO:Creating metrics dataframe
2024-09-01 00:27:25,121:INFO:Uploading results into container
2024-09-01 00:27:25,122:INFO:Uploading model into container now
2024-09-01 00:27:25,122:INFO:_master_model_container: 4
2024-09-01 00:27:25,122:INFO:_display_container: 2
2024-09-01 00:27:25,123:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 00:27:25,123:INFO:create_model() successfully completed......................................
2024-09-01 00:27:25,252:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:25,252:INFO:Creating metrics dataframe
2024-09-01 00:27:25,263:INFO:Initializing SVM - Linear Kernel
2024-09-01 00:27:25,263:INFO:Total runtime is 0.23041404883066816 minutes
2024-09-01 00:27:25,266:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:25,267:INFO:Initializing create_model()
2024-09-01 00:27:25,267:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:25,267:INFO:Checking exceptions
2024-09-01 00:27:25,267:INFO:Importing libraries
2024-09-01 00:27:25,267:INFO:Copying training dataset
2024-09-01 00:27:25,278:INFO:Defining folds
2024-09-01 00:27:25,279:INFO:Declaring metric variables
2024-09-01 00:27:25,283:INFO:Importing untrained model
2024-09-01 00:27:25,289:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 00:27:25,297:INFO:Starting cross validation
2024-09-01 00:27:25,299:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:25,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,738:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,756:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,771:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:25,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:25,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:25,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,019:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,048:INFO:Calculating mean and std
2024-09-01 00:27:26,049:INFO:Creating metrics dataframe
2024-09-01 00:27:26,051:INFO:Uploading results into container
2024-09-01 00:27:26,051:INFO:Uploading model into container now
2024-09-01 00:27:26,052:INFO:_master_model_container: 5
2024-09-01 00:27:26,052:INFO:_display_container: 2
2024-09-01 00:27:26,053:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 00:27:26,053:INFO:create_model() successfully completed......................................
2024-09-01 00:27:26,186:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:26,186:INFO:Creating metrics dataframe
2024-09-01 00:27:26,197:INFO:Initializing Ridge Classifier
2024-09-01 00:27:26,197:INFO:Total runtime is 0.24597050746281945 minutes
2024-09-01 00:27:26,199:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:26,200:INFO:Initializing create_model()
2024-09-01 00:27:26,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:26,200:INFO:Checking exceptions
2024-09-01 00:27:26,200:INFO:Importing libraries
2024-09-01 00:27:26,200:INFO:Copying training dataset
2024-09-01 00:27:26,211:INFO:Defining folds
2024-09-01 00:27:26,211:INFO:Declaring metric variables
2024-09-01 00:27:26,215:INFO:Importing untrained model
2024-09-01 00:27:26,220:INFO:Ridge Classifier Imported successfully
2024-09-01 00:27:26,227:INFO:Starting cross validation
2024-09-01 00:27:26,230:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:26,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,528:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,529:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,543:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,699:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,703:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:26,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:26,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:26,722:INFO:Calculating mean and std
2024-09-01 00:27:26,724:INFO:Creating metrics dataframe
2024-09-01 00:27:26,726:INFO:Uploading results into container
2024-09-01 00:27:26,726:INFO:Uploading model into container now
2024-09-01 00:27:26,727:INFO:_master_model_container: 6
2024-09-01 00:27:26,727:INFO:_display_container: 2
2024-09-01 00:27:26,728:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 00:27:26,728:INFO:create_model() successfully completed......................................
2024-09-01 00:27:26,852:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:26,853:INFO:Creating metrics dataframe
2024-09-01 00:27:26,863:INFO:Initializing Random Forest Classifier
2024-09-01 00:27:26,863:INFO:Total runtime is 0.25708246628443404 minutes
2024-09-01 00:27:26,866:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:26,867:INFO:Initializing create_model()
2024-09-01 00:27:26,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:26,867:INFO:Checking exceptions
2024-09-01 00:27:26,867:INFO:Importing libraries
2024-09-01 00:27:26,867:INFO:Copying training dataset
2024-09-01 00:27:26,878:INFO:Defining folds
2024-09-01 00:27:26,879:INFO:Declaring metric variables
2024-09-01 00:27:26,884:INFO:Importing untrained model
2024-09-01 00:27:26,890:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:27:26,899:INFO:Starting cross validation
2024-09-01 00:27:26,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,111:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,124:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,616:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,631:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,633:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,639:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,642:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:28,644:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:28,651:INFO:Calculating mean and std
2024-09-01 00:27:28,652:INFO:Creating metrics dataframe
2024-09-01 00:27:28,654:INFO:Uploading results into container
2024-09-01 00:27:28,656:INFO:Uploading model into container now
2024-09-01 00:27:28,656:INFO:_master_model_container: 7
2024-09-01 00:27:28,657:INFO:_display_container: 2
2024-09-01 00:27:28,657:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:28,657:INFO:create_model() successfully completed......................................
2024-09-01 00:27:28,784:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:28,785:INFO:Creating metrics dataframe
2024-09-01 00:27:28,808:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 00:27:28,809:INFO:Total runtime is 0.2895180741945903 minutes
2024-09-01 00:27:28,814:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:28,814:INFO:Initializing create_model()
2024-09-01 00:27:28,814:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:28,814:INFO:Checking exceptions
2024-09-01 00:27:28,814:INFO:Importing libraries
2024-09-01 00:27:28,814:INFO:Copying training dataset
2024-09-01 00:27:28,834:INFO:Defining folds
2024-09-01 00:27:28,834:INFO:Declaring metric variables
2024-09-01 00:27:28,842:INFO:Importing untrained model
2024-09-01 00:27:28,847:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 00:27:28,859:INFO:Starting cross validation
2024-09-01 00:27:28,863:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,062:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,132:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,133:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,134:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,135:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,145:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,147:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,149:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,150:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,157:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,250:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:27:29,289:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:29,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:29,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:29,313:INFO:Calculating mean and std
2024-09-01 00:27:29,315:INFO:Creating metrics dataframe
2024-09-01 00:27:29,317:INFO:Uploading results into container
2024-09-01 00:27:29,318:INFO:Uploading model into container now
2024-09-01 00:27:29,318:INFO:_master_model_container: 8
2024-09-01 00:27:29,318:INFO:_display_container: 2
2024-09-01 00:27:29,319:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 00:27:29,319:INFO:create_model() successfully completed......................................
2024-09-01 00:27:29,457:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:29,457:INFO:Creating metrics dataframe
2024-09-01 00:27:29,466:INFO:Initializing Ada Boost Classifier
2024-09-01 00:27:29,466:INFO:Total runtime is 0.300457243124644 minutes
2024-09-01 00:27:29,470:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:29,471:INFO:Initializing create_model()
2024-09-01 00:27:29,471:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:29,471:INFO:Checking exceptions
2024-09-01 00:27:29,472:INFO:Importing libraries
2024-09-01 00:27:29,472:INFO:Copying training dataset
2024-09-01 00:27:29,482:INFO:Defining folds
2024-09-01 00:27:29,483:INFO:Declaring metric variables
2024-09-01 00:27:29,492:INFO:Importing untrained model
2024-09-01 00:27:29,498:INFO:Ada Boost Classifier Imported successfully
2024-09-01 00:27:29,513:INFO:Starting cross validation
2024-09-01 00:27:29,516:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:29,702:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:29,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:30,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,752:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,759:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,820:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:30,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,840:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,848:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:30,853:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:30,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:30,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:27:31,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:31,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:31,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:31,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:31,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:31,571:INFO:Calculating mean and std
2024-09-01 00:27:31,573:INFO:Creating metrics dataframe
2024-09-01 00:27:31,575:INFO:Uploading results into container
2024-09-01 00:27:31,576:INFO:Uploading model into container now
2024-09-01 00:27:31,576:INFO:_master_model_container: 9
2024-09-01 00:27:31,576:INFO:_display_container: 2
2024-09-01 00:27:31,576:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 00:27:31,576:INFO:create_model() successfully completed......................................
2024-09-01 00:27:31,704:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:31,704:INFO:Creating metrics dataframe
2024-09-01 00:27:31,713:INFO:Initializing Gradient Boosting Classifier
2024-09-01 00:27:31,713:INFO:Total runtime is 0.33791769742965705 minutes
2024-09-01 00:27:31,718:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:31,720:INFO:Initializing create_model()
2024-09-01 00:27:31,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:31,720:INFO:Checking exceptions
2024-09-01 00:27:31,720:INFO:Importing libraries
2024-09-01 00:27:31,720:INFO:Copying training dataset
2024-09-01 00:27:31,728:INFO:Defining folds
2024-09-01 00:27:31,728:INFO:Declaring metric variables
2024-09-01 00:27:31,732:INFO:Importing untrained model
2024-09-01 00:27:31,739:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 00:27:31,749:INFO:Starting cross validation
2024-09-01 00:27:31,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:40,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,201:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,206:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,215:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,223:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,229:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,230:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,233:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,240:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,274:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,573:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,584:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,588:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,614:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:40,617:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,624:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:40,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:40,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,255:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,266:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,270:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,280:INFO:Calculating mean and std
2024-09-01 00:27:46,281:INFO:Creating metrics dataframe
2024-09-01 00:27:46,283:INFO:Uploading results into container
2024-09-01 00:27:46,284:INFO:Uploading model into container now
2024-09-01 00:27:46,284:INFO:_master_model_container: 10
2024-09-01 00:27:46,284:INFO:_display_container: 2
2024-09-01 00:27:46,285:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 00:27:46,285:INFO:create_model() successfully completed......................................
2024-09-01 00:27:46,413:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:46,414:INFO:Creating metrics dataframe
2024-09-01 00:27:46,424:INFO:Initializing Linear Discriminant Analysis
2024-09-01 00:27:46,424:INFO:Total runtime is 0.5830902179082236 minutes
2024-09-01 00:27:46,429:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:46,429:INFO:Initializing create_model()
2024-09-01 00:27:46,429:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:46,430:INFO:Checking exceptions
2024-09-01 00:27:46,430:INFO:Importing libraries
2024-09-01 00:27:46,430:INFO:Copying training dataset
2024-09-01 00:27:46,441:INFO:Defining folds
2024-09-01 00:27:46,441:INFO:Declaring metric variables
2024-09-01 00:27:46,445:INFO:Importing untrained model
2024-09-01 00:27:46,450:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 00:27:46,458:INFO:Starting cross validation
2024-09-01 00:27:46,492:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:46,750:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,765:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,781:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,818:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,835:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:27:46,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,952:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:46,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:46,969:INFO:Calculating mean and std
2024-09-01 00:27:46,971:INFO:Creating metrics dataframe
2024-09-01 00:27:46,974:INFO:Uploading results into container
2024-09-01 00:27:46,974:INFO:Uploading model into container now
2024-09-01 00:27:46,975:INFO:_master_model_container: 11
2024-09-01 00:27:46,975:INFO:_display_container: 2
2024-09-01 00:27:46,975:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 00:27:46,975:INFO:create_model() successfully completed......................................
2024-09-01 00:27:47,104:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:47,104:INFO:Creating metrics dataframe
2024-09-01 00:27:47,115:INFO:Initializing Extra Trees Classifier
2024-09-01 00:27:47,115:INFO:Total runtime is 0.5946038087209067 minutes
2024-09-01 00:27:47,118:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:47,119:INFO:Initializing create_model()
2024-09-01 00:27:47,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:47,120:INFO:Checking exceptions
2024-09-01 00:27:47,120:INFO:Importing libraries
2024-09-01 00:27:47,120:INFO:Copying training dataset
2024-09-01 00:27:47,130:INFO:Defining folds
2024-09-01 00:27:47,131:INFO:Declaring metric variables
2024-09-01 00:27:47,135:INFO:Importing untrained model
2024-09-01 00:27:47,141:INFO:Extra Trees Classifier Imported successfully
2024-09-01 00:27:47,148:INFO:Starting cross validation
2024-09-01 00:27:47,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:47,767:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,793:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,877:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,895:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,983:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:47,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:47,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,252:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:48,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:48,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:48,274:INFO:Calculating mean and std
2024-09-01 00:27:48,275:INFO:Creating metrics dataframe
2024-09-01 00:27:48,277:INFO:Uploading results into container
2024-09-01 00:27:48,278:INFO:Uploading model into container now
2024-09-01 00:27:48,278:INFO:_master_model_container: 12
2024-09-01 00:27:48,278:INFO:_display_container: 2
2024-09-01 00:27:48,279:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 00:27:48,280:INFO:create_model() successfully completed......................................
2024-09-01 00:27:48,404:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:48,404:INFO:Creating metrics dataframe
2024-09-01 00:27:48,414:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 00:27:48,414:INFO:Total runtime is 0.6162563602129619 minutes
2024-09-01 00:27:48,417:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:48,418:INFO:Initializing create_model()
2024-09-01 00:27:48,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:48,419:INFO:Checking exceptions
2024-09-01 00:27:48,419:INFO:Importing libraries
2024-09-01 00:27:48,419:INFO:Copying training dataset
2024-09-01 00:27:48,430:INFO:Defining folds
2024-09-01 00:27:48,430:INFO:Declaring metric variables
2024-09-01 00:27:48,433:INFO:Importing untrained model
2024-09-01 00:27:48,440:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 00:27:48,447:INFO:Starting cross validation
2024-09-01 00:27:48,449:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:50,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,361:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,370:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,593:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,603:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,608:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,612:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,659:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,669:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,684:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,780:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:50,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:50,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,511:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,560:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:51,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:51,574:INFO:Calculating mean and std
2024-09-01 00:27:51,576:INFO:Creating metrics dataframe
2024-09-01 00:27:51,578:INFO:Uploading results into container
2024-09-01 00:27:51,579:INFO:Uploading model into container now
2024-09-01 00:27:51,580:INFO:_master_model_container: 13
2024-09-01 00:27:51,580:INFO:_display_container: 2
2024-09-01 00:27:51,580:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 00:27:51,580:INFO:create_model() successfully completed......................................
2024-09-01 00:27:51,720:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:51,720:INFO:Creating metrics dataframe
2024-09-01 00:27:51,739:INFO:Initializing Dummy Classifier
2024-09-01 00:27:51,740:INFO:Total runtime is 0.6717009663581849 minutes
2024-09-01 00:27:51,746:INFO:SubProcess create_model() called ==================================
2024-09-01 00:27:51,747:INFO:Initializing create_model()
2024-09-01 00:27:51,747:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000138193D7250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:51,747:INFO:Checking exceptions
2024-09-01 00:27:51,748:INFO:Importing libraries
2024-09-01 00:27:51,748:INFO:Copying training dataset
2024-09-01 00:27:51,761:INFO:Defining folds
2024-09-01 00:27:51,761:INFO:Declaring metric variables
2024-09-01 00:27:51,767:INFO:Importing untrained model
2024-09-01 00:27:51,774:INFO:Dummy Classifier Imported successfully
2024-09-01 00:27:51,784:INFO:Starting cross validation
2024-09-01 00:27:51,787:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:27:52,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,058:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,066:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,079:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,081:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,086:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,093:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,097:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,207:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,211:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:27:52,213:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,215:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:27:52,224:INFO:Calculating mean and std
2024-09-01 00:27:52,225:INFO:Creating metrics dataframe
2024-09-01 00:27:52,227:INFO:Uploading results into container
2024-09-01 00:27:52,227:INFO:Uploading model into container now
2024-09-01 00:27:52,228:INFO:_master_model_container: 14
2024-09-01 00:27:52,228:INFO:_display_container: 2
2024-09-01 00:27:52,228:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 00:27:52,228:INFO:create_model() successfully completed......................................
2024-09-01 00:27:52,357:INFO:SubProcess create_model() end ==================================
2024-09-01 00:27:52,357:INFO:Creating metrics dataframe
2024-09-01 00:27:52,379:INFO:Initializing create_model()
2024-09-01 00:27:52,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:27:52,379:INFO:Checking exceptions
2024-09-01 00:27:52,381:INFO:Importing libraries
2024-09-01 00:27:52,381:INFO:Copying training dataset
2024-09-01 00:27:52,389:INFO:Defining folds
2024-09-01 00:27:52,389:INFO:Declaring metric variables
2024-09-01 00:27:52,389:INFO:Importing untrained model
2024-09-01 00:27:52,389:INFO:Declaring custom model
2024-09-01 00:27:52,390:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:27:52,391:INFO:Cross validation set to False
2024-09-01 00:27:52,391:INFO:Fitting Model
2024-09-01 00:27:52,690:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:52,690:INFO:create_model() successfully completed......................................
2024-09-01 00:27:52,854:INFO:_master_model_container: 14
2024-09-01 00:27:52,854:INFO:_display_container: 2
2024-09-01 00:27:52,855:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:27:52,855:INFO:compare_models() successfully completed......................................
2024-09-01 00:27:52,905:INFO:Initializing plot_model()
2024-09-01 00:27:52,905:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:52,905:INFO:Checking exceptions
2024-09-01 00:27:52,931:INFO:Preloading libraries
2024-09-01 00:27:52,940:INFO:Copying training dataset
2024-09-01 00:27:52,940:INFO:Plot type: class_report
2024-09-01 00:27:53,354:INFO:Fitting Model
2024-09-01 00:27:53,355:INFO:Scoring test/hold-out set
2024-09-01 00:27:53,677:INFO:Visual Rendered Successfully
2024-09-01 00:27:53,810:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:53,825:INFO:Initializing plot_model()
2024-09-01 00:27:53,825:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:53,826:INFO:Checking exceptions
2024-09-01 00:27:53,854:INFO:Preloading libraries
2024-09-01 00:27:53,861:INFO:Copying training dataset
2024-09-01 00:27:53,861:INFO:Plot type: confusion_matrix
2024-09-01 00:27:54,278:INFO:Fitting Model
2024-09-01 00:27:54,278:INFO:Scoring test/hold-out set
2024-09-01 00:27:54,500:INFO:Visual Rendered Successfully
2024-09-01 00:27:54,630:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:54,650:INFO:Initializing plot_model()
2024-09-01 00:27:54,650:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:54,650:INFO:Checking exceptions
2024-09-01 00:27:54,679:INFO:Preloading libraries
2024-09-01 00:27:54,691:INFO:Copying training dataset
2024-09-01 00:27:54,691:INFO:Plot type: auc
2024-09-01 00:27:55,103:INFO:Fitting Model
2024-09-01 00:27:55,104:INFO:Scoring test/hold-out set
2024-09-01 00:27:55,405:INFO:Visual Rendered Successfully
2024-09-01 00:27:55,534:INFO:plot_model() successfully completed......................................
2024-09-01 00:27:55,553:INFO:Initializing plot_model()
2024-09-01 00:27:55,553:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:27:55,553:INFO:Checking exceptions
2024-09-01 00:27:55,582:INFO:Preloading libraries
2024-09-01 00:27:55,590:INFO:Copying training dataset
2024-09-01 00:27:55,591:INFO:Plot type: learning
2024-09-01 00:27:56,035:INFO:Fitting Model
2024-09-01 00:28:07,483:INFO:Visual Rendered Successfully
2024-09-01 00:28:07,622:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:07,664:INFO:Initializing compare_models()
2024-09-01 00:28:07,665:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Cost-Benefit, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Cost-Benefit', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 00:28:07,665:INFO:Checking exceptions
2024-09-01 00:28:07,672:INFO:Preparing display monitor
2024-09-01 00:28:07,709:INFO:Initializing Logistic Regression
2024-09-01 00:28:07,709:INFO:Total runtime is 1.6669432322184246e-05 minutes
2024-09-01 00:28:07,712:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:07,713:INFO:Initializing create_model()
2024-09-01 00:28:07,713:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:07,713:INFO:Checking exceptions
2024-09-01 00:28:07,713:INFO:Importing libraries
2024-09-01 00:28:07,713:INFO:Copying training dataset
2024-09-01 00:28:07,730:INFO:Defining folds
2024-09-01 00:28:07,730:INFO:Declaring metric variables
2024-09-01 00:28:07,734:INFO:Importing untrained model
2024-09-01 00:28:07,740:INFO:Logistic Regression Imported successfully
2024-09-01 00:28:07,749:INFO:Starting cross validation
2024-09-01 00:28:07,752:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:09,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,837:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,841:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,873:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,890:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:09,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,962:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,972:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:09,978:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,979:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,984:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,987:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:09,989:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,994:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:09,994:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:09,995:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:10,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,014:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:10,019:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:10,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:10,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:11,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:11,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 00:28:11,068:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:11,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,074:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,078:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,087:INFO:Calculating mean and std
2024-09-01 00:28:11,088:INFO:Creating metrics dataframe
2024-09-01 00:28:11,090:INFO:Uploading results into container
2024-09-01 00:28:11,090:INFO:Uploading model into container now
2024-09-01 00:28:11,091:INFO:_master_model_container: 15
2024-09-01 00:28:11,091:INFO:_display_container: 3
2024-09-01 00:28:11,092:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:11,092:INFO:create_model() successfully completed......................................
2024-09-01 00:28:11,229:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:11,229:INFO:Creating metrics dataframe
2024-09-01 00:28:11,252:INFO:Initializing K Neighbors Classifier
2024-09-01 00:28:11,253:INFO:Total runtime is 0.059084888299306235 minutes
2024-09-01 00:28:11,258:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:11,258:INFO:Initializing create_model()
2024-09-01 00:28:11,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:11,258:INFO:Checking exceptions
2024-09-01 00:28:11,258:INFO:Importing libraries
2024-09-01 00:28:11,258:INFO:Copying training dataset
2024-09-01 00:28:11,272:INFO:Defining folds
2024-09-01 00:28:11,272:INFO:Declaring metric variables
2024-09-01 00:28:11,275:INFO:Importing untrained model
2024-09-01 00:28:11,283:INFO:K Neighbors Classifier Imported successfully
2024-09-01 00:28:11,292:INFO:Starting cross validation
2024-09-01 00:28:11,294:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:11,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,732:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,741:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,755:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,760:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,762:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,766:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,783:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,789:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,791:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:11,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:11,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:11,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,027:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,038:INFO:Calculating mean and std
2024-09-01 00:28:12,039:INFO:Creating metrics dataframe
2024-09-01 00:28:12,041:INFO:Uploading results into container
2024-09-01 00:28:12,042:INFO:Uploading model into container now
2024-09-01 00:28:12,042:INFO:_master_model_container: 16
2024-09-01 00:28:12,042:INFO:_display_container: 3
2024-09-01 00:28:12,043:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 00:28:12,043:INFO:create_model() successfully completed......................................
2024-09-01 00:28:12,175:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:12,175:INFO:Creating metrics dataframe
2024-09-01 00:28:12,185:INFO:Initializing Naive Bayes
2024-09-01 00:28:12,185:INFO:Total runtime is 0.07461739778518676 minutes
2024-09-01 00:28:12,189:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:12,190:INFO:Initializing create_model()
2024-09-01 00:28:12,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:12,190:INFO:Checking exceptions
2024-09-01 00:28:12,190:INFO:Importing libraries
2024-09-01 00:28:12,190:INFO:Copying training dataset
2024-09-01 00:28:12,201:INFO:Defining folds
2024-09-01 00:28:12,201:INFO:Declaring metric variables
2024-09-01 00:28:12,205:INFO:Importing untrained model
2024-09-01 00:28:12,209:INFO:Naive Bayes Imported successfully
2024-09-01 00:28:12,218:INFO:Starting cross validation
2024-09-01 00:28:12,220:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:12,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,484:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,494:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,511:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,512:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,532:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 00:28:12,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,551:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,654:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,655:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,660:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,661:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:12,663:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:12,670:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,674:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:12,675:INFO:Calculating mean and std
2024-09-01 00:28:12,677:INFO:Creating metrics dataframe
2024-09-01 00:28:12,681:INFO:Uploading results into container
2024-09-01 00:28:12,682:INFO:Uploading model into container now
2024-09-01 00:28:12,683:INFO:_master_model_container: 17
2024-09-01 00:28:12,683:INFO:_display_container: 3
2024-09-01 00:28:12,683:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 00:28:12,683:INFO:create_model() successfully completed......................................
2024-09-01 00:28:12,815:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:12,816:INFO:Creating metrics dataframe
2024-09-01 00:28:12,826:INFO:Initializing Decision Tree Classifier
2024-09-01 00:28:12,826:INFO:Total runtime is 0.08528912464777628 minutes
2024-09-01 00:28:12,830:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:12,831:INFO:Initializing create_model()
2024-09-01 00:28:12,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:12,831:INFO:Checking exceptions
2024-09-01 00:28:12,831:INFO:Importing libraries
2024-09-01 00:28:12,831:INFO:Copying training dataset
2024-09-01 00:28:12,842:INFO:Defining folds
2024-09-01 00:28:12,842:INFO:Declaring metric variables
2024-09-01 00:28:12,847:INFO:Importing untrained model
2024-09-01 00:28:12,853:INFO:Decision Tree Classifier Imported successfully
2024-09-01 00:28:12,861:INFO:Starting cross validation
2024-09-01 00:28:12,864:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:13,110:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,120:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,143:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,149:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,187:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,190:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,193:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,196:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,207:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,219:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,330:INFO:Calculating mean and std
2024-09-01 00:28:13,332:INFO:Creating metrics dataframe
2024-09-01 00:28:13,334:INFO:Uploading results into container
2024-09-01 00:28:13,335:INFO:Uploading model into container now
2024-09-01 00:28:13,335:INFO:_master_model_container: 18
2024-09-01 00:28:13,335:INFO:_display_container: 3
2024-09-01 00:28:13,336:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 00:28:13,336:INFO:create_model() successfully completed......................................
2024-09-01 00:28:13,468:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:13,468:INFO:Creating metrics dataframe
2024-09-01 00:28:13,476:INFO:Initializing SVM - Linear Kernel
2024-09-01 00:28:13,476:INFO:Total runtime is 0.09613619645436605 minutes
2024-09-01 00:28:13,482:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:13,482:INFO:Initializing create_model()
2024-09-01 00:28:13,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:13,482:INFO:Checking exceptions
2024-09-01 00:28:13,483:INFO:Importing libraries
2024-09-01 00:28:13,483:INFO:Copying training dataset
2024-09-01 00:28:13,493:INFO:Defining folds
2024-09-01 00:28:13,493:INFO:Declaring metric variables
2024-09-01 00:28:13,498:INFO:Importing untrained model
2024-09-01 00:28:13,503:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 00:28:13,510:INFO:Starting cross validation
2024-09-01 00:28:13,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:13,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,939:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,963:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:13,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,983:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,986:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:13,988:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,988:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:13,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:13,996:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,010:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,035:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,051:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,060:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,173:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,181:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,194:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,198:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,198:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,206:INFO:Calculating mean and std
2024-09-01 00:28:14,207:INFO:Creating metrics dataframe
2024-09-01 00:28:14,209:INFO:Uploading results into container
2024-09-01 00:28:14,210:INFO:Uploading model into container now
2024-09-01 00:28:14,210:INFO:_master_model_container: 19
2024-09-01 00:28:14,211:INFO:_display_container: 3
2024-09-01 00:28:14,212:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 00:28:14,212:INFO:create_model() successfully completed......................................
2024-09-01 00:28:14,348:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:14,348:INFO:Creating metrics dataframe
2024-09-01 00:28:14,357:INFO:Initializing Ridge Classifier
2024-09-01 00:28:14,357:INFO:Total runtime is 0.11081366141637167 minutes
2024-09-01 00:28:14,362:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:14,363:INFO:Initializing create_model()
2024-09-01 00:28:14,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:14,363:INFO:Checking exceptions
2024-09-01 00:28:14,363:INFO:Importing libraries
2024-09-01 00:28:14,364:INFO:Copying training dataset
2024-09-01 00:28:14,373:INFO:Defining folds
2024-09-01 00:28:14,373:INFO:Declaring metric variables
2024-09-01 00:28:14,378:INFO:Importing untrained model
2024-09-01 00:28:14,385:INFO:Ridge Classifier Imported successfully
2024-09-01 00:28:14,391:INFO:Starting cross validation
2024-09-01 00:28:14,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:14,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,638:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,641:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,643:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,646:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,653:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,653:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,658:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,665:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,668:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,673:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,678:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,685:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,686:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,687:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,690:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,691:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,692:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,696:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,697:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,699:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,700:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,701:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,702:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,704:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,708:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,710:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,722:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,733:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,817:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:14,823:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,825:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,827:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:14,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:14,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:14,840:INFO:Calculating mean and std
2024-09-01 00:28:14,841:INFO:Creating metrics dataframe
2024-09-01 00:28:14,843:INFO:Uploading results into container
2024-09-01 00:28:14,845:INFO:Uploading model into container now
2024-09-01 00:28:14,846:INFO:_master_model_container: 20
2024-09-01 00:28:14,846:INFO:_display_container: 3
2024-09-01 00:28:14,847:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 00:28:14,847:INFO:create_model() successfully completed......................................
2024-09-01 00:28:14,987:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:14,988:INFO:Creating metrics dataframe
2024-09-01 00:28:15,000:INFO:Initializing Random Forest Classifier
2024-09-01 00:28:15,000:INFO:Total runtime is 0.1215261975924174 minutes
2024-09-01 00:28:15,003:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:15,003:INFO:Initializing create_model()
2024-09-01 00:28:15,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:15,004:INFO:Checking exceptions
2024-09-01 00:28:15,004:INFO:Importing libraries
2024-09-01 00:28:15,004:INFO:Copying training dataset
2024-09-01 00:28:15,015:INFO:Defining folds
2024-09-01 00:28:15,015:INFO:Declaring metric variables
2024-09-01 00:28:15,018:INFO:Importing untrained model
2024-09-01 00:28:15,024:INFO:Random Forest Classifier Imported successfully
2024-09-01 00:28:15,032:INFO:Starting cross validation
2024-09-01 00:28:15,035:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:16,320:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,357:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,417:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,424:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,876:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,884:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:16,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:16,892:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,898:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:16,900:INFO:Calculating mean and std
2024-09-01 00:28:16,901:INFO:Creating metrics dataframe
2024-09-01 00:28:16,903:INFO:Uploading results into container
2024-09-01 00:28:16,903:INFO:Uploading model into container now
2024-09-01 00:28:16,903:INFO:_master_model_container: 21
2024-09-01 00:28:16,904:INFO:_display_container: 3
2024-09-01 00:28:16,904:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 00:28:16,904:INFO:create_model() successfully completed......................................
2024-09-01 00:28:17,047:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:17,047:INFO:Creating metrics dataframe
2024-09-01 00:28:17,056:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 00:28:17,056:INFO:Total runtime is 0.15578850507736205 minutes
2024-09-01 00:28:17,062:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:17,062:INFO:Initializing create_model()
2024-09-01 00:28:17,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:17,062:INFO:Checking exceptions
2024-09-01 00:28:17,063:INFO:Importing libraries
2024-09-01 00:28:17,063:INFO:Copying training dataset
2024-09-01 00:28:17,071:INFO:Defining folds
2024-09-01 00:28:17,071:INFO:Declaring metric variables
2024-09-01 00:28:17,075:INFO:Importing untrained model
2024-09-01 00:28:17,083:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 00:28:17,089:INFO:Starting cross validation
2024-09-01 00:28:17,091:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:17,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,284:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,298:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,329:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,365:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,413:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,421:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,426:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,430:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,435:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 00:28:17,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,520:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:17,521:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,522:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,524:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,527:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,528:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,530:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:17,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:17,536:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:17,546:INFO:Calculating mean and std
2024-09-01 00:28:17,548:INFO:Creating metrics dataframe
2024-09-01 00:28:17,551:INFO:Uploading results into container
2024-09-01 00:28:17,552:INFO:Uploading model into container now
2024-09-01 00:28:17,553:INFO:_master_model_container: 22
2024-09-01 00:28:17,554:INFO:_display_container: 3
2024-09-01 00:28:17,554:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 00:28:17,554:INFO:create_model() successfully completed......................................
2024-09-01 00:28:17,689:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:17,689:INFO:Creating metrics dataframe
2024-09-01 00:28:17,700:INFO:Initializing Ada Boost Classifier
2024-09-01 00:28:17,701:INFO:Total runtime is 0.16654818852742512 minutes
2024-09-01 00:28:17,705:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:17,705:INFO:Initializing create_model()
2024-09-01 00:28:17,705:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:17,705:INFO:Checking exceptions
2024-09-01 00:28:17,705:INFO:Importing libraries
2024-09-01 00:28:17,705:INFO:Copying training dataset
2024-09-01 00:28:17,716:INFO:Defining folds
2024-09-01 00:28:17,716:INFO:Declaring metric variables
2024-09-01 00:28:17,720:INFO:Importing untrained model
2024-09-01 00:28:17,727:INFO:Ada Boost Classifier Imported successfully
2024-09-01 00:28:17,734:INFO:Starting cross validation
2024-09-01 00:28:17,736:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:17,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:17,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:18,880:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:18,883:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,892:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:18,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:18,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:18,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,973:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:18,980:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:18,985:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,002:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,018:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,031:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,033:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,038:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,045:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,050:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,053:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,058:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,067:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,070:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:19,087:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,114:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,138:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 00:28:19,736:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,739:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,748:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:19,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:19,809:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:19,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:19,816:INFO:Calculating mean and std
2024-09-01 00:28:19,817:INFO:Creating metrics dataframe
2024-09-01 00:28:19,819:INFO:Uploading results into container
2024-09-01 00:28:19,820:INFO:Uploading model into container now
2024-09-01 00:28:19,820:INFO:_master_model_container: 23
2024-09-01 00:28:19,820:INFO:_display_container: 3
2024-09-01 00:28:19,822:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 00:28:19,822:INFO:create_model() successfully completed......................................
2024-09-01 00:28:19,952:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:19,952:INFO:Creating metrics dataframe
2024-09-01 00:28:19,964:INFO:Initializing Gradient Boosting Classifier
2024-09-01 00:28:19,964:INFO:Total runtime is 0.20426334142684935 minutes
2024-09-01 00:28:19,968:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:19,968:INFO:Initializing create_model()
2024-09-01 00:28:19,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:19,968:INFO:Checking exceptions
2024-09-01 00:28:19,968:INFO:Importing libraries
2024-09-01 00:28:19,968:INFO:Copying training dataset
2024-09-01 00:28:19,980:INFO:Defining folds
2024-09-01 00:28:19,980:INFO:Declaring metric variables
2024-09-01 00:28:19,983:INFO:Importing untrained model
2024-09-01 00:28:19,989:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 00:28:19,997:INFO:Starting cross validation
2024-09-01 00:28:20,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:28,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,108:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,112:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,122:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,127:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,141:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,212:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,220:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,239:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,245:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,248:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,272:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,275:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,287:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,292:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,294:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,301:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,323:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:28,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:28,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:28,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:28,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:33,788:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,801:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:33,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,829:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:33,833:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,855:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:33,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:33,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:33,873:INFO:Calculating mean and std
2024-09-01 00:28:33,874:INFO:Creating metrics dataframe
2024-09-01 00:28:33,879:INFO:Uploading results into container
2024-09-01 00:28:33,880:INFO:Uploading model into container now
2024-09-01 00:28:33,880:INFO:_master_model_container: 24
2024-09-01 00:28:33,880:INFO:_display_container: 3
2024-09-01 00:28:33,881:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 00:28:33,881:INFO:create_model() successfully completed......................................
2024-09-01 00:28:34,042:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:34,042:INFO:Creating metrics dataframe
2024-09-01 00:28:34,053:INFO:Initializing Linear Discriminant Analysis
2024-09-01 00:28:34,053:INFO:Total runtime is 0.43907773892084756 minutes
2024-09-01 00:28:34,056:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:34,056:INFO:Initializing create_model()
2024-09-01 00:28:34,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:34,057:INFO:Checking exceptions
2024-09-01 00:28:34,057:INFO:Importing libraries
2024-09-01 00:28:34,057:INFO:Copying training dataset
2024-09-01 00:28:34,068:INFO:Defining folds
2024-09-01 00:28:34,068:INFO:Declaring metric variables
2024-09-01 00:28:34,071:INFO:Importing untrained model
2024-09-01 00:28:34,077:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 00:28:34,086:INFO:Starting cross validation
2024-09-01 00:28:34,088:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:34,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,358:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,410:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,414:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,419:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,422:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,433:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,535:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 00:28:34,571:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,577:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:34,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:34,587:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:34,589:INFO:Calculating mean and std
2024-09-01 00:28:34,590:INFO:Creating metrics dataframe
2024-09-01 00:28:34,593:INFO:Uploading results into container
2024-09-01 00:28:34,595:INFO:Uploading model into container now
2024-09-01 00:28:34,595:INFO:_master_model_container: 25
2024-09-01 00:28:34,595:INFO:_display_container: 3
2024-09-01 00:28:34,596:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 00:28:34,596:INFO:create_model() successfully completed......................................
2024-09-01 00:28:34,725:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:34,725:INFO:Creating metrics dataframe
2024-09-01 00:28:34,739:INFO:Initializing Extra Trees Classifier
2024-09-01 00:28:34,739:INFO:Total runtime is 0.4505175789197286 minutes
2024-09-01 00:28:34,744:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:34,744:INFO:Initializing create_model()
2024-09-01 00:28:34,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:34,744:INFO:Checking exceptions
2024-09-01 00:28:34,745:INFO:Importing libraries
2024-09-01 00:28:34,745:INFO:Copying training dataset
2024-09-01 00:28:34,754:INFO:Defining folds
2024-09-01 00:28:34,754:INFO:Declaring metric variables
2024-09-01 00:28:34,759:INFO:Importing untrained model
2024-09-01 00:28:34,766:INFO:Extra Trees Classifier Imported successfully
2024-09-01 00:28:34,772:INFO:Starting cross validation
2024-09-01 00:28:34,776:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:35,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,484:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,498:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,503:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,504:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,523:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,525:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,529:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,572:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,589:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,595:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,610:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,619:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,625:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,652:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,654:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,656:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:35,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:35,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:35,929:INFO:Calculating mean and std
2024-09-01 00:28:35,930:INFO:Creating metrics dataframe
2024-09-01 00:28:35,932:INFO:Uploading results into container
2024-09-01 00:28:35,932:INFO:Uploading model into container now
2024-09-01 00:28:35,933:INFO:_master_model_container: 26
2024-09-01 00:28:35,933:INFO:_display_container: 3
2024-09-01 00:28:35,934:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 00:28:35,934:INFO:create_model() successfully completed......................................
2024-09-01 00:28:36,106:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:36,106:INFO:Creating metrics dataframe
2024-09-01 00:28:36,121:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 00:28:36,121:INFO:Total runtime is 0.47353917360305786 minutes
2024-09-01 00:28:36,124:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:36,126:INFO:Initializing create_model()
2024-09-01 00:28:36,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:36,126:INFO:Checking exceptions
2024-09-01 00:28:36,126:INFO:Importing libraries
2024-09-01 00:28:36,126:INFO:Copying training dataset
2024-09-01 00:28:36,136:INFO:Defining folds
2024-09-01 00:28:36,136:INFO:Declaring metric variables
2024-09-01 00:28:36,140:INFO:Importing untrained model
2024-09-01 00:28:36,147:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 00:28:36,154:INFO:Starting cross validation
2024-09-01 00:28:36,157:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:38,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,050:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,055:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,075:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,293:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,425:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,467:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,618:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,626:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,630:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,650:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,671:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,683:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,688:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,693:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,706:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:38,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,751:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,755:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:38,760:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:38,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,293:INFO:Calculating mean and std
2024-09-01 00:28:39,295:INFO:Creating metrics dataframe
2024-09-01 00:28:39,297:INFO:Uploading results into container
2024-09-01 00:28:39,298:INFO:Uploading model into container now
2024-09-01 00:28:39,299:INFO:_master_model_container: 27
2024-09-01 00:28:39,299:INFO:_display_container: 3
2024-09-01 00:28:39,299:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 00:28:39,299:INFO:create_model() successfully completed......................................
2024-09-01 00:28:39,443:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:39,443:INFO:Creating metrics dataframe
2024-09-01 00:28:39,466:INFO:Initializing Dummy Classifier
2024-09-01 00:28:39,466:INFO:Total runtime is 0.5293021082878113 minutes
2024-09-01 00:28:39,473:INFO:SubProcess create_model() called ==================================
2024-09-01 00:28:39,474:INFO:Initializing create_model()
2024-09-01 00:28:39,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000013817E80ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:39,474:INFO:Checking exceptions
2024-09-01 00:28:39,475:INFO:Importing libraries
2024-09-01 00:28:39,475:INFO:Copying training dataset
2024-09-01 00:28:39,486:INFO:Defining folds
2024-09-01 00:28:39,486:INFO:Declaring metric variables
2024-09-01 00:28:39,491:INFO:Importing untrained model
2024-09-01 00:28:39,496:INFO:Dummy Classifier Imported successfully
2024-09-01 00:28:39,504:INFO:Starting cross validation
2024-09-01 00:28:39,507:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 00:28:39,734:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,745:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,749:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,754:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,766:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,772:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,778:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,782:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,784:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,786:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,787:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,795:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,823:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,834:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 00:28:39,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,922:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 00:28:39,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,928:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 00:28:39,929:INFO:Calculating mean and std
2024-09-01 00:28:39,930:INFO:Creating metrics dataframe
2024-09-01 00:28:39,932:INFO:Uploading results into container
2024-09-01 00:28:39,933:INFO:Uploading model into container now
2024-09-01 00:28:39,933:INFO:_master_model_container: 28
2024-09-01 00:28:39,933:INFO:_display_container: 3
2024-09-01 00:28:39,933:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 00:28:39,933:INFO:create_model() successfully completed......................................
2024-09-01 00:28:40,063:INFO:SubProcess create_model() end ==================================
2024-09-01 00:28:40,064:INFO:Creating metrics dataframe
2024-09-01 00:28:40,094:INFO:Initializing create_model()
2024-09-01 00:28:40,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 00:28:40,094:INFO:Checking exceptions
2024-09-01 00:28:40,097:INFO:Importing libraries
2024-09-01 00:28:40,097:INFO:Copying training dataset
2024-09-01 00:28:40,130:INFO:Defining folds
2024-09-01 00:28:40,131:INFO:Declaring metric variables
2024-09-01 00:28:40,131:INFO:Importing untrained model
2024-09-01 00:28:40,131:INFO:Declaring custom model
2024-09-01 00:28:40,132:INFO:Logistic Regression Imported successfully
2024-09-01 00:28:40,134:INFO:Cross validation set to False
2024-09-01 00:28:40,134:INFO:Fitting Model
2024-09-01 00:28:41,264:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:41,264:INFO:create_model() successfully completed......................................
2024-09-01 00:28:41,441:INFO:_master_model_container: 28
2024-09-01 00:28:41,441:INFO:_display_container: 3
2024-09-01 00:28:41,442:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 00:28:41,442:INFO:compare_models() successfully completed......................................
2024-09-01 00:28:41,509:INFO:Initializing plot_model()
2024-09-01 00:28:41,509:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:41,509:INFO:Checking exceptions
2024-09-01 00:28:41,519:INFO:Preloading libraries
2024-09-01 00:28:41,520:INFO:Copying training dataset
2024-09-01 00:28:41,520:INFO:Plot type: class_report
2024-09-01 00:28:41,931:INFO:Fitting Model
2024-09-01 00:28:41,931:INFO:Scoring test/hold-out set
2024-09-01 00:28:42,184:INFO:Visual Rendered Successfully
2024-09-01 00:28:42,317:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:42,331:INFO:Initializing plot_model()
2024-09-01 00:28:42,331:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:42,332:INFO:Checking exceptions
2024-09-01 00:28:42,341:INFO:Preloading libraries
2024-09-01 00:28:42,343:INFO:Copying training dataset
2024-09-01 00:28:42,343:INFO:Plot type: confusion_matrix
2024-09-01 00:28:42,889:INFO:Fitting Model
2024-09-01 00:28:42,889:INFO:Scoring test/hold-out set
2024-09-01 00:28:43,062:INFO:Visual Rendered Successfully
2024-09-01 00:28:43,199:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:43,214:INFO:Initializing plot_model()
2024-09-01 00:28:43,214:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:43,214:INFO:Checking exceptions
2024-09-01 00:28:43,225:INFO:Preloading libraries
2024-09-01 00:28:43,225:INFO:Copying training dataset
2024-09-01 00:28:43,225:INFO:Plot type: auc
2024-09-01 00:28:43,508:INFO:Fitting Model
2024-09-01 00:28:43,511:INFO:Scoring test/hold-out set
2024-09-01 00:28:43,764:INFO:Visual Rendered Successfully
2024-09-01 00:28:43,898:INFO:plot_model() successfully completed......................................
2024-09-01 00:28:43,917:INFO:Initializing plot_model()
2024-09-01 00:28:43,917:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001382536CC10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 00:28:43,917:INFO:Checking exceptions
2024-09-01 00:28:43,927:INFO:Preloading libraries
2024-09-01 00:28:43,928:INFO:Copying training dataset
2024-09-01 00:28:43,928:INFO:Plot type: learning
2024-09-01 00:28:44,228:INFO:Fitting Model
2024-09-01 00:32:01,214:INFO:Visual Rendered Successfully
2024-09-01 00:32:01,345:INFO:plot_model() successfully completed......................................
2024-09-01 03:21:18,813:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 03:21:18,813:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 03:21:18,813:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 03:21:18,813:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-01 03:21:19,532:INFO:PyCaret ClassificationExperiment
2024-09-01 03:21:19,532:INFO:Logging name: clf-default-name
2024-09-01 03:21:19,532:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-01 03:21:19,532:INFO:version 3.3.2
2024-09-01 03:21:19,532:INFO:Initializing setup()
2024-09-01 03:21:19,532:INFO:self.USI: 50eb
2024-09-01 03:21:19,532:INFO:self._variable_keys: {'X_test', 'fix_imbalance', 'y_train', 'log_plots_param', '_available_plots', 'fold_generator', 'gpu_param', 'USI', 'y', '_ml_usecase', 'data', 'logging_param', 'X_train', 'exp_id', 'memory', 'is_multiclass', 'target_param', 'X', 'fold_shuffle_param', 'y_test', 'seed', 'gpu_n_jobs_param', 'fold_groups_param', 'html_param', 'exp_name_log', 'n_jobs_param', 'pipeline', 'idx'}
2024-09-01 03:21:19,532:INFO:Checking environment
2024-09-01 03:21:19,532:INFO:python_version: 3.11.5
2024-09-01 03:21:19,532:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2024-09-01 03:21:19,532:INFO:machine: AMD64
2024-09-01 03:21:19,532:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-01 03:21:19,533:INFO:Memory: svmem(total=8269512704, available=631660544, percent=92.4, used=7637852160, free=631660544)
2024-09-01 03:21:19,533:INFO:Physical Core: 4
2024-09-01 03:21:19,533:INFO:Logical Core: 8
2024-09-01 03:21:19,533:INFO:Checking libraries
2024-09-01 03:21:19,533:INFO:System:
2024-09-01 03:21:19,533:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2024-09-01 03:21:19,533:INFO:executable: c:\Users\chand\anaconda3\python.exe
2024-09-01 03:21:19,533:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-01 03:21:19,533:INFO:PyCaret required dependencies:
2024-09-01 03:21:20,798:INFO:                 pip: 23.2.1
2024-09-01 03:21:20,798:INFO:          setuptools: 68.0.0
2024-09-01 03:21:20,798:INFO:             pycaret: 3.3.2
2024-09-01 03:21:20,798:INFO:             IPython: 8.15.0
2024-09-01 03:21:20,798:INFO:          ipywidgets: 8.0.4
2024-09-01 03:21:20,798:INFO:                tqdm: 4.65.0
2024-09-01 03:21:20,798:INFO:               numpy: 1.24.3
2024-09-01 03:21:20,798:INFO:              pandas: 2.0.3
2024-09-01 03:21:20,798:INFO:              jinja2: 3.1.2
2024-09-01 03:21:20,798:INFO:               scipy: 1.11.1
2024-09-01 03:21:20,798:INFO:              joblib: 1.2.0
2024-09-01 03:21:20,799:INFO:             sklearn: 1.4.2
2024-09-01 03:21:20,799:INFO:                pyod: 2.0.1
2024-09-01 03:21:20,799:INFO:            imblearn: 0.12.3
2024-09-01 03:21:20,799:INFO:   category_encoders: 2.6.3
2024-09-01 03:21:20,799:INFO:            lightgbm: 4.5.0
2024-09-01 03:21:20,799:INFO:               numba: 0.57.1
2024-09-01 03:21:20,799:INFO:            requests: 2.31.0
2024-09-01 03:21:20,799:INFO:          matplotlib: 3.7.2
2024-09-01 03:21:20,799:INFO:          scikitplot: 0.3.7
2024-09-01 03:21:20,799:INFO:         yellowbrick: 1.5
2024-09-01 03:21:20,799:INFO:              plotly: 5.24.0
2024-09-01 03:21:20,799:INFO:    plotly-resampler: Not installed
2024-09-01 03:21:20,799:INFO:             kaleido: 0.2.1
2024-09-01 03:21:20,799:INFO:           schemdraw: 0.15
2024-09-01 03:21:20,799:INFO:         statsmodels: 0.14.0
2024-09-01 03:21:20,799:INFO:              sktime: 0.26.0
2024-09-01 03:21:20,799:INFO:               tbats: 1.1.3
2024-09-01 03:21:20,799:INFO:            pmdarima: 2.0.4
2024-09-01 03:21:20,799:INFO:              psutil: 5.9.0
2024-09-01 03:21:20,799:INFO:          markupsafe: 2.1.1
2024-09-01 03:21:20,799:INFO:             pickle5: Not installed
2024-09-01 03:21:20,799:INFO:         cloudpickle: 2.2.1
2024-09-01 03:21:20,799:INFO:         deprecation: 2.1.0
2024-09-01 03:21:20,799:INFO:              xxhash: 2.0.2
2024-09-01 03:21:20,799:INFO:           wurlitzer: Not installed
2024-09-01 03:21:20,799:INFO:PyCaret optional dependencies:
2024-09-01 03:21:20,876:INFO:                shap: Not installed
2024-09-01 03:21:20,877:INFO:           interpret: Not installed
2024-09-01 03:21:20,877:INFO:                umap: Not installed
2024-09-01 03:21:20,877:INFO:     ydata_profiling: Not installed
2024-09-01 03:21:20,877:INFO:  explainerdashboard: Not installed
2024-09-01 03:21:20,877:INFO:             autoviz: Not installed
2024-09-01 03:21:20,877:INFO:           fairlearn: Not installed
2024-09-01 03:21:20,877:INFO:          deepchecks: Not installed
2024-09-01 03:21:20,877:INFO:             xgboost: 2.1.1
2024-09-01 03:21:20,877:INFO:            catboost: Not installed
2024-09-01 03:21:20,877:INFO:              kmodes: Not installed
2024-09-01 03:21:20,877:INFO:             mlxtend: 0.22.0
2024-09-01 03:21:20,877:INFO:       statsforecast: Not installed
2024-09-01 03:21:20,877:INFO:        tune_sklearn: Not installed
2024-09-01 03:21:20,877:INFO:                 ray: Not installed
2024-09-01 03:21:20,877:INFO:            hyperopt: Not installed
2024-09-01 03:21:20,877:INFO:              optuna: Not installed
2024-09-01 03:21:20,877:INFO:               skopt: Not installed
2024-09-01 03:21:20,877:INFO:              mlflow: Not installed
2024-09-01 03:21:20,877:INFO:              gradio: Not installed
2024-09-01 03:21:20,878:INFO:             fastapi: Not installed
2024-09-01 03:21:20,878:INFO:             uvicorn: Not installed
2024-09-01 03:21:20,878:INFO:              m2cgen: Not installed
2024-09-01 03:21:20,878:INFO:           evidently: Not installed
2024-09-01 03:21:20,878:INFO:               fugue: Not installed
2024-09-01 03:21:20,878:INFO:           streamlit: 1.38.0
2024-09-01 03:21:20,878:INFO:             prophet: Not installed
2024-09-01 03:21:20,878:INFO:None
2024-09-01 03:21:20,878:INFO:Set up data.
2024-09-01 03:21:20,887:INFO:Set up folding strategy.
2024-09-01 03:21:20,887:INFO:Set up train/test split.
2024-09-01 03:21:20,901:INFO:Set up index.
2024-09-01 03:21:20,901:INFO:Assigning column types.
2024-09-01 03:21:20,910:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-01 03:21:20,958:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 03:21:20,963:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 03:21:20,998:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,047:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-01 03:21:21,047:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 03:21:21,071:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,076:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-01 03:21:21,116:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 03:21:21,144:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,146:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,185:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-01 03:21:21,209:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,211:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,212:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-01 03:21:21,276:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,278:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,340:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,345:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,349:INFO:Preparing preprocessing pipeline...
2024-09-01 03:21:21,352:INFO:Set up label encoding.
2024-09-01 03:21:21,352:INFO:Set up simple imputation.
2024-09-01 03:21:21,355:INFO:Set up encoding of categorical features.
2024-09-01 03:21:21,357:INFO:Set up column name cleaning.
2024-09-01 03:21:21,503:INFO:Finished creating preprocessing pipeline.
2024-09-01 03:21:21,511:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\chand\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['UDI', 'Air temperature [K]',
                                             'Process temperature [K]',
                                             'Rotational speed [rpm]',
                                             'Torque [Nm]', 'Tool wear [min]...
                 TransformerWrapper(exclude=None, include=['Product ID'],
                                    transformer=TargetEncoder(cols=['Product '
                                                                    'ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-01 03:21:21,511:INFO:Creating final display dataframe.
2024-09-01 03:21:21,806:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16               Fold Generator   
17                  Fold Number   
18                     CPU Jobs   
19                      Use GPU   
20               Log Experiment   
21              Experiment Name   
22                          USI   

                                                Value  
0                                                  42  
1                                        Failure Type  
2                                          Multiclass  
3   Heat Dissipation Failure: 0, No Failure: 1, Ov...  
4                                           (678, 21)  
5                                           (678, 23)  
6                                           (474, 23)  
7                                           (204, 23)  
8                                                  18  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                    StratifiedKFold  
17                                                 10  
18                                                 -1  
19                                              False  
20                                              False  
21                                   clf-default-name  
22                                               50eb  
2024-09-01 03:21:21,887:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,956:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-01 03:21:21,959:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-01 03:21:21,961:INFO:setup() successfully completed in 2.43s...............
2024-09-01 03:21:21,969:INFO:Initializing compare_models()
2024-09-01 03:21:21,969:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 03:21:21,970:INFO:Checking exceptions
2024-09-01 03:21:21,979:INFO:Preparing display monitor
2024-09-01 03:21:22,006:INFO:Initializing Logistic Regression
2024-09-01 03:21:22,007:INFO:Total runtime is 1.6939640045166015e-05 minutes
2024-09-01 03:21:22,011:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:22,012:INFO:Initializing create_model()
2024-09-01 03:21:22,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:22,012:INFO:Checking exceptions
2024-09-01 03:21:22,012:INFO:Importing libraries
2024-09-01 03:21:22,012:INFO:Copying training dataset
2024-09-01 03:21:22,022:INFO:Defining folds
2024-09-01 03:21:22,023:INFO:Declaring metric variables
2024-09-01 03:21:22,026:INFO:Importing untrained model
2024-09-01 03:21:22,030:INFO:Logistic Regression Imported successfully
2024-09-01 03:21:22,040:INFO:Starting cross validation
2024-09-01 03:21:22,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:32,068:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,069:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,084:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,109:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,142:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,171:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,173:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,184:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,195:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,200:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,200:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,205:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,209:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,217:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,222:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,225:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,241:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,245:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,249:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,253:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,733:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,742:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:32,760:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,763:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,768:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:32,770:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,771:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,774:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:32,783:INFO:Calculating mean and std
2024-09-01 03:21:32,784:INFO:Creating metrics dataframe
2024-09-01 03:21:32,786:INFO:Uploading results into container
2024-09-01 03:21:32,787:INFO:Uploading model into container now
2024-09-01 03:21:32,787:INFO:_master_model_container: 1
2024-09-01 03:21:32,787:INFO:_display_container: 2
2024-09-01 03:21:32,788:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 03:21:32,788:INFO:create_model() successfully completed......................................
2024-09-01 03:21:32,922:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:32,922:INFO:Creating metrics dataframe
2024-09-01 03:21:32,928:INFO:Initializing K Neighbors Classifier
2024-09-01 03:21:32,929:INFO:Total runtime is 0.18204795916875205 minutes
2024-09-01 03:21:32,936:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:32,936:INFO:Initializing create_model()
2024-09-01 03:21:32,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:32,936:INFO:Checking exceptions
2024-09-01 03:21:32,936:INFO:Importing libraries
2024-09-01 03:21:32,936:INFO:Copying training dataset
2024-09-01 03:21:32,943:INFO:Defining folds
2024-09-01 03:21:32,943:INFO:Declaring metric variables
2024-09-01 03:21:32,947:INFO:Importing untrained model
2024-09-01 03:21:32,951:INFO:K Neighbors Classifier Imported successfully
2024-09-01 03:21:32,958:INFO:Starting cross validation
2024-09-01 03:21:32,960:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:33,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,380:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,412:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,418:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,420:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,437:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,440:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,484:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,486:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,584:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,586:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,587:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,590:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,590:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,592:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,595:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,603:INFO:Calculating mean and std
2024-09-01 03:21:33,604:INFO:Creating metrics dataframe
2024-09-01 03:21:33,606:INFO:Uploading results into container
2024-09-01 03:21:33,607:INFO:Uploading model into container now
2024-09-01 03:21:33,608:INFO:_master_model_container: 2
2024-09-01 03:21:33,608:INFO:_display_container: 2
2024-09-01 03:21:33,608:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 03:21:33,608:INFO:create_model() successfully completed......................................
2024-09-01 03:21:33,749:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:33,749:INFO:Creating metrics dataframe
2024-09-01 03:21:33,756:INFO:Initializing Naive Bayes
2024-09-01 03:21:33,756:INFO:Total runtime is 0.1958382844924927 minutes
2024-09-01 03:21:33,759:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:33,760:INFO:Initializing create_model()
2024-09-01 03:21:33,760:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:33,760:INFO:Checking exceptions
2024-09-01 03:21:33,760:INFO:Importing libraries
2024-09-01 03:21:33,761:INFO:Copying training dataset
2024-09-01 03:21:33,769:INFO:Defining folds
2024-09-01 03:21:33,769:INFO:Declaring metric variables
2024-09-01 03:21:33,773:INFO:Importing untrained model
2024-09-01 03:21:33,776:INFO:Naive Bayes Imported successfully
2024-09-01 03:21:33,786:INFO:Starting cross validation
2024-09-01 03:21:33,788:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:33,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,970:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,972:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,974:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:33,974:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,974:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,976:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,991:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,995:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:33,999:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,003:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,054:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,055:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,058:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,091:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,094:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,101:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,106:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,107:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,113:INFO:Calculating mean and std
2024-09-01 03:21:34,114:INFO:Creating metrics dataframe
2024-09-01 03:21:34,116:INFO:Uploading results into container
2024-09-01 03:21:34,116:INFO:Uploading model into container now
2024-09-01 03:21:34,117:INFO:_master_model_container: 3
2024-09-01 03:21:34,117:INFO:_display_container: 2
2024-09-01 03:21:34,117:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 03:21:34,117:INFO:create_model() successfully completed......................................
2024-09-01 03:21:34,234:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:34,235:INFO:Creating metrics dataframe
2024-09-01 03:21:34,241:INFO:Initializing Decision Tree Classifier
2024-09-01 03:21:34,241:INFO:Total runtime is 0.2039218862851461 minutes
2024-09-01 03:21:34,244:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:34,244:INFO:Initializing create_model()
2024-09-01 03:21:34,245:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:34,245:INFO:Checking exceptions
2024-09-01 03:21:34,245:INFO:Importing libraries
2024-09-01 03:21:34,245:INFO:Copying training dataset
2024-09-01 03:21:34,254:INFO:Defining folds
2024-09-01 03:21:34,254:INFO:Declaring metric variables
2024-09-01 03:21:34,257:INFO:Importing untrained model
2024-09-01 03:21:34,264:INFO:Decision Tree Classifier Imported successfully
2024-09-01 03:21:34,270:INFO:Starting cross validation
2024-09-01 03:21:34,272:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:34,438:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,442:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,449:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,451:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,452:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,453:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,455:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,458:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,469:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,470:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,470:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,474:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,474:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,505:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,509:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,511:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,513:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,550:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,552:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,569:INFO:Calculating mean and std
2024-09-01 03:21:34,570:INFO:Creating metrics dataframe
2024-09-01 03:21:34,572:INFO:Uploading results into container
2024-09-01 03:21:34,573:INFO:Uploading model into container now
2024-09-01 03:21:34,574:INFO:_master_model_container: 4
2024-09-01 03:21:34,574:INFO:_display_container: 2
2024-09-01 03:21:34,574:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 03:21:34,574:INFO:create_model() successfully completed......................................
2024-09-01 03:21:34,701:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:34,701:INFO:Creating metrics dataframe
2024-09-01 03:21:34,708:INFO:Initializing SVM - Linear Kernel
2024-09-01 03:21:34,708:INFO:Total runtime is 0.21169825394948324 minutes
2024-09-01 03:21:34,712:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:34,712:INFO:Initializing create_model()
2024-09-01 03:21:34,712:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:34,712:INFO:Checking exceptions
2024-09-01 03:21:34,712:INFO:Importing libraries
2024-09-01 03:21:34,713:INFO:Copying training dataset
2024-09-01 03:21:34,719:INFO:Defining folds
2024-09-01 03:21:34,719:INFO:Declaring metric variables
2024-09-01 03:21:34,722:INFO:Importing untrained model
2024-09-01 03:21:34,725:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 03:21:34,734:INFO:Starting cross validation
2024-09-01 03:21:34,735:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:34,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,890:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,902:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,906:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,915:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:34,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:34,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:34,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,008:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,014:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,019:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,024:INFO:Calculating mean and std
2024-09-01 03:21:35,025:INFO:Creating metrics dataframe
2024-09-01 03:21:35,028:INFO:Uploading results into container
2024-09-01 03:21:35,028:INFO:Uploading model into container now
2024-09-01 03:21:35,029:INFO:_master_model_container: 5
2024-09-01 03:21:35,029:INFO:_display_container: 2
2024-09-01 03:21:35,029:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 03:21:35,029:INFO:create_model() successfully completed......................................
2024-09-01 03:21:35,127:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:35,127:INFO:Creating metrics dataframe
2024-09-01 03:21:35,135:INFO:Initializing Ridge Classifier
2024-09-01 03:21:35,135:INFO:Total runtime is 0.21881806453069053 minutes
2024-09-01 03:21:35,137:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:35,138:INFO:Initializing create_model()
2024-09-01 03:21:35,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:35,138:INFO:Checking exceptions
2024-09-01 03:21:35,138:INFO:Importing libraries
2024-09-01 03:21:35,138:INFO:Copying training dataset
2024-09-01 03:21:35,145:INFO:Defining folds
2024-09-01 03:21:35,145:INFO:Declaring metric variables
2024-09-01 03:21:35,149:INFO:Importing untrained model
2024-09-01 03:21:35,153:INFO:Ridge Classifier Imported successfully
2024-09-01 03:21:35,160:INFO:Starting cross validation
2024-09-01 03:21:35,162:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:35,361:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,368:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,369:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,371:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,379:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,381:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,401:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,476:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,480:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,483:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,483:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:35,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,489:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:35,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:35,498:INFO:Calculating mean and std
2024-09-01 03:21:35,499:INFO:Creating metrics dataframe
2024-09-01 03:21:35,501:INFO:Uploading results into container
2024-09-01 03:21:35,503:INFO:Uploading model into container now
2024-09-01 03:21:35,503:INFO:_master_model_container: 6
2024-09-01 03:21:35,503:INFO:_display_container: 2
2024-09-01 03:21:35,504:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 03:21:35,504:INFO:create_model() successfully completed......................................
2024-09-01 03:21:35,613:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:35,613:INFO:Creating metrics dataframe
2024-09-01 03:21:35,623:INFO:Initializing Random Forest Classifier
2024-09-01 03:21:35,623:INFO:Total runtime is 0.22694857915242514 minutes
2024-09-01 03:21:35,628:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:35,628:INFO:Initializing create_model()
2024-09-01 03:21:35,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:35,629:INFO:Checking exceptions
2024-09-01 03:21:35,629:INFO:Importing libraries
2024-09-01 03:21:35,629:INFO:Copying training dataset
2024-09-01 03:21:35,635:INFO:Defining folds
2024-09-01 03:21:35,635:INFO:Declaring metric variables
2024-09-01 03:21:35,638:INFO:Importing untrained model
2024-09-01 03:21:35,643:INFO:Random Forest Classifier Imported successfully
2024-09-01 03:21:35,651:INFO:Starting cross validation
2024-09-01 03:21:35,652:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:36,088:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,090:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,094:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,096:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,116:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,121:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,126:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,164:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,194:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,196:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,271:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,274:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,401:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,406:INFO:Calculating mean and std
2024-09-01 03:21:36,407:INFO:Creating metrics dataframe
2024-09-01 03:21:36,410:INFO:Uploading results into container
2024-09-01 03:21:36,411:INFO:Uploading model into container now
2024-09-01 03:21:36,411:INFO:_master_model_container: 7
2024-09-01 03:21:36,411:INFO:_display_container: 2
2024-09-01 03:21:36,411:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 03:21:36,411:INFO:create_model() successfully completed......................................
2024-09-01 03:21:36,516:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:36,517:INFO:Creating metrics dataframe
2024-09-01 03:21:36,526:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 03:21:36,527:INFO:Total runtime is 0.24200992186864217 minutes
2024-09-01 03:21:36,530:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:36,531:INFO:Initializing create_model()
2024-09-01 03:21:36,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:36,531:INFO:Checking exceptions
2024-09-01 03:21:36,531:INFO:Importing libraries
2024-09-01 03:21:36,531:INFO:Copying training dataset
2024-09-01 03:21:36,537:INFO:Defining folds
2024-09-01 03:21:36,537:INFO:Declaring metric variables
2024-09-01 03:21:36,542:INFO:Importing untrained model
2024-09-01 03:21:36,546:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 03:21:36,551:INFO:Starting cross validation
2024-09-01 03:21:36,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:36,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,694:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,714:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,714:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,718:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,718:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,719:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,721:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,721:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,722:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,722:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,722:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,723:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,726:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,726:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,726:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,737:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,739:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,740:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,776:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,779:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:21:36,800:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,802:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:36,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:36,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:36,814:INFO:Calculating mean and std
2024-09-01 03:21:36,815:INFO:Creating metrics dataframe
2024-09-01 03:21:36,817:INFO:Uploading results into container
2024-09-01 03:21:36,817:INFO:Uploading model into container now
2024-09-01 03:21:36,818:INFO:_master_model_container: 8
2024-09-01 03:21:36,818:INFO:_display_container: 2
2024-09-01 03:21:36,818:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 03:21:36,818:INFO:create_model() successfully completed......................................
2024-09-01 03:21:36,920:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:36,920:INFO:Creating metrics dataframe
2024-09-01 03:21:36,929:INFO:Initializing Ada Boost Classifier
2024-09-01 03:21:36,930:INFO:Total runtime is 0.24874027967453002 minutes
2024-09-01 03:21:36,933:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:36,933:INFO:Initializing create_model()
2024-09-01 03:21:36,933:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:36,933:INFO:Checking exceptions
2024-09-01 03:21:36,933:INFO:Importing libraries
2024-09-01 03:21:36,933:INFO:Copying training dataset
2024-09-01 03:21:36,938:INFO:Defining folds
2024-09-01 03:21:36,939:INFO:Declaring metric variables
2024-09-01 03:21:36,943:INFO:Importing untrained model
2024-09-01 03:21:36,946:INFO:Ada Boost Classifier Imported successfully
2024-09-01 03:21:36,952:INFO:Starting cross validation
2024-09-01 03:21:36,954:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:37,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,043:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,047:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,072:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,298:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,304:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,310:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,316:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,318:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,319:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,328:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,345:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,345:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,349:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,353:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,355:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,358:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,359:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:21:37,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,564:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,578:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:37,580:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:37,584:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:37,588:INFO:Calculating mean and std
2024-09-01 03:21:37,589:INFO:Creating metrics dataframe
2024-09-01 03:21:37,592:INFO:Uploading results into container
2024-09-01 03:21:37,593:INFO:Uploading model into container now
2024-09-01 03:21:37,594:INFO:_master_model_container: 9
2024-09-01 03:21:37,594:INFO:_display_container: 2
2024-09-01 03:21:37,594:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 03:21:37,594:INFO:create_model() successfully completed......................................
2024-09-01 03:21:37,701:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:37,701:INFO:Creating metrics dataframe
2024-09-01 03:21:37,711:INFO:Initializing Gradient Boosting Classifier
2024-09-01 03:21:37,711:INFO:Total runtime is 0.2617576837539673 minutes
2024-09-01 03:21:37,714:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:37,715:INFO:Initializing create_model()
2024-09-01 03:21:37,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:37,715:INFO:Checking exceptions
2024-09-01 03:21:37,715:INFO:Importing libraries
2024-09-01 03:21:37,715:INFO:Copying training dataset
2024-09-01 03:21:37,721:INFO:Defining folds
2024-09-01 03:21:37,721:INFO:Declaring metric variables
2024-09-01 03:21:37,728:INFO:Importing untrained model
2024-09-01 03:21:37,732:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 03:21:37,737:INFO:Starting cross validation
2024-09-01 03:21:37,739:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:39,881:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:39,883:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,887:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:39,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:39,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:39,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,915:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:39,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:39,925:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:39,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:39,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:39,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,144:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:40,147:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,150:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:40,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:40,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,153:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:40,162:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:40,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:40,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:40,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:40,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:40,173:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,252:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,258:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,265:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,267:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,271:INFO:Calculating mean and std
2024-09-01 03:21:41,272:INFO:Creating metrics dataframe
2024-09-01 03:21:41,274:INFO:Uploading results into container
2024-09-01 03:21:41,274:INFO:Uploading model into container now
2024-09-01 03:21:41,275:INFO:_master_model_container: 10
2024-09-01 03:21:41,275:INFO:_display_container: 2
2024-09-01 03:21:41,275:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 03:21:41,275:INFO:create_model() successfully completed......................................
2024-09-01 03:21:41,377:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:41,377:INFO:Creating metrics dataframe
2024-09-01 03:21:41,386:INFO:Initializing Linear Discriminant Analysis
2024-09-01 03:21:41,386:INFO:Total runtime is 0.3230006297429403 minutes
2024-09-01 03:21:41,391:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:41,392:INFO:Initializing create_model()
2024-09-01 03:21:41,392:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:41,392:INFO:Checking exceptions
2024-09-01 03:21:41,392:INFO:Importing libraries
2024-09-01 03:21:41,392:INFO:Copying training dataset
2024-09-01 03:21:41,398:INFO:Defining folds
2024-09-01 03:21:41,399:INFO:Declaring metric variables
2024-09-01 03:21:41,401:INFO:Importing untrained model
2024-09-01 03:21:41,407:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 03:21:41,414:INFO:Starting cross validation
2024-09-01 03:21:41,416:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:41,615:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,618:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,618:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,623:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,625:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,627:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,628:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,632:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,633:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,636:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,639:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,641:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,641:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,642:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,643:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,644:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,644:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,646:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,647:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,648:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 03:21:41,649:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,650:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,651:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,653:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,654:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,657:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,660:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,662:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,664:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,718:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,721:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,727:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:41,731:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,736:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:41,739:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:41,745:INFO:Calculating mean and std
2024-09-01 03:21:41,746:INFO:Creating metrics dataframe
2024-09-01 03:21:41,748:INFO:Uploading results into container
2024-09-01 03:21:41,749:INFO:Uploading model into container now
2024-09-01 03:21:41,749:INFO:_master_model_container: 11
2024-09-01 03:21:41,749:INFO:_display_container: 2
2024-09-01 03:21:41,750:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 03:21:41,750:INFO:create_model() successfully completed......................................
2024-09-01 03:21:41,867:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:41,867:INFO:Creating metrics dataframe
2024-09-01 03:21:41,876:INFO:Initializing Extra Trees Classifier
2024-09-01 03:21:41,877:INFO:Total runtime is 0.3311922311782837 minutes
2024-09-01 03:21:41,880:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:41,880:INFO:Initializing create_model()
2024-09-01 03:21:41,880:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:41,880:INFO:Checking exceptions
2024-09-01 03:21:41,880:INFO:Importing libraries
2024-09-01 03:21:41,880:INFO:Copying training dataset
2024-09-01 03:21:41,887:INFO:Defining folds
2024-09-01 03:21:41,888:INFO:Declaring metric variables
2024-09-01 03:21:41,891:INFO:Importing untrained model
2024-09-01 03:21:41,895:INFO:Extra Trees Classifier Imported successfully
2024-09-01 03:21:41,905:INFO:Starting cross validation
2024-09-01 03:21:41,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:42,298:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,307:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,308:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,311:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,314:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,317:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,329:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,342:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,346:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,402:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,403:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,546:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,552:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,557:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:42,559:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:42,564:INFO:Calculating mean and std
2024-09-01 03:21:42,565:INFO:Creating metrics dataframe
2024-09-01 03:21:42,566:INFO:Uploading results into container
2024-09-01 03:21:42,566:INFO:Uploading model into container now
2024-09-01 03:21:42,567:INFO:_master_model_container: 12
2024-09-01 03:21:42,567:INFO:_display_container: 2
2024-09-01 03:21:42,567:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 03:21:42,568:INFO:create_model() successfully completed......................................
2024-09-01 03:21:42,682:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:42,683:INFO:Creating metrics dataframe
2024-09-01 03:21:42,693:INFO:Initializing Extreme Gradient Boosting
2024-09-01 03:21:42,693:INFO:Total runtime is 0.3447782715161642 minutes
2024-09-01 03:21:42,696:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:42,696:INFO:Initializing create_model()
2024-09-01 03:21:42,696:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:42,696:INFO:Checking exceptions
2024-09-01 03:21:42,697:INFO:Importing libraries
2024-09-01 03:21:42,697:INFO:Copying training dataset
2024-09-01 03:21:42,703:INFO:Defining folds
2024-09-01 03:21:42,703:INFO:Declaring metric variables
2024-09-01 03:21:42,708:INFO:Importing untrained model
2024-09-01 03:21:42,712:INFO:Extreme Gradient Boosting Imported successfully
2024-09-01 03:21:42,717:INFO:Starting cross validation
2024-09-01 03:21:42,720:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:43,161:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,162:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,171:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,178:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,180:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,182:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,186:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,189:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,190:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,191:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,193:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,194:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,203:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,207:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,208:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,210:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:43,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:43,407:INFO:Calculating mean and std
2024-09-01 03:21:43,408:INFO:Creating metrics dataframe
2024-09-01 03:21:43,411:INFO:Uploading results into container
2024-09-01 03:21:43,411:INFO:Uploading model into container now
2024-09-01 03:21:43,412:INFO:_master_model_container: 13
2024-09-01 03:21:43,412:INFO:_display_container: 2
2024-09-01 03:21:43,413:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-01 03:21:43,413:INFO:create_model() successfully completed......................................
2024-09-01 03:21:43,552:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:43,553:INFO:Creating metrics dataframe
2024-09-01 03:21:43,564:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 03:21:43,564:INFO:Total runtime is 0.35929963588714603 minutes
2024-09-01 03:21:43,568:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:43,569:INFO:Initializing create_model()
2024-09-01 03:21:43,569:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:43,569:INFO:Checking exceptions
2024-09-01 03:21:43,569:INFO:Importing libraries
2024-09-01 03:21:43,569:INFO:Copying training dataset
2024-09-01 03:21:43,579:INFO:Defining folds
2024-09-01 03:21:43,580:INFO:Declaring metric variables
2024-09-01 03:21:43,583:INFO:Importing untrained model
2024-09-01 03:21:43,591:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 03:21:43,597:INFO:Starting cross validation
2024-09-01 03:21:43,599:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:44,981:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:44,988:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:44,990:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:44,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,005:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,011:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,013:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,263:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,264:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,269:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,472:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,475:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,631:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,638:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,640:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,643:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,813:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,942:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:45,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,991:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,995:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:45,997:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,004:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,293:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,341:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,344:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,345:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,356:INFO:Calculating mean and std
2024-09-01 03:21:46,357:INFO:Creating metrics dataframe
2024-09-01 03:21:46,360:INFO:Uploading results into container
2024-09-01 03:21:46,361:INFO:Uploading model into container now
2024-09-01 03:21:46,361:INFO:_master_model_container: 14
2024-09-01 03:21:46,361:INFO:_display_container: 2
2024-09-01 03:21:46,362:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 03:21:46,362:INFO:create_model() successfully completed......................................
2024-09-01 03:21:46,503:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:46,503:INFO:Creating metrics dataframe
2024-09-01 03:21:46,513:INFO:Initializing Dummy Classifier
2024-09-01 03:21:46,513:INFO:Total runtime is 0.4084600687026978 minutes
2024-09-01 03:21:46,519:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:46,519:INFO:Initializing create_model()
2024-09-01 03:21:46,520:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A40067D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:46,520:INFO:Checking exceptions
2024-09-01 03:21:46,520:INFO:Importing libraries
2024-09-01 03:21:46,520:INFO:Copying training dataset
2024-09-01 03:21:46,527:INFO:Defining folds
2024-09-01 03:21:46,527:INFO:Declaring metric variables
2024-09-01 03:21:46,533:INFO:Importing untrained model
2024-09-01 03:21:46,540:INFO:Dummy Classifier Imported successfully
2024-09-01 03:21:46,548:INFO:Starting cross validation
2024-09-01 03:21:46,551:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:46,705:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,710:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,715:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,725:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,726:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,727:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,728:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,729:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,730:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,735:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,743:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,744:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,753:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,756:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,758:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,769:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,773:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,775:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,777:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,785:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,790:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,792:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,844:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,846:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,862:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,863:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:46,871:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:46,876:INFO:Calculating mean and std
2024-09-01 03:21:46,877:INFO:Creating metrics dataframe
2024-09-01 03:21:46,879:INFO:Uploading results into container
2024-09-01 03:21:46,880:INFO:Uploading model into container now
2024-09-01 03:21:46,881:INFO:_master_model_container: 15
2024-09-01 03:21:46,881:INFO:_display_container: 2
2024-09-01 03:21:46,881:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 03:21:46,881:INFO:create_model() successfully completed......................................
2024-09-01 03:21:47,034:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:47,035:INFO:Creating metrics dataframe
2024-09-01 03:21:47,061:INFO:Initializing create_model()
2024-09-01 03:21:47,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:47,062:INFO:Checking exceptions
2024-09-01 03:21:47,064:INFO:Importing libraries
2024-09-01 03:21:47,064:INFO:Copying training dataset
2024-09-01 03:21:47,073:INFO:Defining folds
2024-09-01 03:21:47,073:INFO:Declaring metric variables
2024-09-01 03:21:47,073:INFO:Importing untrained model
2024-09-01 03:21:47,073:INFO:Declaring custom model
2024-09-01 03:21:47,075:INFO:Random Forest Classifier Imported successfully
2024-09-01 03:21:47,077:INFO:Cross validation set to False
2024-09-01 03:21:47,077:INFO:Fitting Model
2024-09-01 03:21:47,330:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 03:21:47,330:INFO:create_model() successfully completed......................................
2024-09-01 03:21:47,525:INFO:_master_model_container: 15
2024-09-01 03:21:47,525:INFO:_display_container: 2
2024-09-01 03:21:47,525:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 03:21:47,525:INFO:compare_models() successfully completed......................................
2024-09-01 03:21:47,550:INFO:Initializing plot_model()
2024-09-01 03:21:47,550:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:21:47,551:INFO:Checking exceptions
2024-09-01 03:21:47,599:INFO:Preloading libraries
2024-09-01 03:21:47,609:INFO:Copying training dataset
2024-09-01 03:21:47,609:INFO:Plot type: class_report
2024-09-01 03:21:48,042:INFO:Fitting Model
2024-09-01 03:21:48,042:INFO:Scoring test/hold-out set
2024-09-01 03:21:48,411:INFO:Visual Rendered Successfully
2024-09-01 03:21:48,540:INFO:plot_model() successfully completed......................................
2024-09-01 03:21:48,560:INFO:Initializing plot_model()
2024-09-01 03:21:48,561:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:21:48,561:INFO:Checking exceptions
2024-09-01 03:21:48,581:INFO:Preloading libraries
2024-09-01 03:21:48,592:INFO:Copying training dataset
2024-09-01 03:21:48,592:INFO:Plot type: confusion_matrix
2024-09-01 03:21:48,891:INFO:Fitting Model
2024-09-01 03:21:48,891:INFO:Scoring test/hold-out set
2024-09-01 03:21:49,078:INFO:Visual Rendered Successfully
2024-09-01 03:21:49,194:INFO:plot_model() successfully completed......................................
2024-09-01 03:21:49,209:INFO:Initializing plot_model()
2024-09-01 03:21:49,209:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:21:49,209:INFO:Checking exceptions
2024-09-01 03:21:49,230:INFO:Preloading libraries
2024-09-01 03:21:49,240:INFO:Copying training dataset
2024-09-01 03:21:49,240:INFO:Plot type: auc
2024-09-01 03:21:49,560:INFO:Fitting Model
2024-09-01 03:21:49,561:INFO:Scoring test/hold-out set
2024-09-01 03:21:49,825:INFO:Visual Rendered Successfully
2024-09-01 03:21:49,951:INFO:plot_model() successfully completed......................................
2024-09-01 03:21:49,966:INFO:Initializing plot_model()
2024-09-01 03:21:49,966:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:21:49,966:INFO:Checking exceptions
2024-09-01 03:21:49,989:INFO:Preloading libraries
2024-09-01 03:21:49,999:INFO:Copying training dataset
2024-09-01 03:21:49,999:INFO:Plot type: learning
2024-09-01 03:21:50,359:INFO:Fitting Model
2024-09-01 03:21:56,220:INFO:Visual Rendered Successfully
2024-09-01 03:21:56,339:INFO:plot_model() successfully completed......................................
2024-09-01 03:21:56,375:INFO:Initializing compare_models()
2024-09-01 03:21:56,375:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Cost-Benefit, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Cost-Benefit', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-01 03:21:56,375:INFO:Checking exceptions
2024-09-01 03:21:56,380:INFO:Preparing display monitor
2024-09-01 03:21:56,415:INFO:Initializing Logistic Regression
2024-09-01 03:21:56,415:INFO:Total runtime is 0.0 minutes
2024-09-01 03:21:56,422:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:56,424:INFO:Initializing create_model()
2024-09-01 03:21:56,424:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:56,425:INFO:Checking exceptions
2024-09-01 03:21:56,425:INFO:Importing libraries
2024-09-01 03:21:56,425:INFO:Copying training dataset
2024-09-01 03:21:56,434:INFO:Defining folds
2024-09-01 03:21:56,435:INFO:Declaring metric variables
2024-09-01 03:21:56,439:INFO:Importing untrained model
2024-09-01 03:21:56,445:INFO:Logistic Regression Imported successfully
2024-09-01 03:21:56,454:INFO:Starting cross validation
2024-09-01 03:21:56,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:56,851:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,863:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,867:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,879:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,886:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,906:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,906:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:56,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,912:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,915:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,922:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,925:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,932:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,933:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,937:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,947:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:56,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:56,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:56,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:57,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-01 03:21:57,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:57,164:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:57,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,171:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,174:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,175:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,180:INFO:Calculating mean and std
2024-09-01 03:21:57,180:INFO:Creating metrics dataframe
2024-09-01 03:21:57,182:INFO:Uploading results into container
2024-09-01 03:21:57,182:INFO:Uploading model into container now
2024-09-01 03:21:57,183:INFO:_master_model_container: 16
2024-09-01 03:21:57,183:INFO:_display_container: 3
2024-09-01 03:21:57,183:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 03:21:57,183:INFO:create_model() successfully completed......................................
2024-09-01 03:21:57,295:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:57,295:INFO:Creating metrics dataframe
2024-09-01 03:21:57,301:INFO:Initializing K Neighbors Classifier
2024-09-01 03:21:57,301:INFO:Total runtime is 0.014760339260101318 minutes
2024-09-01 03:21:57,304:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:57,305:INFO:Initializing create_model()
2024-09-01 03:21:57,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:57,305:INFO:Checking exceptions
2024-09-01 03:21:57,305:INFO:Importing libraries
2024-09-01 03:21:57,305:INFO:Copying training dataset
2024-09-01 03:21:57,311:INFO:Defining folds
2024-09-01 03:21:57,311:INFO:Declaring metric variables
2024-09-01 03:21:57,314:INFO:Importing untrained model
2024-09-01 03:21:57,318:INFO:K Neighbors Classifier Imported successfully
2024-09-01 03:21:57,327:INFO:Starting cross validation
2024-09-01 03:21:57,329:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:57,560:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,564:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,565:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,566:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:57,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,571:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,574:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,575:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,576:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,576:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,577:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,580:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:57,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,585:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,576:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:57,587:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,589:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,590:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,592:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,592:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,592:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,594:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,596:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,598:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,599:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

 metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,599:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,601:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:57,602:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,602:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,605:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,608:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,611:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,709:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,711:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,712:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:57,713:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,716:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,717:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,718:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,720:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:57,724:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:57,726:INFO:Calculating mean and std
2024-09-01 03:21:57,727:INFO:Creating metrics dataframe
2024-09-01 03:21:57,728:INFO:Uploading results into container
2024-09-01 03:21:57,729:INFO:Uploading model into container now
2024-09-01 03:21:57,729:INFO:_master_model_container: 17
2024-09-01 03:21:57,729:INFO:_display_container: 3
2024-09-01 03:21:57,729:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-01 03:21:57,729:INFO:create_model() successfully completed......................................
2024-09-01 03:21:57,829:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:57,829:INFO:Creating metrics dataframe
2024-09-01 03:21:57,836:INFO:Initializing Naive Bayes
2024-09-01 03:21:57,836:INFO:Total runtime is 0.023670093218485514 minutes
2024-09-01 03:21:57,841:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:57,841:INFO:Initializing create_model()
2024-09-01 03:21:57,841:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:57,841:INFO:Checking exceptions
2024-09-01 03:21:57,841:INFO:Importing libraries
2024-09-01 03:21:57,841:INFO:Copying training dataset
2024-09-01 03:21:57,847:INFO:Defining folds
2024-09-01 03:21:57,847:INFO:Declaring metric variables
2024-09-01 03:21:57,850:INFO:Importing untrained model
2024-09-01 03:21:57,854:INFO:Naive Bayes Imported successfully
2024-09-01 03:21:57,862:INFO:Starting cross validation
2024-09-01 03:21:57,863:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:57,999:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,004:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,006:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,007:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,009:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,012:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,013:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,013:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,015:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,028:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,030:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,032:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,034:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,037:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,039:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,040:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,041:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,042:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,044:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,046:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,048:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,049:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,052:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,052:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,055:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,056:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,057:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,061:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,125:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,126:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,126:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,127:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,128:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,132:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,132:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,134:INFO:Calculating mean and std
2024-09-01 03:21:58,135:INFO:Creating metrics dataframe
2024-09-01 03:21:58,136:INFO:Uploading results into container
2024-09-01 03:21:58,138:INFO:Uploading model into container now
2024-09-01 03:21:58,139:INFO:_master_model_container: 18
2024-09-01 03:21:58,139:INFO:_display_container: 3
2024-09-01 03:21:58,139:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-01 03:21:58,139:INFO:create_model() successfully completed......................................
2024-09-01 03:21:58,245:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:58,246:INFO:Creating metrics dataframe
2024-09-01 03:21:58,252:INFO:Initializing Decision Tree Classifier
2024-09-01 03:21:58,252:INFO:Total runtime is 0.03061753511428833 minutes
2024-09-01 03:21:58,257:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:58,257:INFO:Initializing create_model()
2024-09-01 03:21:58,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:58,257:INFO:Checking exceptions
2024-09-01 03:21:58,258:INFO:Importing libraries
2024-09-01 03:21:58,258:INFO:Copying training dataset
2024-09-01 03:21:58,265:INFO:Defining folds
2024-09-01 03:21:58,265:INFO:Declaring metric variables
2024-09-01 03:21:58,269:INFO:Importing untrained model
2024-09-01 03:21:58,274:INFO:Decision Tree Classifier Imported successfully
2024-09-01 03:21:58,281:INFO:Starting cross validation
2024-09-01 03:21:58,283:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:58,423:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,428:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,431:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,432:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,434:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,436:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,439:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,445:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,450:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,454:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,457:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,459:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,479:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,482:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,490:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,491:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,492:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,495:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,497:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,500:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,508:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,514:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,561:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,564:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,564:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,565:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,565:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,566:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,569:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,572:INFO:Calculating mean and std
2024-09-01 03:21:58,573:INFO:Creating metrics dataframe
2024-09-01 03:21:58,575:INFO:Uploading results into container
2024-09-01 03:21:58,575:INFO:Uploading model into container now
2024-09-01 03:21:58,576:INFO:_master_model_container: 19
2024-09-01 03:21:58,576:INFO:_display_container: 3
2024-09-01 03:21:58,577:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-09-01 03:21:58,577:INFO:create_model() successfully completed......................................
2024-09-01 03:21:58,698:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:58,698:INFO:Creating metrics dataframe
2024-09-01 03:21:58,706:INFO:Initializing SVM - Linear Kernel
2024-09-01 03:21:58,706:INFO:Total runtime is 0.03817966381708781 minutes
2024-09-01 03:21:58,710:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:58,711:INFO:Initializing create_model()
2024-09-01 03:21:58,711:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:58,712:INFO:Checking exceptions
2024-09-01 03:21:58,712:INFO:Importing libraries
2024-09-01 03:21:58,712:INFO:Copying training dataset
2024-09-01 03:21:58,719:INFO:Defining folds
2024-09-01 03:21:58,719:INFO:Declaring metric variables
2024-09-01 03:21:58,726:INFO:Importing untrained model
2024-09-01 03:21:58,729:INFO:SVM - Linear Kernel Imported successfully
2024-09-01 03:21:58,738:INFO:Starting cross validation
2024-09-01 03:21:58,740:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:58,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,899:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,913:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,916:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,922:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,925:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,929:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,931:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,936:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,944:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:58,972:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:58,974:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,976:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,977:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:58,979:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:58,982:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,016:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,017:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,022:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,026:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,027:INFO:Calculating mean and std
2024-09-01 03:21:59,028:INFO:Creating metrics dataframe
2024-09-01 03:21:59,030:INFO:Uploading results into container
2024-09-01 03:21:59,030:INFO:Uploading model into container now
2024-09-01 03:21:59,031:INFO:_master_model_container: 20
2024-09-01 03:21:59,031:INFO:_display_container: 3
2024-09-01 03:21:59,031:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-01 03:21:59,031:INFO:create_model() successfully completed......................................
2024-09-01 03:21:59,131:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:59,132:INFO:Creating metrics dataframe
2024-09-01 03:21:59,140:INFO:Initializing Ridge Classifier
2024-09-01 03:21:59,140:INFO:Total runtime is 0.04540372292200724 minutes
2024-09-01 03:21:59,142:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:59,142:INFO:Initializing create_model()
2024-09-01 03:21:59,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:59,143:INFO:Checking exceptions
2024-09-01 03:21:59,143:INFO:Importing libraries
2024-09-01 03:21:59,143:INFO:Copying training dataset
2024-09-01 03:21:59,182:INFO:Defining folds
2024-09-01 03:21:59,182:INFO:Declaring metric variables
2024-09-01 03:21:59,186:INFO:Importing untrained model
2024-09-01 03:21:59,191:INFO:Ridge Classifier Imported successfully
2024-09-01 03:21:59,201:INFO:Starting cross validation
2024-09-01 03:21:59,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:21:59,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,336:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,341:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,341:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,343:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,345:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,347:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,348:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,350:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,351:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,354:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,356:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,357:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,358:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,360:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,362:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,364:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,367:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,381:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,384:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,405:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,460:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:21:59,461:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,462:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,464:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,465:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,466:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,468:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:21:59,471:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:21:59,473:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,478:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:21:59,481:INFO:Calculating mean and std
2024-09-01 03:21:59,483:INFO:Creating metrics dataframe
2024-09-01 03:21:59,485:INFO:Uploading results into container
2024-09-01 03:21:59,486:INFO:Uploading model into container now
2024-09-01 03:21:59,486:INFO:_master_model_container: 21
2024-09-01 03:21:59,486:INFO:_display_container: 3
2024-09-01 03:21:59,487:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-09-01 03:21:59,487:INFO:create_model() successfully completed......................................
2024-09-01 03:21:59,600:INFO:SubProcess create_model() end ==================================
2024-09-01 03:21:59,600:INFO:Creating metrics dataframe
2024-09-01 03:21:59,609:INFO:Initializing Random Forest Classifier
2024-09-01 03:21:59,609:INFO:Total runtime is 0.05322434902191162 minutes
2024-09-01 03:21:59,612:INFO:SubProcess create_model() called ==================================
2024-09-01 03:21:59,613:INFO:Initializing create_model()
2024-09-01 03:21:59,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:21:59,613:INFO:Checking exceptions
2024-09-01 03:21:59,613:INFO:Importing libraries
2024-09-01 03:21:59,613:INFO:Copying training dataset
2024-09-01 03:21:59,618:INFO:Defining folds
2024-09-01 03:21:59,618:INFO:Declaring metric variables
2024-09-01 03:21:59,622:INFO:Importing untrained model
2024-09-01 03:21:59,625:INFO:Random Forest Classifier Imported successfully
2024-09-01 03:21:59,630:INFO:Starting cross validation
2024-09-01 03:21:59,632:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:00,128:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,129:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,130:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,132:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,134:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,136:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,138:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,139:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,140:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,142:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,142:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,143:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,144:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,148:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,149:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,150:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,155:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,164:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,188:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,192:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,193:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,196:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,202:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,276:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,280:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,281:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,290:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,481:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,483:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,485:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,488:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,493:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,496:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,499:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,502:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,506:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,508:INFO:Calculating mean and std
2024-09-01 03:22:00,509:INFO:Creating metrics dataframe
2024-09-01 03:22:00,510:INFO:Uploading results into container
2024-09-01 03:22:00,511:INFO:Uploading model into container now
2024-09-01 03:22:00,511:INFO:_master_model_container: 22
2024-09-01 03:22:00,511:INFO:_display_container: 3
2024-09-01 03:22:00,512:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-09-01 03:22:00,512:INFO:create_model() successfully completed......................................
2024-09-01 03:22:00,627:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:00,627:INFO:Creating metrics dataframe
2024-09-01 03:22:00,636:INFO:Initializing Quadratic Discriminant Analysis
2024-09-01 03:22:00,636:INFO:Total runtime is 0.07034659385681152 minutes
2024-09-01 03:22:00,639:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:00,640:INFO:Initializing create_model()
2024-09-01 03:22:00,640:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:00,640:INFO:Checking exceptions
2024-09-01 03:22:00,640:INFO:Importing libraries
2024-09-01 03:22:00,640:INFO:Copying training dataset
2024-09-01 03:22:00,646:INFO:Defining folds
2024-09-01 03:22:00,646:INFO:Declaring metric variables
2024-09-01 03:22:00,649:INFO:Importing untrained model
2024-09-01 03:22:00,654:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-01 03:22:00,660:INFO:Starting cross validation
2024-09-01 03:22:00,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:00,746:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,747:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,757:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,759:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,761:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,764:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,772:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,794:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,796:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,798:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,799:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,803:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,804:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,805:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,806:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,807:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,808:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,810:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,811:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,812:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,814:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,815:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,816:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,819:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,821:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,822:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,824:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,826:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,828:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,830:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,831:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,832:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,837:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,838:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,839:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,840:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,842:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,843:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,847:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,872:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-01 03:22:00,888:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,889:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,891:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,892:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,894:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:00,895:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,897:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,898:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:00,900:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:00,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:00,905:INFO:Calculating mean and std
2024-09-01 03:22:00,906:INFO:Creating metrics dataframe
2024-09-01 03:22:00,907:INFO:Uploading results into container
2024-09-01 03:22:00,908:INFO:Uploading model into container now
2024-09-01 03:22:00,908:INFO:_master_model_container: 23
2024-09-01 03:22:00,908:INFO:_display_container: 3
2024-09-01 03:22:00,908:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-01 03:22:00,908:INFO:create_model() successfully completed......................................
2024-09-01 03:22:01,006:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:01,006:INFO:Creating metrics dataframe
2024-09-01 03:22:01,014:INFO:Initializing Ada Boost Classifier
2024-09-01 03:22:01,014:INFO:Total runtime is 0.07664581139882405 minutes
2024-09-01 03:22:01,016:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:01,017:INFO:Initializing create_model()
2024-09-01 03:22:01,017:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:01,017:INFO:Checking exceptions
2024-09-01 03:22:01,017:INFO:Importing libraries
2024-09-01 03:22:01,017:INFO:Copying training dataset
2024-09-01 03:22:01,024:INFO:Defining folds
2024-09-01 03:22:01,024:INFO:Declaring metric variables
2024-09-01 03:22:01,027:INFO:Importing untrained model
2024-09-01 03:22:01,033:INFO:Ada Boost Classifier Imported successfully
2024-09-01 03:22:01,040:INFO:Starting cross validation
2024-09-01 03:22:01,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:01,120:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,123:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,126:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,131:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,133:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,137:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,146:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,363:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,366:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,371:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,372:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,373:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,374:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,375:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,376:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,377:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,381:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,381:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,382:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,391:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,396:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,399:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,400:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,401:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,404:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,406:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,407:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,408:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,409:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,411:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,415:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,443:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,447:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-01 03:22:01,577:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,579:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,580:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:01,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,581:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,582:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,583:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,585:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:01,586:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:01,587:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,590:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:01,591:INFO:Calculating mean and std
2024-09-01 03:22:01,592:INFO:Creating metrics dataframe
2024-09-01 03:22:01,593:INFO:Uploading results into container
2024-09-01 03:22:01,594:INFO:Uploading model into container now
2024-09-01 03:22:01,594:INFO:_master_model_container: 24
2024-09-01 03:22:01,594:INFO:_display_container: 3
2024-09-01 03:22:01,594:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-09-01 03:22:01,594:INFO:create_model() successfully completed......................................
2024-09-01 03:22:01,691:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:01,691:INFO:Creating metrics dataframe
2024-09-01 03:22:01,698:INFO:Initializing Gradient Boosting Classifier
2024-09-01 03:22:01,698:INFO:Total runtime is 0.08805034160614013 minutes
2024-09-01 03:22:01,703:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:01,704:INFO:Initializing create_model()
2024-09-01 03:22:01,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:01,704:INFO:Checking exceptions
2024-09-01 03:22:01,704:INFO:Importing libraries
2024-09-01 03:22:01,704:INFO:Copying training dataset
2024-09-01 03:22:01,710:INFO:Defining folds
2024-09-01 03:22:01,710:INFO:Declaring metric variables
2024-09-01 03:22:01,713:INFO:Importing untrained model
2024-09-01 03:22:01,716:INFO:Gradient Boosting Classifier Imported successfully
2024-09-01 03:22:01,724:INFO:Starting cross validation
2024-09-01 03:22:01,727:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:03,845:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,849:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,854:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,855:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,858:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,859:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,861:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,865:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,868:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,870:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,877:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,893:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,896:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,903:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,908:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,919:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,927:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,946:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,952:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,954:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,956:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,957:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,964:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,966:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:03,989:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:03,991:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,993:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:03,994:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:03,995:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:04,001:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:04,020:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:04,021:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:04,023:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:04,024:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:04,025:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:04,029:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,255:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,256:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,257:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,259:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,260:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,261:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,262:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,268:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,270:INFO:Calculating mean and std
2024-09-01 03:22:05,271:INFO:Creating metrics dataframe
2024-09-01 03:22:05,272:INFO:Uploading results into container
2024-09-01 03:22:05,273:INFO:Uploading model into container now
2024-09-01 03:22:05,273:INFO:_master_model_container: 25
2024-09-01 03:22:05,273:INFO:_display_container: 3
2024-09-01 03:22:05,273:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-01 03:22:05,274:INFO:create_model() successfully completed......................................
2024-09-01 03:22:05,371:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:05,371:INFO:Creating metrics dataframe
2024-09-01 03:22:05,379:INFO:Initializing Linear Discriminant Analysis
2024-09-01 03:22:05,379:INFO:Total runtime is 0.14939284324645996 minutes
2024-09-01 03:22:05,382:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:05,382:INFO:Initializing create_model()
2024-09-01 03:22:05,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:05,382:INFO:Checking exceptions
2024-09-01 03:22:05,383:INFO:Importing libraries
2024-09-01 03:22:05,383:INFO:Copying training dataset
2024-09-01 03:22:05,389:INFO:Defining folds
2024-09-01 03:22:05,389:INFO:Declaring metric variables
2024-09-01 03:22:05,392:INFO:Importing untrained model
2024-09-01 03:22:05,395:INFO:Linear Discriminant Analysis Imported successfully
2024-09-01 03:22:05,402:INFO:Starting cross validation
2024-09-01 03:22:05,404:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:05,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,528:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,532:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,533:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,534:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,535:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,536:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,538:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,540:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,541:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,543:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,543:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,544:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,545:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,548:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,549:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,549:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,551:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

.capitalize()} is", len(result))

2024-09-01 03:22:05,551:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,551:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,553:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,554:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,555:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,556:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,556:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,556:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,559:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,560:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,560:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,563:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,566:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,567:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,568:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,570:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,571:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,573:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,577:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,629:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,630:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,633:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-01 03:22:05,633:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,634:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,635:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,636:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,637:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:05,638:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:05,639:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,642:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:05,643:INFO:Calculating mean and std
2024-09-01 03:22:05,644:INFO:Creating metrics dataframe
2024-09-01 03:22:05,646:INFO:Uploading results into container
2024-09-01 03:22:05,646:INFO:Uploading model into container now
2024-09-01 03:22:05,646:INFO:_master_model_container: 26
2024-09-01 03:22:05,646:INFO:_display_container: 3
2024-09-01 03:22:05,647:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-01 03:22:05,647:INFO:create_model() successfully completed......................................
2024-09-01 03:22:05,744:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:05,744:INFO:Creating metrics dataframe
2024-09-01 03:22:05,755:INFO:Initializing Extra Trees Classifier
2024-09-01 03:22:05,755:INFO:Total runtime is 0.15566805998484293 minutes
2024-09-01 03:22:05,757:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:05,758:INFO:Initializing create_model()
2024-09-01 03:22:05,758:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:05,758:INFO:Checking exceptions
2024-09-01 03:22:05,758:INFO:Importing libraries
2024-09-01 03:22:05,758:INFO:Copying training dataset
2024-09-01 03:22:05,763:INFO:Defining folds
2024-09-01 03:22:05,764:INFO:Declaring metric variables
2024-09-01 03:22:05,767:INFO:Importing untrained model
2024-09-01 03:22:05,770:INFO:Extra Trees Classifier Imported successfully
2024-09-01 03:22:05,777:INFO:Starting cross validation
2024-09-01 03:22:05,778:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:06,150:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,151:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,152:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,154:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,156:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,158:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,159:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,160:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,162:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,162:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,163:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,165:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,166:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,167:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,168:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,169:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,171:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,172:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,170:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,176:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,177:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,179:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,185:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,237:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,240:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,242:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,244:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,251:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,392:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,394:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,401:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,403:INFO:Calculating mean and std
2024-09-01 03:22:06,404:INFO:Creating metrics dataframe
2024-09-01 03:22:06,405:INFO:Uploading results into container
2024-09-01 03:22:06,406:INFO:Uploading model into container now
2024-09-01 03:22:06,406:INFO:_master_model_container: 27
2024-09-01 03:22:06,406:INFO:_display_container: 3
2024-09-01 03:22:06,406:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-09-01 03:22:06,406:INFO:create_model() successfully completed......................................
2024-09-01 03:22:06,511:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:06,511:INFO:Creating metrics dataframe
2024-09-01 03:22:06,524:INFO:Initializing Extreme Gradient Boosting
2024-09-01 03:22:06,525:INFO:Total runtime is 0.16849778493245443 minutes
2024-09-01 03:22:06,527:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:06,527:INFO:Initializing create_model()
2024-09-01 03:22:06,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:06,527:INFO:Checking exceptions
2024-09-01 03:22:06,528:INFO:Importing libraries
2024-09-01 03:22:06,528:INFO:Copying training dataset
2024-09-01 03:22:06,536:INFO:Defining folds
2024-09-01 03:22:06,536:INFO:Declaring metric variables
2024-09-01 03:22:06,539:INFO:Importing untrained model
2024-09-01 03:22:06,543:INFO:Extreme Gradient Boosting Imported successfully
2024-09-01 03:22:06,550:INFO:Starting cross validation
2024-09-01 03:22:06,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:06,901:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,904:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,905:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,907:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,909:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,910:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,911:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,914:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,917:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,918:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,921:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,923:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,930:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,934:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,937:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,938:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,940:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,941:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,943:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,945:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,948:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,949:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,950:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,951:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,953:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,955:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,960:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:06,965:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,968:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,969:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:06,971:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:06,975:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:07,095:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,097:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,098:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:07,099:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,100:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:07,102:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:07,104:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:07,105:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:07,106:INFO:Calculating mean and std
2024-09-01 03:22:07,108:INFO:Creating metrics dataframe
2024-09-01 03:22:07,110:INFO:Uploading results into container
2024-09-01 03:22:07,110:INFO:Uploading model into container now
2024-09-01 03:22:07,110:INFO:_master_model_container: 28
2024-09-01 03:22:07,110:INFO:_display_container: 3
2024-09-01 03:22:07,110:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-01 03:22:07,110:INFO:create_model() successfully completed......................................
2024-09-01 03:22:07,206:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:07,206:INFO:Creating metrics dataframe
2024-09-01 03:22:07,215:INFO:Initializing Light Gradient Boosting Machine
2024-09-01 03:22:07,216:INFO:Total runtime is 0.18001721302668253 minutes
2024-09-01 03:22:07,219:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:07,219:INFO:Initializing create_model()
2024-09-01 03:22:07,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:07,219:INFO:Checking exceptions
2024-09-01 03:22:07,219:INFO:Importing libraries
2024-09-01 03:22:07,219:INFO:Copying training dataset
2024-09-01 03:22:07,225:INFO:Defining folds
2024-09-01 03:22:07,225:INFO:Declaring metric variables
2024-09-01 03:22:07,229:INFO:Importing untrained model
2024-09-01 03:22:07,234:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-01 03:22:07,239:INFO:Starting cross validation
2024-09-01 03:22:07,240:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:08,065:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,069:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,071:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,074:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,076:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,080:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,082:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,085:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,092:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,273:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,277:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,279:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,281:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,378:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,383:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,385:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,387:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,510:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,515:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,517:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,519:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,526:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,531:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,535:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,537:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,539:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,542:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,547:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,549:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,552:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,558:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,660:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,665:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,667:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,669:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,675:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,920:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,924:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,926:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,928:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,935:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,954:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,958:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,959:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:08,961:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:08,967:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:08,969:INFO:Calculating mean and std
2024-09-01 03:22:08,970:INFO:Creating metrics dataframe
2024-09-01 03:22:08,973:INFO:Uploading results into container
2024-09-01 03:22:08,974:INFO:Uploading model into container now
2024-09-01 03:22:08,974:INFO:_master_model_container: 29
2024-09-01 03:22:08,975:INFO:_display_container: 3
2024-09-01 03:22:08,975:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-01 03:22:08,976:INFO:create_model() successfully completed......................................
2024-09-01 03:22:09,098:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:09,098:INFO:Creating metrics dataframe
2024-09-01 03:22:09,107:INFO:Initializing Dummy Classifier
2024-09-01 03:22:09,107:INFO:Total runtime is 0.21152237256368 minutes
2024-09-01 03:22:09,110:INFO:SubProcess create_model() called ==================================
2024-09-01 03:22:09,110:INFO:Initializing create_model()
2024-09-01 03:22:09,111:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A4028CED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:09,111:INFO:Checking exceptions
2024-09-01 03:22:09,111:INFO:Importing libraries
2024-09-01 03:22:09,111:INFO:Copying training dataset
2024-09-01 03:22:09,117:INFO:Defining folds
2024-09-01 03:22:09,117:INFO:Declaring metric variables
2024-09-01 03:22:09,120:INFO:Importing untrained model
2024-09-01 03:22:09,124:INFO:Dummy Classifier Imported successfully
2024-09-01 03:22:09,130:INFO:Starting cross validation
2024-09-01 03:22:09,132:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-01 03:22:09,278:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,283:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,285:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,288:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,291:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,296:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,297:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,299:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,300:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,302:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,303:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,305:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,306:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,309:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,313:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,321:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,322:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,324:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,325:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,327:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,330:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,331:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,333:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,334:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,335:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,337:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,339:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,386:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,388:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,389:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,390:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,393:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,395:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,397:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-01 03:22:09,398:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Tool Wear Failure') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-01 03:22:09,401:WARNING:c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(custom_cost_benefit, response_method='predict')' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\chand\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 106, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: custom_cost_benefit() got an unexpected keyword argument 'sample_weight'

  warnings.warn(

2024-09-01 03:22:09,402:INFO:Calculating mean and std
2024-09-01 03:22:09,403:INFO:Creating metrics dataframe
2024-09-01 03:22:09,405:INFO:Uploading results into container
2024-09-01 03:22:09,405:INFO:Uploading model into container now
2024-09-01 03:22:09,406:INFO:_master_model_container: 30
2024-09-01 03:22:09,406:INFO:_display_container: 3
2024-09-01 03:22:09,406:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-09-01 03:22:09,406:INFO:create_model() successfully completed......................................
2024-09-01 03:22:09,503:INFO:SubProcess create_model() end ==================================
2024-09-01 03:22:09,503:INFO:Creating metrics dataframe
2024-09-01 03:22:09,523:INFO:Initializing create_model()
2024-09-01 03:22:09,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-01 03:22:09,523:INFO:Checking exceptions
2024-09-01 03:22:09,525:INFO:Importing libraries
2024-09-01 03:22:09,525:INFO:Copying training dataset
2024-09-01 03:22:09,530:INFO:Defining folds
2024-09-01 03:22:09,530:INFO:Declaring metric variables
2024-09-01 03:22:09,531:INFO:Importing untrained model
2024-09-01 03:22:09,531:INFO:Declaring custom model
2024-09-01 03:22:09,531:INFO:Logistic Regression Imported successfully
2024-09-01 03:22:09,532:INFO:Cross validation set to False
2024-09-01 03:22:09,532:INFO:Fitting Model
2024-09-01 03:22:09,704:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 03:22:09,704:INFO:create_model() successfully completed......................................
2024-09-01 03:22:09,823:INFO:_master_model_container: 30
2024-09-01 03:22:09,824:INFO:_display_container: 3
2024-09-01 03:22:09,824:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-01 03:22:09,824:INFO:compare_models() successfully completed......................................
2024-09-01 03:22:09,868:INFO:Initializing plot_model()
2024-09-01 03:22:09,869:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=class_report, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:22:09,869:INFO:Checking exceptions
2024-09-01 03:22:09,873:INFO:Preloading libraries
2024-09-01 03:22:09,874:INFO:Copying training dataset
2024-09-01 03:22:09,874:INFO:Plot type: class_report
2024-09-01 03:22:10,128:INFO:Fitting Model
2024-09-01 03:22:10,128:INFO:Scoring test/hold-out set
2024-09-01 03:22:10,312:INFO:Visual Rendered Successfully
2024-09-01 03:22:10,418:INFO:plot_model() successfully completed......................................
2024-09-01 03:22:10,431:INFO:Initializing plot_model()
2024-09-01 03:22:10,431:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:22:10,431:INFO:Checking exceptions
2024-09-01 03:22:10,437:INFO:Preloading libraries
2024-09-01 03:22:10,437:INFO:Copying training dataset
2024-09-01 03:22:10,437:INFO:Plot type: confusion_matrix
2024-09-01 03:22:10,719:INFO:Fitting Model
2024-09-01 03:22:10,720:INFO:Scoring test/hold-out set
2024-09-01 03:22:10,853:INFO:Visual Rendered Successfully
2024-09-01 03:22:10,969:INFO:plot_model() successfully completed......................................
2024-09-01 03:22:10,985:INFO:Initializing plot_model()
2024-09-01 03:22:10,986:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:22:10,986:INFO:Checking exceptions
2024-09-01 03:22:10,991:INFO:Preloading libraries
2024-09-01 03:22:10,991:INFO:Copying training dataset
2024-09-01 03:22:10,991:INFO:Plot type: auc
2024-09-01 03:22:11,252:INFO:Fitting Model
2024-09-01 03:22:11,252:INFO:Scoring test/hold-out set
2024-09-01 03:22:11,410:INFO:Visual Rendered Successfully
2024-09-01 03:22:11,551:INFO:plot_model() successfully completed......................................
2024-09-01 03:22:11,568:INFO:Initializing plot_model()
2024-09-01 03:22:11,568:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A39C53D10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-01 03:22:11,568:INFO:Checking exceptions
2024-09-01 03:22:11,573:INFO:Preloading libraries
2024-09-01 03:22:11,573:INFO:Copying training dataset
2024-09-01 03:22:11,573:INFO:Plot type: learning
2024-09-01 03:22:11,820:INFO:Fitting Model
2024-09-01 03:22:24,392:INFO:Visual Rendered Successfully
2024-09-01 03:22:24,489:INFO:plot_model() successfully completed......................................
